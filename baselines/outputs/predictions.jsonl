{"specification":"MiniTorch\nMiniTorch is a diy teaching library for machine learning engineers who wish to learn about the internal concepts underlying deep learning systems.\nIt is a pure Python re-implementation of the Torch API designed to be simple, easy-to-read, tested, and incremental. The final library can run Torch\ncode.\nThe main repo for the course is available on GitHub. To complete the assignments you fill in TODO statements and pass the included unit test suite.\nThere are also additional visualization tools to follow your progress.\nIndividual assignments cover:\nML Programming Foundations\nAutodifferentiation\nTensors\nGPUs and Parallel Programming\nFoundational Deep Learning\nThe project was developed for the course Machine Learning Engineering at Cornell Tech and based on my experiences working at Hugging Face.\nReach out if you are interested in the teacher's version of the repository.\nEnjoy!\nSasha Rush (@srush_nlp) with Ge Gao, Anton Abilov, and Aaron Gokaslan.\nclass ReLU(ScalarFunction):\n    @staticmethod\n    def forward(ctx, a):\n        # TODO: Implement for Task 1.2.\n        raise NotImplementedError('Need to implement for Task 1.2')\n    @staticmethod\n    def backward(ctx, d_output):\n        # TODO: Implement for Task 1.4.\n        raise NotImplementedError('Need to implement for Task 1.4')\nSetup\nMiniTorch requires Python 3.8 or higher. To check your version of Python, run either:\nWe recommend creating a global MiniTorch workspace directory that you will use for all modules.\nWe also highly recommand setting up a virtual environment. The virtual environment lets you install packages that are only used for your\nassignments and do not impact the rest of the system. We suggest venv or anaconda.\nFor example, if you choose venv, run the following command:\nThe first line should be run only once, whereas the second needs to be run whenever you open a new terminal to get started for the class. You can\ntell if the second line works by checking if your terminal starts with (venv) . See https:\/\/docs.python.org\/3\/library\/venv.html for further\ninstructions on how this works.\nEach assignment is distributed through a Git repo. We assume the knowledge of git throughout the course. See https:\/\/guides.github.com for a\ntutorial about using git and GitHub.\nYou should fork the template of the assignment and then edit yours in your forked repo. Once you have forked the template code, you can clone\nyour own version by running the following command:\nThe last step is to install packages. There are several packages used throughout these assignments, and you can install them in your virtual\nenviroment by running:\nFor anaconda users, you need to run an extra command to install llvmlite:\nMake sure that everything is installed by running python  and then checking:\n>>> python --version\n>>> python3 --version\n>>> mkdir workspace; cd workspace\n>>> python -m venv venv\n>>> source venv\/bin\/activate\n>>> git clone {{ASSIGNMENT}}\n>>> cd {{ASSIGNMENT}}\n>>> python -m pip install -r requirements.txt\n>>> python -m pip install -r requirements.extra.txt\n>>> python -m pip install -Ue .\n>>> conda install llvmlite\n>>> import minitorch\nFundamentals\nThis introductory module is focused on introducing several core software engineering methods for testing and debugging, and also includes some\nbasic mathematical foundations.\nBefore starting this assignment, make sure to set up your workspace following the setup guide, to understand how the code should be organized.\nGuides\nEach module has a set of guides to help with the background material. We recommend working through the assignment and utilizing the guides\nsuggested for each task.\nContributing\nFunctional Python\nProperty Testing\nModules\nVisualization\nTask 0.1: Operators\nThis task is designed to help you get comfortable with style checking and testing. We ask you to implement a series of basic mathematical\nfunctions. These functions are simple, but they form the basis of MiniTorch. Make sure that you understand each of them as some terminologies\nmight be new.\nComplete the following functions in minitorch\/operators.py  and pass tests marked as task0_1 .\nminitorch.operators.mul(x: float, y: float) -> float\nminitorch.operators.id(x: float) -> float\nminitorch.operators.eq(x: float, y: float) -> float\n 1.0 if x is equal to y else 0.0\nminitorch.operators.neg(x: float) -> float\nminitorch.operators.add(x: float, y: float) -> float\nminitorch.operators.max(x: float, y: float) -> float\n x if x is greater than y else y\nminitorch.operators.lt(x: float, y: float) -> float\n 1.0 if x is less than y else 0.0\nminitorch.operators.sigmoid(x: float) -> float\n(See https:\/\/en.wikipedia.org\/wiki\/Sigmoid_function )\nCalculate as\nTodo\nf(x, y) = x \u2217y\nf(x) = x\nf(x) =\nf(x) = \u2212x\nf(x, y) = x + y\nf(x) =\nf(x) =\nf(x) =\n1.0\n(1.0+e\u2212x)\n if x >=0 else \nfor stability.\nminitorch.operators.relu(x: float) -> float\n x if x is greater than 0, else 0\n(See https:\/\/en.wikipedia.org\/wiki\/Rectifier_(neural_networks) .)\nminitorch.operators.inv(x: float) -> float\nminitorch.operators.inv_back(x: float, d: float) -> float\nIf \n compute \nminitorch.operators.relu_back(x: float, d: float) -> float\nIf \n compute \nminitorch.operators.log_back(x: float, d: float) -> float\nIf \n as above, compute \nminitorch.operators.is_close(x: float, y: float) -> float\nTask 0.2: Testing and Debugging\nWe ask you to implement property tests for your operators from Task 0.1. These tests should ensure that your functions not only work but also\nobey high-level mathematical properties for any input. Note that you need to change arguments for those test functions.\nComplete the test functions in tests\/test_operators.py  marked as task0_2 .\nTask 0.3: Functional Python\nTo practice the use of higher-order functions in Python, implement three basic functional concepts. Use them in combination with operators\ndescribed in Task 0.1 to build up more complex mathematical operations that work on lists instead of single values.\nComplete the following functions in minitorch\/operators.py  and pass tests marked as tasks0_3 .\nminitorch.operators.map(fn: Callable[[float], float]) -> Callable[[Iterable[float]], Iterable[float]]\nHigher-order map.\nSee https:\/\/en.wikipedia.org\/wiki\/Map_(higher-order_function)\nParameters:\nfn ( Callable[[float], float] ) \u2013 Function from one value to one value.\nReturns:\nCallable[[Iterable[float]], Iterable[float]]  \u2013 A function that takes a list, applies fn  to each element, and returns a\nCallable[[Iterable[float]], Iterable[float]]  \u2013 new list\nminitorch.operators.negList(ls: Iterable[float]) -> Iterable[float]\nUse map  and neg  to negate each element in ls\nminitorch.operators.zipWith(fn: Callable[[float, float], float]) -> Callable[[Iterable[float], Iterable[float]], Iterable[float]]\nHigher-order zipwith (or map2).\nf(x) =\n1.0\n(1.0+e\u2212x)\nex\n(1.0+ex)\nf(x) =\nf(x) = 1\/x\nf(x) = 1\/x\nd \u00d7 f \u2032(x)\nf = relu\nd \u00d7 f \u2032(x)\nf = log\nd \u00d7 f \u2032(x)\nf(x) = |x \u2212y| < 1e \u22122\nTodo\nTodo\nSee https:\/\/en.wikipedia.org\/wiki\/Map_(higher-order_function)\nParameters:\nfn ( Callable[[float, float], float] ) \u2013 combine two values\nReturns:\nCallable[[Iterable[float], Iterable[float]], Iterable[float]]  \u2013 Function that takes two equally sized lists ls1  and ls2 , produce\na new list by\nCallable[[Iterable[float], Iterable[float]], Iterable[float]]  \u2013 applying fn(x, y) on each pair of elements.\nminitorch.operators.addLists(ls1: Iterable[float], ls2: Iterable[float]) -> Iterable[float]\nAdd the elements of ls1  and ls2  using zipWith  and add\nminitorch.operators.reduce(fn: Callable[[float, float], float], start: float) -> Callable[[Iterable[float]], float]\nHigher-order reduce.\nParameters:\nfn ( Callable[[float, float], float] ) \u2013 combine two values\nstart ( float ) \u2013 start value \nReturns:\nCallable[[Iterable[float]], float]  \u2013 Function that takes a list ls  of elements\nCallable[[Iterable[float]], float]  \u2013 \n and computes the reduction :math:`fn(x_3, fn(x_2,\nCallable[[Iterable[float]], float]  \u2013 fn(x_1, x_0)))`\nminitorch.operators.sum(ls: Iterable[float]) -> float\nSum up a list using reduce  and add .\nminitorch.operators.prod(ls: Iterable[float]) -> float\nProduct of a list using reduce  and mul .\nTask 0.4: Modules\nThis task is to implement the core structure of the :class: minitorch.Module  class. We ask you to implement a tree data structure that stores\nnamed :class: minitorch.Parameter  on each node. Such a data structure makes it easy for users to create trees that can be walked  to find all of\nthe parameters of interest.\nTo experiment with the system use the Module Sandbox :\nComplete the functions in minitorch\/module.py  and pass tests marked as tasks0_4 .\nminitorch.module.Module.train() -> None\nSet the mode of this module and all descendent modules to train .\nminitorch.module.Module.eval() -> None\nSet the mode of this module and all descendent modules to eval .\nminitorch.module.Module.named_parameters() -> Sequence[Tuple[str, Parameter]]\nCollect all the parameters of this module and its descendents.\nReturns:\nSequence[Tuple[str, Parameter]]  \u2013 The name and Parameter  of each ancestor parameter.\nminitorch.module.Module.parameters() -> Sequence[Parameter]\nx0\nx1 \u2026 xn\n>>> streamlit run app.py -- 0\nTodo\nEnumerate over all the parameters of this module and its descendents.\nTask 0.5: Visualization\nFor the first few assignments, we use a set of datasets implemented in minitorch\/datasets.py , which are 2D point classification datasets. (See\nTensorFlow Playground for similar examples.) Each of these dataset can be added to the visualization.\nTo experiment with the system use:\nRead through the code in project\/run_torch.py  to get a sneak peek of an implementation of a model for these datasets using Torch.\nYou can also provide a model that attempts to perform the classification by manipulating the parameters.\nStart a streamlit server and print an image of the dataset. Hand-create classifiers that split the linear dataset into the correct colors.\nAdd the image in the README file in your repo along with the parameters that your used.\nstreamlit run app.py -- 0\nTodo\nML Primer\nThis guide is a primer on the very basics of machine learning that are necessary to complete the assignments and motivate the final system.\nMachine learning is a rich and well-developed field with many different models, goals, and learning settings. There are many great texts that\ncover all the aspects of the area in detail. This guide is not that. Our goal is to explain the minimal details of one dataset with one class of model.\nSpecifically, this is an introduction to supervised binary classification with neural networks. The goal of this section is to learn how a basic neural\nnetwork works to classify simple points.\nDataset\nSupervised learning problems begin with a labeled training  dataset. We assume that we are given a set of labeled points. Each point has two\ncoordinates \n and \n, and has a label  corresponding to an O or X. For instance, here is one O labeled point:\nAnd here is an X labeled point.\nIt is often convenient to plot all of the points together on one set of axes.\nx1\nx2\ny\nHere we can see that all the X points are in the top-right and all the O points are on the bottom-left. Not all datasets is this simple, and here is\nanother dataset where points are split up a bit more.\nLater in the class, we will consider datasets of different forms, e.g. a dataset of handwritten numbers, where some are 8's and others are 2's:\nHere is an example of what this dataset looks like.\nModel\nOur ML system needs to specify a model that we want to the data. A model is a function that assigns labels to data points. We can specify a\nmodel in Python through its parameters and function.\nThis model can be written mathematically as,\n.\nWe call it a linear model because it divides the data points up based on a line. We can visualize this be computing the \"decision boundary\", i.e. the\nareas where this function returns a positive and negative boundary.\n@dataclass\nclass Linear:\n    # Parameters\n    w1: float\n    w2: float\n    b: float\n    def forward(self, x1: float, x2: float) -> float:\n        return self.w1 * x1 + self.w2 * x2 + self.b\nm(x1, x2; w1, w2, b) = x1 \u00d7 w1 + x2 \u00d7 w2 + b\nmodel = Linear(1, 1, -0.9)\nWe can overlay the simple dataset described earlier over this model. This tells us roughly how well the model fits this dataset.\nModels can take many different forms, Here is another model which has a compound form. We will discuss these types of models more below. It\nsplits its decision into three regions (Model B).\nModels may also have strange shapes and even disconnected regions. Any blue\/red split will do, for instance (Model C):\n@dataclass\nclass Split:\n    m1: Linear\n    m2: Linear\n    def forward(self, x1, x2):\n        return self.m1.forward(x1, x2) * self.m2.forward(x1, x2)\nmodel_b = Split(Linear(1, 1, -1.5), Linear(1, 1, -0.5))\n@dataclass\nclass Part:\n    def forward(self, x1, x2):\n        return 1 if (0.0 <= x1 < 0.5 and 0.0 <= x2 < 0.6) else 0\nParameters\nOnce we have decided on the shape that we are using, we need a way to move between models in that class. Ideally, we would have internal\nknobs that alter the properties of the model.\nIn the case of the linear models, there are two knobs,\na. rotating the separator\nshow(Linear(1, 1, -0.5))\nshow(Linear(1, 1, -1))\n\u2192\nb. changing the separator cutoff\n\u2192\nParameters are the set of numerical values that fully define a model's decisions. Parameters are critical for storing how a model acts, and\nnecessary for producing its decision on a given data point.\nRecall the functional form of the model is,\nHere \n are parameters, \n are the input point. The semi-colon notation indicates which arguments are for parameters and which\nare for data.\nOur goal in this class will be to move these knobs to find the best data fit.\nm(x1, x2; w1, w2, b) = x1 \u00d7 w1 + x2 \u00d7 w2 + b\nw1, w2, b\nx1, x2\nbiases = [(i \/ 25.0) - 0.1 for i in range(0, 26, 5)]\nLoss\nObserving the data, we can see that some parameters lead to good models with few classification errors,\nAnd some are bad and make multiple errors,\nIn order to find a good model, we need to first define what good means. We do this through a loss  function that quantifies how badly we are\ncurrently doing. A good model has small loss.\nshow(Linear(1, 1, -1.0))\nshow(Linear(1, 1, -0.5))\nOur loss function will be based on the distance and direction of the line from each point to the decision boundary.\nConsider a single point with different models.\nThis point might be classified the correct side and very far from this line (Point A, \"great\"):\nOr it might be on the correct side of the line, but close to the line (Point B, \"worrisome\"):\nOr this point might be classified on the wrong side of the line (Point C, \"bad\"):\nThe loss is determined based on a function of this distance. The most commonly used function (and the one we will focus on) is the sigmoid\nfunction. For strong negative inputs, it goes to zero, and for strong positive, it goes to 1. In between, it forms a smooth S-curve.\nFor computational reasons, in practice we work with the log of this function. This yields a loss function that gets much worse as we move\nfurther from the decision boundary.\nThe losses of three X points land on the following positions for the sigmoid curve. Almost zero for Point A, middle value for Point B, and nearly\none for Point C.\ndef point_loss(x):\n    return -math.log(minitorch.operators.sigmoid(-x))\nLoss is given for the red points as well, but they are penalized in the opposite direction,\nThe total loss function  for a model is the sum of each of the individual losses. Specifically,\nWhere \n are the datapoints,  is the sigmoid function, and multiplying by  reverses the function based on the true class of the point. Here\nis what this looks like in code.\nL\nL(w1, w2, b) = \u2212\u2211\nj\nlog \u03c3(yj \u00d7 m(xj\n1, xj\n2; w1, w2, b))\n(xj, yj)\n\u03c3\ny\ndef full_loss(m):\n    l = 0\n    for x, y in zip(s.X, s.y):\n        l += point_loss(-y * m.forward(*x))\n    return -l\n#-\n# Fitting Parameters\n# --------------------\n# To review, the model class tells us what shapes we can consider, the parameters\n# tell us the decision boundary, and the loss tells us how well the current model is doing.\n#\n# The last step is to produce a method for finding a good model\n# given a loss function, referred to as *parameter fitting*.\n# Exact parameter fitting is difficult. For all but the\n# simplest models, it is a challenging task.\n# This example has just 3 parameters, but some large models may have billions of parameters that need to \nbe fit.\n# We will focus on parameter fitting with *gradient\n# descent*. Gradient descent works in the following manner.\nAs we noted, this model has a high loss, and we want to consider ways to \"turn the knobs\" of the parameters to find a better model. Let us focus\non the parameter controlling the intercept.\nWe can consider how the loss changes with respect to just varying this parameter. It seems like the loss will go down if we move the intercept a\nbit.\n# 1. Compute the loss function, $L$, for the data with the current parameters.\n# 2. See how small changes to each of the parameters would change the loss.\n# 3. Update the parameters with a small change in the direction that locally\n#    most reduces the loss.\n# Let's return to the incorrect model above.\nm = Linear(1, 1, -0.5)\nm = Linear(1, 1, -0.55)\nDoing this leads to a better model.\nWe can repeat this process for the intercept as well as for all the other parameters in the model.\nBut how did we know how the loss function will change? For a small problem, we can move and see. But remember that machine learning\nmodels are large.\nIn the first module of Minitorch, we will see how to compute the direction efficiently for small problems, and then scale it up to much large\nmodels.\nNeural Networks\nThe linear model class can be used to find good fits to the data we have considered so far, but it fails for data that splits up into multiple\nsegments. These datasets are not linearly separable. Let us consider a very simple dataset with this property.\nLet's look at our dataset:\nchalk.set_svg_height(200)\nsplit_graph(s1_hard, s2_hard, show_origin=True)\nAn alternative model class for this data is a neural network. Neural networks can be used to specify a much wider range of separators.\nNeural networks are compound model classes that divide classification into two or more stages.\nEach stage uses a linear model to seperate the data. And then an activation function to reshape it.\nTo see how this works consider how we might split up the datasets above. Instead of splitting all the points directly, we might first split off the\nleft points,\nAnd then produce another separator (green) to pull apart the red points,\nWe would like only points in the green or yellow sections to be classified as X's.\nTo do this, we employ an activation function that filters out only these points. This function is known as a ReLU function, which is a fancy way of\nsaying \"threshold\".\nmodel = Linear(1, 1, -0.7)\nReLU(z) = { z\nz \u22650\n0\nz < 0\nFor the yellow separator, the ReLU yields the following values:\nBasically the right X's are thresholed to positive values and the other O's and X's are 0.\nFinally yellow and green become our new \n. Since all the O's are now at the origin it is very easy to separate out the space.\nLooking back at the original model, this process appears like it has produced two lines to pull apart the data.\nx1, x2\n@dataclass\nclass MLP:\n    lin1: Linear\n    lin2: Linear\n    final: Linear\n    def forward(self, x1, x2):\n        x1_1 = minitorch.operators.relu(self.lin1.forward(x1, x2))\n        x2_1 = minitorch.operators.relu(self.lin2.forward(x1, x2))\n        return self.final.forward(x1_1, x2_1)\nmlp = MLP(green, yellow, Linear(3, 3, -0.3))\ndraw_with_hard_points(mlp)\nMathematically we can think of the transformed data as values \n which we get from applying separators with different parameters to the\noriginal data. The final prediction then applies a separator to \n.\nHere \n are all parameters. We have gained more flexible models, at the cost of now needing to fit many more\nparameters to the data.\nThis neural network will be the main focus for the first couple models. It appears quite simple, but fitting it effectively will require building up\nsystems infrastructure. Once we have this infrastructure, though, we will be able to easily support most modern neural network models.\nh1, h2\nh1, h2\nh1 = ReLU(x1 \u00d7 w0\n1 + x2 \u00d7 w0\n2 + b0)\nh2 = ReLU(x1 \u00d7 w1\n1 + x2 \u00d7 w1\n2 + b1)\nm(x1, x2) = h1 \u00d7 w1 + h2 \u00d7 w2 + b\nw1, w2, w0\n1, w0\n2, w1\n1, w1\n2, b, b0, b1\nAssignment\nThis module shows how to build the first version of MiniTorch using only simple values and functions. This covers key aspects of auto-\ndifferentiation: the key technique in the system. Then you will use your code to train a preliminary model.\nGuides\nDerivatives\nScalar\nAutodifferentiation\nBackpropagation\nTask 1.1: Numerical Derivatives\nImplement scalar numerical derivative calculation. This function will not be used in the main library but will be critical for testing the whole module.\nComplete the following function in minitorch\/autodiff.py  and pass tests marked as task1_1 .\nminitorch.autodiff.central_difference(f: Any, *vals: Any, arg: int = 0, epsilon: float = 1e-06) -> Any\nComputes an approximation to the derivative of f  with respect to one arg.\nSee :doc: derivative  or https:\/\/en.wikipedia.org\/wiki\/Finite_difference for more details.\nParameters:\nf \u2013 arbitrary function from n-scalar args to one value\n*vals \u2013 n-float values \narg \u2013 the number  of the arg to compute the derivative\nepsilon \u2013 a small constant\nReturns:\nAny  \u2013 An approximation of \nTask 1.2: Scalars\nImplement the overridden mathematical functions required for the minitorch.Scalar  class. Each of these requires wiring the internal Python\noperator to the correct minitorch.Function.forward  call.\nRead the example ScalarFunctions that we have implemented for guidelines. You may find it useful to reuse the operators from Module 0.\nWe have built a debugging tool for you to observe the workings of your expressions to see how the graph is built. You can run it in the Autodiff\nSandbox. You can alter the expression at the top of the file and then run the code to create a graph in Streamlit:\nComplete the following functions in minitorch\/scalar_functions.py .\nminitorch.scalar_functions.Mul.forward(ctx: Context, a: float, b: float) -> float \nminitorch.scalar_functions.Inv.forward(ctx: Context, a: float) -> float \nminitorch.scalar_functions.Neg.forward(ctx: Context, a: float) -> float \nminitorch.scalar_functions.Sigmoid.forward(ctx: Context, a: float) -> float \nTodo\nx0 \u2026 xn\u22121\ni\nf \u2032\ni(x0, \u2026 , xn\u22121)\nstreamlit run app.py -- 1\nTodo\nstaticmethod\nstaticmethod\nstaticmethod\nstaticmethod\nminitorch.scalar_functions.ReLU.forward(ctx: Context, a: float) -> float \nminitorch.scalar_functions.Exp.forward(ctx: Context, a: float) -> float \nminitorch.scalar_functions.LT.forward(ctx: Context, a: float, b: float) -> float \nminitorch.scalar_functions.EQ.forward(ctx: Context, a: float, b: float) -> float \nComplete the following function in minitorch\/scalar.py , and pass tests marked as task1_2 . See Python numerical overrides for the interface of these\nmethods. All of these functions should return minitorch.Scalar  arguments.\nminitorch.scalar.Scalar.__lt__(b: ScalarLike) -> Scalar\nminitorch.scalar.Scalar.__gt__(b: ScalarLike) -> Scalar\nminitorch.scalar.Scalar.__sub__(b: ScalarLike) -> Scalar\nminitorch.scalar.Scalar.__neg__() -> Scalar\nminitorch.scalar.Scalar.__add__(b: ScalarLike) -> Scalar\nminitorch.scalar.Scalar.log() -> Scalar\nminitorch.scalar.Scalar.exp() -> Scalar\nminitorch.scalar.Scalar.sigmoid() -> Scalar\nminitorch.scalar.Scalar.relu() -> Scalar\nTask 1.3: Chain Rule\nImplement the chain_rule function in Scalar for functions of arbitrary arguments. This function should be able to backward process a function by\npassing it in a context and  and then collecting the local derivatives. It should then pair these with the right variables and return them. This\nfunction is also where we filter out constants that were used on the forward pass, but do not need derivatives.\nComplete the following function in minitorch\/scalar.py , and pass tests marked as task1_3 .\nminitorch.scalar.Scalar.chain_rule(d_output: Any) -> Iterable[Tuple[Variable, Any]]\nTask 1.4: Backpropagation\nImplement backpropagation. Each of these requires wiring the internal Python operator to the correct minitorch.Function.backward  call.\nRead the example ScalarFunctions that we have implemented for guidelines. Feel free to also consult differentiation rules if you forget how these\nidentities work.\nComplete the following functions in minitorch\/autodiff.py  and minitorch\/scalar.py , and pass tests marked as task1_4 .\nminitorch.topological_sort(variable: Variable) -> Iterable[Variable]\nComputes the topological order of the computation graph.\nParameters:\nvariable ( Variable ) \u2013 The right-most variable\nReturns:\nIterable[Variable]  \u2013 Non-constant Variables in topological order starting from the right.\nminitorch.backpropagate(variable: Variable, deriv: Any) -> None\nstaticmethod\nstaticmethod\nstaticmethod\nstaticmethod\nTodo\nd\nTodo\nTodo\nRuns backpropagation on the computation graph in order to compute derivatives for the leave nodes.\nParameters:\nvariable ( Variable ) \u2013 The right-most variable\nderiv (  ) \u2013 Its derivative that we want to propagate backward to the leaves.\nNo return. Should write to its results to the derivative values of each leaf through accumulate_derivative .\nminitorch.scalar_functions.Mul.backward(ctx: Context, d_output: float) -> Tuple[float, float] \nminitorch.scalar_functions.Inv.backward(ctx: Context, d_output: float) -> float \nminitorch.scalar_functions.Neg.backward(ctx: Context, d_output: float) -> float \nminitorch.scalar_functions.Sigmoid.backward(ctx: Context, d_output: float) -> float \nminitorch.scalar_functions.ReLU.backward(ctx: Context, d_output: float) -> float \nminitorch.scalar_functions.Exp.backward(ctx: Context, d_output: float) -> float \nTask 1.5: Training\nIf your code works, you should now be able to run the training script. Study the code in project\/run_scalar.py  carefully to understand what the\nneural network is doing.\nYou will also need Module code to implement the parameters Network  and for Linear . You can modify the dataset and the module with the\nparameters at the bottom of the file. Start with this simple config:\nYou can then move up to something more complex, for instance:\nIf your code is successful, you should be able to run the full visualization:\nTrain a scalar model for each of the 4 main datasets.\nAdd the output training logs and final images to your README file.\nstaticmethod\nstaticmethod\nstaticmethod\nstaticmethod\nstaticmethod\nstaticmethod\nPTS = 50\nDATASET = minitorch.datasets[\"Simple\"](PTS)\nHIDDEN = 2\nRATE = 0.5\nPTS = 50\nDATASET = minitorch.datasets[\"Xor\"](PTS)\nHIDDEN = 10\nRATE = 0.5\nstreamlit run app.py -- 1\nTodo\nTensors\nWe now have a fully developed autodifferentiation system built around scalars. This system is correct, but you saw during training that it is\ninefficient. Every scalar number requires building an object, and each operation requires storing a graph of all the values that we have previously\ncreated. Training requires repeating the above operations, and running models, such as a linear model, requires a for  loop over each of the terms\nin the network.\nThis module introduces and implements a tensor object that will solve these problems. Tensors group together many repeated operations to save\nPython overhead and to pass off grouped operations to faster implementations.\nGuides\nTensors\nBroadcasting\nTensor Operations\nAutograd\nFor this module we have implemented the skeleton tensor.py  file for you. This is similar in spirit to scalar.py  from the last assignment. Before\nstarting, it is worth reading through this file to have a sense of what a Tensor does. Each of the following tasks asks you to implement the methods\nthis file relies on:\ntensor_data.py  : Indexing, strides, and storage\ntensor_ops.py  : Higher-order tensor operations\ntensor_functions.py  : Autodifferentiation-ready functions\nTasks 2.1: Tensor Data - Indexing\nThe MiniTorch library implements the core tensor backend as minitorch.TensorData . This class handles indexing, storage, transposition, and low-\nlevel details such as strides. You will first implement these core functions before turning to the user-facing class minitorch.Tensor .\nComplete the following functions in minitorch\/tensor_data.py , and pass tests marked as task2_1 .\nminitorch.index_to_position(index: Index, strides: Strides) -> int\nConverts a multidimensional tensor index  into a single-dimensional position in storage based on strides.\nParameters:\nindex \u2013 index tuple of ints\nstrides \u2013 tensor strides\nReturns:\nint  \u2013 Position in storage\nminitorch.to_index(ordinal: int, shape: Shape, out_index: OutIndex) -> None\nConvert an ordinal  to an index in the shape . Should ensure that enumerating position 0 ... size of a tensor produces every index exactly once.\nIt may not be the inverse of index_to_position .\nParameters:\nordinal ( int ) \u2013 ordinal position to convert.\nshape \u2013 tensor shape.\nout_index \u2013 return index corresponding to position.\nminitorch.tensor_data.TensorData.permute(*order: int) -> TensorData\nPermute the dimensions of the tensor.\nTodo\nParameters:\n*order ( int ) \u2013 a permutation of the dimensions\nReturns:\nTensorData  \u2013 New TensorData  with the same storage and a new dimension order.\nTasks 2.2: Tensor Broadcasting\nComplete following functions in minitorch\/tensor_data.py  and pass tests marked as task2_2 .\nminitorch.shape_broadcast(shape1: UserShape, shape2: UserShape) -> UserShape\nBroadcast two shapes to create a new union shape.\nParameters:\nshape1 \u2013 first shape\nshape2 \u2013 second shape\nReturns:\nUserShape  \u2013 broadcasted shape\nRaises:\nIndexingError  \u2013 if cannot broadcast\nminitorch.broadcast_index(big_index: Index, big_shape: Shape, shape: Shape, out_index: OutIndex) -> None\nConvert a big_index  into big_shape  to a smaller out_index  into shape  following broadcasting rules. In this case it may be larger or with\nmore dimensions than the shape  given. Additional dimensions may need to be mapped to 0 or removed.\nParameters:\nbig_index \u2013 multidimensional index of bigger tensor\nbig_shape \u2013 tensor shape of bigger tensor\nshape \u2013 tensor shape of smaller tensor\nout_index \u2013 multidimensional index of smaller tensor\nReturns:\nNone  \u2013 None\nTasks 2.3: Tensor Operations\nTensor operations apply high-level, higher-order operations to all elements in a tensor simultaneously. In particular, you can map, zip, and reduce\ntensor data objects together. On top of this foundation, we can build up a Function  class for Tensor, similar to what we did for the ScalarFunction.\nIn this task, you will first implement generic tensor operations and then use them to implement forward  for specific operations.\nWe have built a debugging tool for you to observe the workings of your expressions to see how the graph is built. You can alter the expression at in\nStreamlit  to view the graph\nTodo\ny = x * z + 10.0\n>>> python project\/show_expression.py\nAdd functions in minitorch\/tensor_ops.py  and minitorch\/tensor_functions.py  for each of the following, and pass tests marked as task2_3 .\nminitorch.tensor_ops.tensor_map(fn: Callable[[float], float]) -> Callable[[Storage, Shape, Strides, Storage, Shape, Strides], None]\nLow-level implementation of tensor map between tensors with possibly different strides.\nSimple version:\nFill in the out  array by applying fn  to each value of in_storage  assuming out_shape  and in_shape  are the same size.\nBroadcasted version:\nFill in the out  array by applying fn  to each value of in_storage  assuming out_shape  and in_shape  broadcast. ( in_shape  must be\nsmaller than out_shape ).\nParameters:\nfn ( Callable[[float], float] ) \u2013 function from float-to-float to apply\nReturns:\nCallable[[Storage, Shape, Strides, Storage, Shape, Strides], None]  \u2013 Tensor map function.\nminitorch.tensor_ops.tensor_zip(fn: Callable[[float, float], float]) -> Callable[[Storage, Shape, Strides, Storage, Shape, Strides, \nStorage, Shape, Strides], None]\nLow-level implementation of tensor zip between tensors with possibly different strides.\nSimple version:\nFill in the out  array by applying fn  to each value of a_storage  and b_storage  assuming out_shape  and a_shape  are the same size.\nBroadcasted version:\nFill in the out  array by applying fn  to each value of a_storage  and b_storage  assuming a_shape  and b_shape  broadcast to\nout_shape .\nParameters:\nfn ( Callable[[float, float], float] ) \u2013 function mapping two floats to float to apply\nReturns:\nCallable[[Storage, Shape, Strides, Storage, Shape, Strides, Storage, Shape, Strides], None]  \u2013 Tensor zip function.\nTodo\nminitorch.tensor_ops.tensor_reduce(fn: Callable[[float, float], float]) -> Callable[[Storage, Shape, Strides, Storage, Shape, \nStrides, int], None]\nLow-level implementation of tensor reduce.\nout_shape  will be the same as a_shape  except with reduce_dim  turned to size 1\nParameters:\nfn ( Callable[[float, float], float] ) \u2013 reduction function mapping two floats to float\nReturns:\nCallable[[Storage, Shape, Strides, Storage, Shape, Strides, int], None]  \u2013 Tensor reduce function.\nminitorch.tensor_functions.Mul.forward(ctx: Context, a: Tensor, b: Tensor) -> Tensor \nminitorch.tensor_functions.Sigmoid.forward(ctx: Context, t1: Tensor) -> Tensor \nminitorch.tensor_functions.ReLU.forward(ctx: Context, t1: Tensor) -> Tensor \nminitorch.tensor_functions.Log.forward(ctx: Context, t1: Tensor) -> Tensor \nminitorch.tensor_functions.Exp.forward(ctx: Context, t1: Tensor) -> Tensor \nminitorch.tensor_functions.LT.forward(ctx: Context, a: Tensor, b: Tensor) -> Tensor \nminitorch.tensor_functions.EQ.forward(ctx: Context, a: Tensor, b: Tensor) -> Tensor \nminitorch.tensor_functions.Permute.forward(ctx: Context, a: Tensor, order: Tensor) -> Tensor \nminitorch.tensor_functions.IsClose.forward(ctx: Context, a: Tensor, b: Tensor) -> Tensor \nTasks 2.4: Gradients and Autograd\nSimilar to minitorch.Scalar , minitorch.Tensor  is a Variable that supports autodifferentiation. In this task, you will implement backward\nfunctions for tensor operations.\nComplete following functions in minitorch\/tensor_functions.py , and pass tests marked as task2_4 .\nminitorch.tensor_functions.Mul.backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor] \nminitorch.tensor_functions.Sigmoid.backward(ctx: Context, grad_output: Tensor) -> Tensor \nminitorch.tensor_functions.ReLU.backward(ctx: Context, grad_output: Tensor) -> Tensor \nminitorch.tensor_functions.Log.backward(ctx: Context, grad_output: Tensor) -> Tensor \nminitorch.tensor_functions.Exp.backward(ctx: Context, grad_output: Tensor) -> Tensor \nminitorch.tensor_functions.LT.backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor] \nminitorch.tensor_functions.EQ.backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor] \nminitorch.tensor_functions.Permute.backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, float] \nTask 2.5: Training\nIf your code works you should now be able to move on to the tensor training script in project\/run_tensor.py . This code runs the same basic\ntraining setup as in module1 , but now utilize your tensor code.\nstaticmethod\nstaticmethod\nstaticmethod\nstaticmethod\nstaticmethod\nstaticmethod\nstaticmethod\nstaticmethod\nstaticmethod\nTodo\nstaticmethod\nstaticmethod\nstaticmethod\nstaticmethod\nstaticmethod\nstaticmethod\nstaticmethod\nstaticmethod\nImplement the missing forward  functions in project\/run_tensor.py . They should do exactly the same thing as the corresponding functions in\nproject\/run_scalar.py , but now use the tensor code base.\nTrain a tensor model and add your results for all datasets to the README.\nRecord the time per epoch reported by the trainer. (It is okay if it is slow).\nTodo\nEfficiency\nIn addition to helping simplify code, tensors provide a basis for speeding up computation. In fact, they are really the only way to efficiently write\ndeep learning code in a slow language like Python. However, nothing we have done so far really makes anything faster than module0 . This module\nis focused on taking advantage of tensors to write fast code, first on standard CPUs and then using GPUs.\nYou need the files from previous assignments, so make sure to pull them over to your new repo.\nGuides\nParallelism\nMatrix Multiply\nCUDA\nFor this assignment, you will need access to a GPU. We recommend running commands in a Google Colab environment. Follow these instructions\nfor Colab setup.\nThis assignment does not require you to change the main tensor object. Instead you will only change the core higher-order operations code.\nfast_ops.py  : Low-level CPU operations\ncuda_ops.py  : Low-level GPU operations\nTask 3.1: Parallelization\nThis task requires basic familiarity with Numba prange .\nBe sure to very carefully read the section on parallelism, Numba.\nThe main backend for our codebase are the three functions map , zip , and reduce . If we can speed up these three, everything we built so far will\nget better. This exercise asks you to utilize Numba and the njit  function to speed up these functions. In particular if you can utilize parallelization\nthrough prange  you can get some big wins. Be careful though! Parallelization can lead to funny bugs.\nIn order to help debug this code, we have created a parallel analytics script for you\nRunning this script will run NUMBA diagnostics on your functions.\nComplete the following in minitorch\/fast_ops.py  and pass tests marked as task3_1 .\nInclude the diagnostics output from the above script in your README.\nBe sure that the code implements the optimizations specified in the docstrings. We will check for this explicitly.\nminitorch.fast_ops.tensor_map(fn: Callable[[float], float]) -> Callable[[Storage, Shape, Strides, Storage, Shape, Strides], None]\nNUMBA low_level tensor_map function. See tensor_ops.py  for description.\nOptimizations:\nMain loop in parallel\nAll indices use numpy buffers\nWhen out  and in  are stride-aligned, avoid indexing\nParameters:\nfn ( Callable[[float], float] ) \u2013 function mappings floats-to-floats to apply.\nNote\npython project\/parallel_check.py\nTodo\nReturns:\nCallable[[Storage, Shape, Strides, Storage, Shape, Strides], None]  \u2013 Tensor map function.\nminitorch.fast_ops.tensor_zip(fn: Callable[[float, float], float]) -> Callable[[Storage, Shape, Strides, Storage, Shape, Strides, \nStorage, Shape, Strides], None]\nNUMBA higher-order tensor zip function. See tensor_ops.py  for description.\nOptimizations:\nMain loop in parallel\nAll indices use numpy buffers\nWhen out , a , b  are stride-aligned, avoid indexing\nParameters:\nfn ( Callable[[float, float], float] ) \u2013 function maps two floats to float to apply.\nReturns:\nCallable[[Storage, Shape, Strides, Storage, Shape, Strides, Storage, Shape, Strides], None]  \u2013 Tensor zip function.\nminitorch.fast_ops.tensor_reduce(fn: Callable[[float, float], float]) -> Callable[[Storage, Shape, Strides, Storage, Shape, \nStrides, int], None]\nNUMBA higher-order tensor reduce function. See tensor_ops.py  for description.\nOptimizations:\nMain loop in parallel\nAll indices use numpy buffers\nInner-loop should not call any functions or write non-local variables\nParameters:\nfn ( Callable[[float, float], float] ) \u2013 reduction function mapping two floats to float.\nReturns:\nCallable[[Storage, Shape, Strides, Storage, Shape, Strides, int], None]  \u2013 Tensor reduce function\nTask 3.2: Matrix Multiplication\nMatrix multiplication is key to all the models that we have trained so far. In the last module, we computed matrix multiplication using broadcasting.\nIn this task, we ask you to implement it directly as a function. Do your best to make the function efficient, but for now all that matters is that you\ncorrectly produce a multiply function that passes our tests and has some parallelism.\nIn order to use this function, you will also need to add a new MatMul  Function to tensor_functions.py . We have added a version in the starter\ncode you can copy. You might also find it useful to add a slow broadcasted matrix_multiply  to tensor_ops.py  for debugging.\nIn order to help debug this code, you can use the parallel analytics script.\nAfter you finish this task, you may want to skip to 3.5 and experiment with training on the real task under speed conditions.\nComplete the following function in minitorch\/fast_ops.py . Pass tests marked as task3_2 .\nInclude the diagnostics output from the above script in your README.\nBe sure that the code implements the optimizations specified in the docstrings. We will check for this explicitly.\nminitorch.fast_ops._tensor_matrix_multiply(out: Storage, out_shape: Shape, out_strides: Strides, a_storage: Storage, a_shape: \nShape, a_strides: Strides, b_storage: Storage, b_shape: Shape, b_strides: Strides) -> None\nNUMBA tensor matrix multiply function.\nShould work for any tensor shapes that broadcast as long as\nTodo\nassert a_shape[-1] == b_shape[-2]\nOptimizations:\nOuter loop in parallel\nNo index buffers or function calls\nInner loop should have no global writes, 1 multiply.\nParameters:\nout ( Storage ) \u2013 storage for out  tensor\nout_shape ( Shape ) \u2013 shape for out  tensor\nout_strides ( Strides ) \u2013 strides for out  tensor\na_storage ( Storage ) \u2013 storage for a  tensor\na_shape ( Shape ) \u2013 shape for a  tensor\na_strides ( Strides ) \u2013 strides for a  tensor\nb_storage ( Storage ) \u2013 storage for b  tensor\nb_shape ( Shape ) \u2013 shape for b  tensor\nb_strides ( Strides ) \u2013 strides for b  tensor\nReturns:\nNone( None  ) \u2013 Fills in out\nTask 3.3: CUDA Operations\nWe can do even better than parallelization if we have access to specialized hardware. This task asks you to build a GPU implementation of the\nbackend operations. It will be hard to equal what PyTorch does, but if you are clever you can make these computations really fast (aim for 2x of\ntask 3.1).\nReduce is a particularly challenging function. We provide guides and a simple practice function to help you get started.\nComplete the following functions in minitorch\/cuda_ops.py , and pass the tests marked as task3_3 .\nminitorch.cuda_ops.tensor_map(fn: Callable[[float], float]) -> Callable[[Storage, Shape, Strides, Storage, Shape, Strides], None]\nCUDA higher-order tensor map function. ::\nfn_map = tensor_map(fn) fn_map(out, ... )\nParameters:\nfn ( Callable[[float], float] ) \u2013 function mappings floats-to-floats to apply.\nReturns:\nCallable[[Storage, Shape, Strides, Storage, Shape, Strides], None]  \u2013 Tensor map function.\nminitorch.cuda_ops.tensor_zip(fn: Callable[[float, float], float]) -> Callable[[Storage, Shape, Strides, Storage, Shape, Strides, \nStorage, Shape, Strides], None]\nCUDA higher-order tensor zipWith (or map2) function ::\nfn_zip = tensor_zip(fn) fn_zip(out, ...)\nParameters:\nfn ( Callable[[float, float], float] ) \u2013 function mappings two floats to float to apply.\nReturns:\nCallable[[Storage, Shape, Strides, Storage, Shape, Strides, Storage, Shape, Strides], None]  \u2013 Tensor zip function.\nminitorch.cuda_ops._sum_practice(out: Storage, a: Storage, size: int) -> None\nThis is a practice sum kernel to prepare for reduce.\nGiven an array of length  and out of size \n it should sum up each blockDim values into an out cell.\nTodo\nn\nn\/\/extblockDIM\n|\nNote: Each block must do the sum using shared memory!\nParameters:\nout ( Storage ) \u2013 storage for out  tensor.\na ( Storage ) \u2013 storage for a  tensor.\nsize ( int ) \u2013 length of a.\nminitorch.cuda_ops.tensor_reduce(fn: Callable[[float, float], float]) -> Callable[[Storage, Shape, Strides, Storage, Shape, \nStrides, int], None]\nCUDA higher-order tensor reduce function.\nParameters:\nfn ( Callable[[float, float], float] ) \u2013 reduction function maps two floats to float.\nReturns:\nCallable[[Storage, Shape, Strides, Storage, Shape, Strides, int], None]  \u2013 Tensor reduce function.\nTask 3.4: CUDA Matrix Multiplication\nFinally we can combine both these approaches and implement CUDA matmul . This operation is probably the most important in all of deep learning\nand is central to making models fast. Again, we first strive for accuracy, but, the faster you can make it, the better.\nImplementing matrix multiplication and reduction efficiently is hugely important for many deep learning tasks. Follow the guides provided in class\nfor implementing these functions.\nYou should document your code to show us that you understand each line. Prove to us that these lead to speed-ups on large matrix operations by\nmaking a graph comparing them to naive operations.\nImplement minitorch\/cuda_ops.py  with CUDA, and pass tests marked as task3_4 . Follow the requirements specified in the docs.\nminitorch.cuda_ops._mm_practice(out: Storage, a: Storage, b: Storage, size: int) -> None\nThis is a practice square MM kernel to prepare for matmul.\nGiven a storage out  and two storage a  and b . Where we know both are shape [size, size] with strides [size, 1].\nSize is always < 32.\nRequirements:\nAll data must be first moved to shared memory.\nOnly read each cell in a  and b  once.\nOnly write to global memory once per kernel.\nCompute\nParameters:\nout ( Storage ) \u2013 storage for out  tensor.\na ( Storage ) \u2013 storage for a  tensor.\nb ( Storage ) \u2013 storage for b  tensor.\nsize ( int ) \u2013 size of the square\n[a1, a2, . . . , a100]\n[a1+. . . +a31, a32+. . . +a64, . . . , ]\nTodo\n for i:\n     for j:\n          for k:\n              out[i, j] += a[i, k] * b[k, j]\nminitorch.cuda_ops._tensor_matrix_multiply(out: Storage, out_shape: Shape, out_strides: Strides, out_size: int, a_storage: Storage, \na_shape: Shape, a_strides: Strides, b_storage: Storage, b_shape: Shape, b_strides: Strides) -> None\nCUDA tensor matrix multiply function.\nRequirements:\nAll data must be first moved to shared memory.\nOnly read each cell in a  and b  once.\nOnly write to global memory once per kernel.\nShould work for any tensor shapes that broadcast as long as ::\nReturns:\nNone( None  ) \u2013 Fills in out\nTask 3.5: Training\nIf your code works, you should now be able to move on to the tensor training script in project\/run_fast_tensor.py . This code is the same basic\ntraining setup as module2 , but now utilizes your fast tensor code. We have left the matmul  layer blank for you to implement with your tensor code.\nHere is the command ::\nImplement the missing functions in project\/run_fast_tensor.py . These should do exactly the same thing as the corresponding functions in\nproject\/run_tensor.py , but now use the faster backend\nTrain a tensor model and add your results for all dataset to the README.\nRun a bigger model and record the time per epoch reported by the trainer.\nTrain a tensor model and add your results for all three dataset to the README. Also record the time per epoch reported by the trainer. (As a reference, our\nparallel implementation gave a 10x speedup). On a standard Colab GPU setup, aim for you CPU to get below 2 seconds per epoch and GPU to be below 1\nsecond per epoch. (With some cleverness you can do much better.)\nassert a_shape[-1] == b_shape[-2]\npython run_fast_tensor.py --BACKEND gpu --HIDDEN 100 --DATASET split --RATE 0.05\npython run_fast_tensor.py --BACKEND cpu --HIDDEN 100 --DATASET split --RATE 0.05\nTodo\nNetworks\nWe now have a fully working deep learning library with most of the features of a real industrial system like Torch. To take advantage of this hard\nwork, this module is entirely based on using the software framework. In particular, we are going to build an image recognition system. We will do\nthis by build the infrastructure for a version of LeNet on MNIST: a classic convolutional neural network (CNN) for digit recognition, and for a 1D\nconv for NLP sentiment classification.\nYou need the files from previous assignments, so maker sure to pull them over to your new repo. We recommend you to get familiar with tensor.py,\nsince you might find some of those functions useful for implementing this Module.\nGuides\nConvolution\nPooling\nSoftmax\nTask 4.1: 1D Convolution\nYou will implement the 1D convolution in Numba. This function gets used by the forward  and backward  pass of conv1d.\nComplete the following function in minitorch\/fast_conv.py , and pass tests marked as task4_1 .\nminitorch._tensor_conv1d(out: Tensor, out_shape: Shape, out_strides: Strides, out_size: int, input: Tensor, input_shape: Shape, \ninput_strides: Strides, weight: Tensor, weight_shape: Shape, weight_strides: Strides, reverse: bool) -> None\n1D Convolution implementation.\nGiven input tensor of\nbatch, in_channels, width\nand weight tensor\nout_channels, in_channels, k_width\nComputes padded output of\nbatch, out_channels, width\nreverse  decides if weight is anchored left (False) or right. (See diagrams)\nParameters:\nout ( Storage ) \u2013 storage for out  tensor.\nout_shape ( Shape ) \u2013 shape for out  tensor.\nout_strides ( Strides ) \u2013 strides for out  tensor.\nout_size ( int ) \u2013 size of the out  tensor.\ninput ( Storage ) \u2013 storage for input  tensor.\ninput_shape ( Shape ) \u2013 shape for input  tensor.\ninput_strides ( Strides ) \u2013 strides for input  tensor.\nweight ( Storage ) \u2013 storage for input  tensor.\nweight_shape ( Shape ) \u2013 shape for input  tensor.\nweight_strides ( Strides ) \u2013 strides for input  tensor.\nreverse ( bool ) \u2013 anchor weight at left or right\nTask 4.2: 2D Convolution\nTodo\nYou will implement the 2D convolution in Numba. This function gets used by the forward  and backward  pass of conv2d.\nComplete the following function in minitorch\/fast_conv.py , and pass tests marked as task4_2 .\nminitorch._tensor_conv2d(out: Tensor, out_shape: Shape, out_strides: Strides, out_size: int, input: Tensor, input_shape: Shape, \ninput_strides: Strides, weight: Tensor, weight_shape: Shape, weight_strides: Strides, reverse: bool) -> None\n2D Convolution implementation.\nGiven input tensor of\nbatch, in_channels, height, width\nand weight tensor\nout_channels, in_channels, k_height, k_width\nComputes padded output of\nbatch, out_channels, height, width\nReverse  decides if weight is anchored top-left (False) or bottom-right. (See diagrams)\nParameters:\nout ( Storage ) \u2013 storage for out  tensor.\nout_shape ( Shape ) \u2013 shape for out  tensor.\nout_strides ( Strides ) \u2013 strides for out  tensor.\nout_size ( int ) \u2013 size of the out  tensor.\ninput ( Storage ) \u2013 storage for input  tensor.\ninput_shape ( Shape ) \u2013 shape for input  tensor.\ninput_strides ( Strides ) \u2013 strides for input  tensor.\nweight ( Storage ) \u2013 storage for input  tensor.\nweight_shape ( Shape ) \u2013 shape for input  tensor.\nweight_strides ( Strides ) \u2013 strides for input  tensor.\nreverse ( bool ) \u2013 anchor weight at top-left or bottom-right\nTask 4.3: Pooling\nYou will implement 2D pooling on tensors with an average operation.\nComplete the following functions in minitorch\/nn.py , and pass tests marked as task4_3 .\nminitorch.tile(input: Tensor, kernel: Tuple[int, int]) -> Tuple[Tensor, int, int]\nReshape an image tensor for 2D pooling\nParameters:\ninput ( Tensor ) \u2013 batch x channel x height x width\nkernel ( Tuple[int, int] ) \u2013 height x width of pooling\nReturns:\nTuple[Tensor, int, int]  \u2013 Tensor of size batch x channel x new_height x new_width x (kernel_height * kernel_width) as well as the\nnew_height and new_width value.\nminitorch.avgpool2d(input: Tensor, kernel: Tuple[int, int]) -> Tensor\nTiled average pooling 2D\nTodo\nTodo\nParameters:\ninput \u2013 batch x channel x height x width\nkernel \u2013 height x width of pooling\nReturns:\nTensor  \u2013 Pooled tensor\nTask 4.4: Softmax and Dropout\nYou will implement max, softmax, and log softmax on tensors as well as the dropout and max-pooling operations.\nComplete the following functions in minitorch\/nn.py , and pass tests marked as task4_4 .\nAdd a property tests for the function in test\/test_nn.py  and ensure that you understand its gradient computation.\nminitorch.max(input: Tensor, dim: int) -> Tensor\nminitorch.softmax(input: Tensor, dim: int) -> Tensor\nCompute the softmax as a tensor.\nParameters:\ninput \u2013 input tensor\ndim \u2013 dimension to apply softmax\nReturns:\nTensor  \u2013 softmax tensor\nminitorch.logsoftmax(input: Tensor, dim: int) -> Tensor\nCompute the log of the softmax as a tensor.\nSee https:\/\/en.wikipedia.org\/wiki\/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\nParameters:\ninput \u2013 input tensor\ndim \u2013 dimension to apply log-softmax\nReturns:\nTensor  \u2013 log of softmax tensor\nminitorch.maxpool2d(input: Tensor, kernel: Tuple[int, int]) -> Tensor\nTiled max pooling 2D\nParameters:\ninput ( Tensor ) \u2013 batch x channel x height x width\nkernel ( Tuple[int, int] ) \u2013 height x width of pooling\nReturns:\nTensor( Tensor  ) \u2013 pooled tensor\nminitorch.dropout(input: Tensor, rate: float, ignore: bool = False) -> Tensor\nDropout positions based on random noise.\nParameters:\nTodo\nzi =\nexi\n\u2211i exi\nzi = xi \u2212log \u2211i exi\ninput \u2013 input tensor\nrate \u2013 probability [0, 1) of dropping out each position\nignore \u2013 skip dropout, i.e. do nothing at all\nReturns:\nTensor  \u2013 tensor with random positions dropped out\nTask 4.4b: Extra Credit\nImplementing convolution and pooling efficiently is critical for large-scale image recognition. However, both are a bit harder than some of the basic\nCUDA functions we have written so far. For this task, add an extra file cuda_conv.py  that implements conv1d  and conv2d  on CUDA. Show the\noutput on colab.\nTask 4.5: Training an Image Classifier\nIf your code works, you should now be able to move on to the NLP and CV training scripts in project\/run_sentiment.py  and\nproject\/run_mnist_multiclass.py . This script has the same basic training setup as :doc: module3 , but now adapted to sentiment and image\nclassification. You need to implement Conv1D , Conv2D , and Network  for both files.\nWe recommend running on the command line when testing. But you can also use the Streamlit visualization to view hidden states of your model,\nlike the following:\nTrain a model on Sentiment (SST2), and add your training printout logs as a text file sentiment.txt  to the repo. It should show train loss, train\naccuracy and validation accuracy. (The model should get to >70% best validation accuracy.)\nTrain a model on Digit classification (MNIST) logs as a text file mnist.txt  to the repo. It should show train loss and validation accuracy out of 16\nTodo\nContributing\nThe Minitorch codebase is structured to mimic the experience of contributing to a real open-source project. It is not sufficient to implement\nfunctions correctly; the code itself needs to meet the specific contributor requirements. These are checked by the system automatically before\nacceptance.\nStyle\nIt is required to keep your code organized and clean to make it easier to debug, optimize, and document. To help with this process, we utilize\nrequired formatting on all assignments.\nFixing style bugs can be an annoying process. However, there are now tools to fix most formatting issues automatically. We use black  to\nautomatically reformat all of your code to fit most of the requirements (see psf\/black for more details):\nBlack will fix many of your issues, but cannot check for aspects like using unknown variables. You will also need to run the flake8  linter in your\ndirectory to check for remaining issues:\nWe recommend setting up your editor or IDE to highlight other style issues. Many developers utilize VSCode with plugins to check for these issues\nas they code.\nTesting\nEach assignment has a series of tests that require your code to pass. These tests are in the tests\/  directory and are in the pytest  format\n(https:\/\/docs.pytest.org\/en\/stable\/). Any function in that directory starting with test  is run as part of the test suite.\nEach assignment has 4 task groups that you will need to pass. To run individual task groups you can use the -m  option.\nIn addition to running a full task which runs all of the tests, you can run tests in a single file with:\nOr even a particular test with:\nNote: PyTest will hide all print statements unless a test fails. If you want to see output for a given tests you may need to cause an assertion failure.\nType Checking\nModern versions of Python allow for static type checking to ensure that functions take and returns objects of the correct type. The Minitorch code\nbase is fully annoted with typed on each function. Users will not have to provide types, but they provide documentation as to what functions\nexpect as arguments and as for their return values. For example, if we are writing a function that does multiplication it would have the following\nsignature.\nAs we get to more complex topics the type signatures will get more complex as well. For example, Iterable  is use to represent a type that can be\niterated over.\n>>> black .\n>>> flake8\n>>> pytest\n>>> pytest -m task0_0\n>>> pytest tests\/test_operators.py\n>>> pytest tests\/test_operators.py -k test_sum\ndef mul(x: float, y: float) -> float:\n    ...\ndef negList(ls: Iterable[float]) -> Iterable[float]:\n    ...\nIn order to check that the code matches the types, the project requires that you use the mypy  library. This is run as part of the style check.\nDocumentation\nThroughout the codebase, we require to document all functions in a standardized style. Documentation is critical for our Python codebase, and we\nuse it to convey requirements on many functions. Functions should have docstrings in the following form (known as Google docstyle):\nA full description of this docstyle is listed here https:\/\/sphinxcontrib-napoleon.readthedocs.io\/en\/latest\/example_google.html.\nThe project also requires that you keep documentation up-to-standard throughout. Lint errors will be thrown by flake8  if your documentation is in\nthe incorrect format.\nPre-Commit\nThese elements can be checked automatically through a tool known as pre-commit. Using the precommit tool is optional but will likely save you\ntime in your coding process and ensure that you are not making too many unnecessary failed pull requests. You can install the tool by running,\nYou can run all the checks (black, flake8, mypy, etc) and corrections on your code with the following command:\nYou can also 'install' pre-commits which will run on every commit automatically, and prevent you from committing bad code.\nContinuous Integration (CI)\nIn addition to local testing, the project is set up such that on each code push, tests are automatically run and checked on the server. You are able to\nsee how well you are doing on the assignment by committing your code, pushing to the server, and then logging in to GitHub. This process takes\nseveral minutes, but it is an easy way to keep track of your progress as you go.\nSpecifically, you can run:\nThen go to your GitHub and click on \"Pull requests\". Clicking on the request itself gives a link to show the current progress of your work.\n  def index(ls: List[Any], i: int) -> Any:\n      \"\"\"\n      List indexing.\n      Args:\n          ls: A list of any type.\n          i: An index into the list\n      Returns:\n          Value at ls[i].\n      \"\"\"\n      ...\npip install pre-commit\npre-commit run --all\npre-commit install\n>>> git commit -am \"First commit\"\n>>> git push origin master\nProperty Testing\nTesting and debugging are critical for software engineering in general, and particularly necessary for framework code that will be used in\nunexpected ways. Unfortunately, machine learning code is notoriously hard to test and debug. Many practitioners seem to just run models and\nwait until they are trained before starting the debugging process.\nBut how do you effectively test mathematically oriented code? Unlike many software projects where unit tests can cover most of the input\ncases, mathematical functions make this impossible.\nFor example, let's say you have a function that is meant to add two numbers (this sounds really silly, but we will see it is not)\nA (somewhat naive) unit test might look like this::\nThis is fine, and certainly can help catch easy bugs, but it is not very reassuring. It is particularly devastating when your code has been running\nfor 20 hours, and then encounters some cases where your add function fails.\nAn alternative idea is to test properties instead of specific cases. That is, check if key aspects of the expected behavior always hold. For\ninstance, you might imagine directly checking if these properties hold for every pair of integers:\nThis provides better coverage, but is also naive and clearly hopelessly inefficient. Unit tests are supposed to be quick easy snippets of code that\ncan be run quickly while developing.\nA clever middle ground is to use randomized  property checking. This method was popularized by a library called QuickCheck\n(http:\/\/wikipedia.org\/wiki\/quickcheck). This approach randomly selects interesting inputs in order to test your codebase's correctness. It gives\nyou the speed of the first approach and some of the breadth of the second. Another nice benefit of randomized property checking is that it\nactually makes tests shorter and easier to write since it generates cases for you.\nIn MiniTorch, we will use a property checking library in Python called Hypothesis  (https:\/\/hypothesis.readthedocs.io\/). Hypothesis predefines a\nwhole set of building block strategies that the user can pick from when writing tests. (You can also write your strategies, which you will do in the\nfrom hypothesis import example, given\nfrom hypothesis.strategies import integers\ndef my_add(a: int, b: int) -> int:\n    \"A customized integer addition function.\"\n    out = a\n    for _ in range(-b):\n        out -= 1\n    for _ in range(b):\n        out += 1\n    return out\ndef test_add_basic():\n    # Check same as slow system add\n    assert my_add(10, 7) == 10 + 7\n    # Check that order doesn't matter\n    assert my_add(10, 7) == my_add(7, 10)\ntest_add_basic()\ndef test_add_naive():\n    for a in range(-100, 100):\n        for b in range(-100, 100):\n            assert my_add(a, b) == a + b\n            assert my_add(a, b) == my_add(b, a)\nnext assignment.) You can generate integers, floats, lists, strings, etc. Each test can be decorated with values that it operates on::\nThe function integers  is a strategy function that tells us what type of values to test on. It is a function because we may want to constrain the\nvalues in various ways. When debugging we can force it to give us some values.\nIt is also easy to combine randomized testing with example based testing. This can be useful if you want to create easy to debug test cases.\n@given(integers(), integers())\ndef test_add(a, b):\n    # Check same as slow system add\n    assert my_add(a, b) == a + b\n    # Check that order doesn't matter\n    assert my_add(a, b) == my_add(b, a)\nintegers(min_value=0, max_value=10).example()\n10\n@given(integers(), integers())\n@example(5, 7)\ndef test_add2(a, b):\n    # Check same as slow system add\n    assert my_add(a, b) == a + b\n    # Check that order doesn't matter\n    assert my_add(a, b) == my_add(b, a)\nModules\nResearchers disagree on exactly what the term deep learning means, but one aspect that everyone agrees on is that deep models are big and\ncomplex. Models can include hundreds of millions of learned parameters . In order to work with such complex systems, it is important to have\ndata structures which abstract away the complexity so that it is easier to access and manipulate specific components, and group together\nshared regions. These structures are not rigorous mathematical objects, but a convenient way of managing complex systems.\nOn the programming side, Modules  have become a popular paradigm to group parameters together to make them easy to manage, access, and\naddress. There is nothing specific to machine learning about this setup (and everything in MiniTorch could be done without modules), but they\nmake life easier and code more organized. For now, do not worry about what parameters are for, just that we would like to group and name\nthem in a convenient way.\nIn Torch, modules are a hierarchical data structure. Each module stores three things: 1) parameters, 2) user data, 3) other modules. Internally,\nthe user interacts with each of these on self , but under the hood the module sorts everything into three types.\nLet's work through an example of module.\nInternally Module partitions these elements. Parameters (type 1) are stored in a parameters dictionary, user data (type 2) is stored on self ,\nmodules (type 3) are stored in a modules dictionary. This is a bit tricky. Python is a very dynamic language and allows us to override simple\nthings like assignment.\nBe careful. All subclasses must begin their initialization by calling super().__init__() . This line allows the module to capture any members of\ntype Module  or Parameter .\nThe benefit of this behavior is that it allows us to easily extract all parameters and subparameters from modules. Specifically we can get out all\nof a modules parameters using the named_parameters  function. This function returns a dictionary of all of the parameters in the module and in\nall descendent sub-modules.\nThe names here refer to the keys in the dictionary which give the path to each parameter in the tree (similar to python dot notation). Critically\nthis function does not just return the current module's parameters, but recursively collects parameters from all the modules below as well.\nHere is an example of how you can create a tree of modules and then extract the flattened parameters\nclass OtherModule(Module):\n    pass\nclass MyModule(Module):\n    def __init__(self):\n        # Must initialize the super class!\n        super().__init__()\n        # Type 1, a parameter.\n        self.parameter1 = Parameter(15)\n        # Type 2, user data\n        self.data = 25\n        # Type 3. another Module\n        self.sub_module = OtherModule()\nMyModule().named_parameters()\n[('parameter1', 15)]\nclass Module1(Module):\n    def __init__(self):\n        super().__init__()\n        self.p1 = Parameter(5)\n        self.a = Module2()\n        self.b = Module3()\nModules can also be used to find all submodules.\nAdditionally, a module can have a mode  indicating how it is currently operated. The mode should propagate to all of its child modules. For\nsimplicity, we only consider the train and eval mode.\nclass Module2(Module):\n    def __init__(self):\n        super().__init__()\n        self.p2 = Parameter(10)\nclass Module3(Module):\n    def __init__(self):\n        super().__init__()\n        self.c = Module4()\nclass Module4(Module):\n    def __init__(self):\n        super().__init__()\n        self.p3 = Parameter(15)\nModule1().named_parameters()\n[('p1', 5), ('a.p2', 10), ('b.c.p3', 15)]\nModule1().modules()\n[Module2(),\n Module3(\n   (c): Module4()\n )]\nmodule1 = Module1()\nmodule1.train()\nmodule1.training\nTrue\nmodule1.a.training\nTrue\nmodule1.eval()\nmodule1.training\nFalse\nFunctional\nExternally, MiniTorch supports the standard Torch API, which allows Python users to develop in a standard Python coding style. Internally, the\nlibrary uses a functional-style. This approach is preferred for two reasons: first, it makes it easy to test, and secondly it makes it easy to\noptimize. While this style requires a bit of extra thought to understand, it has some benefits.\nPrimarily we will use the functional style to define higher-order, functions. These are functions that take functions as arguments and return\nfunctions as results. Python defines a special type for these: Callable .\nAny function can be turned into a variable of type callable. (Although this in itself is not very interesting).\nIt is interesting though, to pass functions as arguments to other functions. For example, we can pass a callable to a function that uses it.\nWe can also use this approach to create new functions as reutrn arguments.\nAs an extended example, let's create a higher-order version of the filter function. Filter should take a list and return only the values that are true\nunder a function.\nfrom typing import Callable, Iterable\ndef add(a: float, b: float) -> float:\n    return a + b\nv: Callable[[float, float], float] = add\ndef mul(a: float, b: float) -> float:\n    return a * b\nv: Callable[[float, float], float] = mul\ndef combine3(\n    fn: Callable[[float, float], float], a: float, b: float, c: float\n) -> float:\n    return fn(fn(a, b), c)\nprint(combine3(add, 1, 3, 5))\nprint(combine3(mul, 1, 3, 5))\n9\n15\ndef combine3(fn):\n    def new_fn(a: float, b: float, c: float) -> float:\n        return fn(fn(a, b), c)\n    return new_fn\nadd3: Callable[[float, float, float], float] = combine3(add)\nmul3: Callable[[float, float, float], float] = combine3(mul)\nprint(add3(1, 3, 5))\n9\nWe then use this to create a new function.\nFunctional programming can be elegant, but also hard. When in doubt remember that you can always write things out in a simpler form first and\nthen check that you get the same sensible result.\ndef filter(fn: Callable[[float], bool]) -> Callable[[Iterable[float]], Iterable[float]]:\n    def apply(ls: Iterable[float]):\n        ret = []\n        for x in ls:\n            if fn(x):\n                ret.append(x)\n        return ret\n    return apply\ndef more_than_4(x: float) -> bool:\n    return x > 4\nfilter_for_more_than_4 = filter(more_than_4)\nfilter_for_more_than_4([1, 10, 3, 5])\n[10, 5]\nVisualization\nWhile testing is nice for maintaining correctness, exploratory analysis is also critical for gaining intuition. When you are stuck, often the best thing\nto do is to just look at your data and outputs. Visualizing our system can't prove that it is correct, but it can often directly help us to figure out what\ngoes wrong. Throughout our development, we will use visualization to observe intermediate states, training progress, outputs, and even final\nmodels.\nThe main library we will use is called Streamlit (streamlit\/streamlit).\nYou can think of it as sending images and graphs from your code to a centralized, organized place. Nothing that magical, we could just output\nthem to a directory, but we will see this has some nice benefits.\nTo start streamlit, you need to run the following command in your virtual env.\nNext, open up a browser window and, go to http:\/\/localhost:8501 (or whichever port it starts on).\nEach unit will include a set of different sandbox elements that you can use and visualize to explore your underlying problem. You can use these to\nhelp you debug as well as plot functions that go directly to this board.\nThere is a lot more you can do with Streamlit. Check out [streamlit] (https:\/\/docs.streamlit.io\/en\/stable\/) for a list of goodies.\n>>> streamlit run app.py -- {{module number}}\nLoading [Contrib]\/a11y\/accessibility-menu.js\nLoading [Contrib]\/a11y\/accessibility-menu.js\nDerivatives\nLet's begin by discussing derivatives in the setting of programming. We assume that you have not seen derivatives in a while, so we will start slow and develop some notation first.\nSymbolic Derivatives\nAssume we are given a function, \n. We can compute a function for its derivative by applying rules from univariate calculus. We will use Lagrange notation where the\nderivative of a one-argument function \n as is denoted \n. To compute \n we can apply standard univariate derivative rules.\nWe also will work with two-argument functions.\nimport math\nf(x) = sin(2 \u00d7 x)\nf \u2032\nf \u2032\nf \u2032\nf \u2032(x) = 2 \u00d7 cos(2x)\ndef f(x):\n    return math.sin(2 * x)\ndef d_f(x):\n    return 2 * math.cos(2 * x)\nplot_function(\"f(x) = sin(2x)\", f)\nplot_function(\"f'(x) = cos(2x)\", d_f)\n\u22124\n\u22122\n0\n2\n4\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n2\nf'(x) = cos(2x)\nf(x, y) = sin(x) + 2 cos(y)\ndef f(x, y):\n    return math.sin(x) + 2 * math.cos(y)\nplot_function3D(\"f(x, y) = sin(x) + 2 * cos(y)\", f)\n3\nf(x, y) = sin(x) + 2 * cos(y)\nWe use a subscript notation to indicate which argument we are taking a derivative with respect to.\nIn general, we will refer to this process of mathematical transformation as the symbolic derivative of the function. When available symbolic derivatives are ideal, they tell us everything we need\nto know about the derivative of the function.\nNumerical Derivatives\nVisually, derivative functions correspond to slopes of tangent lines in 2D. Let's start with this simple function:\n\u22122\n\u22121\n0\n1\n2\n3\ndef d_f_x(x, y):\n    return math.cos(x)\nplot_function3D(\"f'_x(x, y) = cos(x)\", d_f_x)\n\u22120.5\n0\n0.5\n1\nf'_x(x, y) = cos(x)\nf(x) = x2 + 1\ndef f(x):\n    return x * x + 1.0\nplot_function(\"f(x)\", f)\n\u22124\n\u22122\n0\n2\n4\n0\n5\n10\n15\n20\n25\nf(x)\nIts derivative at an arbitrary input is the slope of the line tangent to that input.\nThe above visual representation motivates an alternative approach to estimate a [numerical]{.title-ref} derivative. The underlying assumption is that we assume we do not know the symbolic\nform of the function, and instead want to estimate it by querying specific values.\nRecall one definition of the derivative function is this slope as we approach to a tangent line:\nIf we set \n to be very small, we get an approximation of the derivative function:\nAlternatively, you could imagine approaching x from the other side, which would yield a different derivative function:\nYou can show that doing both simultaneously yields a better approximation (you probably proved this in high school!):\nimage\n{.align-center}\nThis formula is known as the central difference, and is a specific case of finite differences.\nWhen working with functions of multiple arguments. The each derivative corresponds to the slope of one dimension of the tangent plane. The central difference approach can only tell us one\nof these slope at a time.\nThe more variables we have, the more function calls we need to make.\ndef d_f(x):\n    return 2 * x\ndef tangent_line(slope, x, y):\n    def line(x_):\n        return slope * (x_ - x) + y\n    return line\nplot_function(\"f(x) vs f'(2)\", f, fn2=tangent_line(d_f(2), 2, f(2)))\n\u22124\n\u22122\n0\n2\n4\n\u221220\n\u221210\n0\n10\n20\ntrace 0\ntrace 1\nf(x) vs f'(2)\nepsilon\neps = 1e-5\nslope = (f(2 + eps) - f(2 - eps)) \/ (2 * eps)\nplot_function(\"f(x) vs f'(2)\", f, fn2=tangent_line(slope, 2, f(2)))\nImplementing Numerical Approximations\nThe key benefit of the [numerical]{.title-ref} approach is that we do not need to know everything about the function: all we need is to be able to compute its value under a given input. From a\nprogramming sense, this means we can approximate the derivative for any black-box function. Note, we did not need to actually know the specifics of the function to compute this derivative.\nIn implementation, it means we can write a [higher-order function]{.title-ref} of the following form:\nAssume we are just given an arbitrary python function:\nwe can call central_difference(f, x)  to immediately approximate the derivative of this function f on input x.\nWe will see that this approach is not a great way to train machine learning models, but it provides a generic alternative approach to check if your derivative functions are correct, e.g. free\nproperty testing.\nHere are some examples of Module-0 functions with central difference applied. It is important to know what derivatives of these important functions look like.\ndef central_difference(f: Callable[[float], float], x: float) -> float:\n    ...\ndef f(x: float) -> float:\n    \"Compute some unknown function of x.\"\n    ...\nplot_function(\"sigmoid\", minitorch.operators.sigmoid)\n\u22124\n\u22122\n0\n2\n4\n0\n0.2\n0.4\n0.6\n0.8\n1\nsigmoid\ndef d_sigmoid(x):\n    return minitorch.central_difference(minitorch.operators.sigmoid, x)\nplot_function(\"Derivative of sigmoid\", d_sigmoid)\n\u22124\n\u22122\n0\n2\n4\n0\n0.05\n0.1\n0.15\n0.2\n0.25\nDerivative of sigmoid\nplot_function(\"exp\", minitorch.operators.exp)\n120\n140\nexp\n\u22124\n\u22122\n0\n2\n4\n0\n20\n40\n60\n80\n100\ndef d_exp(x):\n    return minitorch.central_difference(minitorch.operators.exp, x)\nplot_function(\"Derivative of exp\", d_exp)\n\u22124\n\u22122\n0\n2\n4\n0\n20\n40\n60\n80\n100\n120\n140\nDerivative of exp\nplot_function(\"ReLU\", minitorch.operators.relu)\n\u22124\n\u22122\n0\n2\n4\n0\n1\n2\n3\n4\n5\nReLU\ndef d_relu(x):\n    return minitorch.central_difference(minitorch.operators.relu, x)\nplot_function(\"Derivative of ReLU\", d_relu)\n0.2\n0.4\n0.6\n0.8\n1\nDerivative of ReLU\n\u22124\n\u22122\n0\n2\n4\n0\nScalar\nIn last section, we discussed two ways to compute derivatives. Symbolic derivatives require access to the full symbolic function, whereas\nnumerical derivatives require only a black-box function. The first is precise but rigid, whereas the second is imprecise but more flexible. This\nmodule introduces a third approach known as autodifferentiation which is a tradeoff between symbolic and numerical methods.\nAutodifferentiation works by collecting information about the computation path used within the function, and then transforming this\ninformation into a procedure for computing derivatives. Unlike the black-box method, autodifferentiation will allow us to use this information to\ncompute each step more precisely.\nHowever, in order to collect the information about the computation path, we need to track the internal computation of the function. This can be\nhard to do since Python does not expose how its inputs are used in the function directly: all we get is the output only. This doc describes one\nmethod for tracking computation.\nOverriding Numbers\nSince we do not have access to the underlying language interpreter, we are going to build a system to track the mathematical operations applied\nto each number.\n1. Replace all numbers with proxy a class, which we will call Scalar .\n2. Replace all mathematical functions with proxy operators.\n3. Remember what operators were applied to each Scalar.\nConsider the following code which shows the result of this approach.\nScalar should behave exactly like numbers. The goal is that the user cannot tell the difference. But we will utilize the extra information to\nimplement the operations we need.\nFunctions\nWhen working with these new number we restrict ourselves to use a small set of mathematical functions  of one or two arguments.\nGraphically, we will think of functions as little boxes. For example, a one-argument function would look like this,\nInternally, the box unwraps the content of , manipulates it, and returns a new value with the saved history. We can chain together two of these\nfunctions to produce more complex functions.\nx1 = minitorch.Scalar(10)\nx2 = minitorch.Scalar(30)\ny = x1 + x2\ny.history\nScalarHistory(last_fn=<class 'minitorch.scalar_functions.Add'>, ctx=Context(no_grad=False, saved_values=\n()), inputs=[Scalar(10.000000), Scalar(30.000000)])\nf\nx\nSimilarly, a two-argument function unwraps the content of both inputs  and , manipulates them, and returns a new wrapped version:\nFinally we can create more complex functions that chain these together in various ways.\nImplementation\nWe will implement tracking using the Scalar  class. It wraps a single number (which is stored as an attribute) and its history.\nTo implement functions there is a corresponding class ScalarFunction . We will need to reimplement each mathematical function that we would\nlike to use by inheriting from this class.\nFor example, say our function is Neg , \nOr, say the function is Mul , \n that multiplies x by y\nx\ny\nx = minitorch.Scalar(10)\ng(x) = \u2212x\nclass Neg(ScalarFunction):\n    @staticmethod\n    def forward(ctx, x):\n        return -x\nf(x, y) = x \u00d7 y\nWithin the forward function,  and  are always unwrapped numbers. Forward function processes and returns unwrapped values.\nIf we have scalars \n, we can apply the above function by\nNote, that we do not call forward directly but instead apply. Internally 'apply' converts the inputs to standard numbers to call forward, and then\nwraps the output float with the history it needs.\nHere out  has remembered the graph that led to its creation.\nMinitorch includes a library to allow you to draw these box diagrams for arbitrarily complex functions.\nout\nNeg (Op 0)\n5\nNeg (Op 1)\n4\nout\nMul (Op 0)\n4\nNeg (Op 1)\n5\nOperators\nThere is still one minor issue. This is what our code looks like to use [Mul]{.title-ref},\nclass Mul(ScalarFunction):\n    @staticmethod\n    def forward(ctx: Context, x: float, y: float) -> float:\n        return x * y\nx\ny\nx, y\nz = Neg.apply(x)\nout = Neg.apply(z)\n# or\nout2 = Mul.apply(x, z)\nprint(out.history)\nScalarHistory(last_fn=<class '__main__.Neg'>, ctx=Context(no_grad=False, saved_values=()), inputs=[Scalar(-\n10.000000)])\nIt is a bit annoying to write code this way. Also, we promised that we would have functions that look just like the Python operators we are used\nto writing.\nTo get around this issue, we need to augment the Scalar  class so that it can behave normally under standard mathematical operations.\nInstead of calling regular multiplication, Python will call our mul . Once this is achieved, we will have the ability to record and track how  is used\nin the Function, while still being able to write\nTo achieve this, the class needs to provide syntax that makes it appear like a number when in use. You can read emulating numeric types to\nlearn how this could be done.\nout2 = Mul.apply(x, y)\nx\nout2 = x * y\nAutodifferentiation\nWe are going to utilize the computation graph as a way to automatically compute derivatives of arbitrary python functions. The trick behind this\nautodifferentiation is to implement the derivative of each invidual function, and then utilize the chain rule to compute a derivative for any scale\nvalue.\nBackward\nOur computation graph was made up of individual atomic functions \n. For each of these functions we are now going to implement a\nbackward method to provide this local derivative information.\nThe API for backward is to compute \n where \n is the derivative of the function and  is a value passed to backward (discussed more\nbelow).\nFor a simple function \n, we can consult our derivative rules and get \n. Therefore the backward is,\nNote that backward works a bit different than the mathematical notation. Sometimes the function for the derivative \n depends directly on x;\nhowever, backward does not take  as an argument. This is where the context arguments ctx  comes in.\nConsider a function Sin, \n which has derivative \n. We need to write it in code as,\nf(x)\nd \u22c5f \u2032(x)\nf \u2032(x)\nd\nf(x) = \u2212x\nf \u2032(x) = \u22121\nclass Neg(ScalarFunction):\n    @staticmethod\n    def forward(ctx, x):\n        return -x\n    @staticmethod\n    def backward(ctx, d):\n        f_prime = -1\n        return f_prime * d\nf \u2032(x)\nx\nf(x) = sin(x)\nf \u2032(x) = cos(x)\nclass Sin(ScalarFunction):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return math.sin(x)\n    @staticmethod\n    def backward(ctx, d):\n        (x,) = ctx.saved_values\n        f_prime = math.cos(x)\n        return f_prime * d\nFor functions that take multiple arguments, backward returns derivatives with respect to each input argument. For example, if the function\ncomputes \n, we need to return \n and \nChain Rule\nThis section discusses implementation of the chain rule for univariate differentiation. Before reading, review the mathematical definition of\nChain Rule.\nComputing backward gives a way to compute the derivative for simple functions, but what if we have more complex functions? Let\\'s go\nthrough each of the different cases to compute the derivatives.\nOne argument\nTwo argument\nSame argument\nOne argument\nLet us say that we have a complex function \n. We want to compute \n. For simplicity we use \n, and draw  as two\nboxes left to right.\ndef d_call(x):\n    ctx = minitorch.Context()\n    Sin.forward(ctx, x)\n    return Sin.backward(ctx, 1)\nplot_function(\"f(x) = sin(x)\", lambda x: Sin.apply(x).data)\nplot_function(\"1 * f'(x) = cos(x)\", d_call)\nf(x, y)\nf \u2032\nx(x, y)\nf \u2032\ny(x, y)\nclass Mul(ScalarFunction):\n    @staticmethod\n    def forward(ctx, x, y):\n        ctx.save_for_backward(x, y)\n        return x * y\n    @staticmethod\n    def backward(ctx, d):\n        # Compute f'_x(x, y) * d, f'_y(x, y) * d\n        x, y = ctx.saved_values\n        f_x_prime = y\n        f_y_prime = x\n        return f_x_prime * d, f_y_prime * d\nh(x) = f(g(x))\nh\u2032(x)\nz = g(x)\nh\nThe chain rule tell us how to compute this term. Specifically it gives the following formula.\nThe above derivative function tells us to compute the derivative of the right-most function ( ), and then multiply it by the derivative of the left\nfunction ( ).\nHere is where the perspective of thinking of functions as boxes pays off. We simply reverse the order.\nThe  multiplier passed to backward of the first box (left) should be the value returned by [backward]{.title-ref} of the second box. The 1 at the\nend is to start off the chain rule process with a value for \n.\nTwo arguments\nNext is the case of a two argument function. We will write this as \n where \n.\nApplying the chain rule we get the following equations.\nDrawing this again with boxes.\nd = 1 \u22c5f \u2032(z)\nh\u2032\nx(x) = d \u22c5g\u2032(x)\nf\ng\nd\ndout\nh(x, y) = f(g(x, y))\nz = g(x, y)\nd = 1 \u22c5f \u2032(z)\nh\u2032\nx(x, y) = d \u22c5g\u2032\nx(x, y)\nh\u2032\ny(x, y) = d \u22c5g\u2032\ny(x, y)\nNote that this shows that the second box ( ) does not care how many arguments the first box ( ) has, as long as it passes back  which is\nenough for the chain rule to work.\nMultiple Uses\nFinally, what happens when 1 value is used by two future boxes? Next is the case of a two argument function. We will write this as\n where \n.\nDerivatives are linear, so the  term that comes from the second box is just the sum of the two individual derivatives.\nSpecifically in terms of boxes, this means that if an output is used multiple times, we should sum together the derivative terms. This rule is\nimportant, as it means that we cannot call backward until we have aggregated together all the values that we need to calculate .\nf\ng\nd\nh(x) = f(z1, z2)\nz1 = z2 = g(x)\nd\nd = 1 \u22c5f \u2032\nz1(z1, z2) + 1 \u22c5f \u2032\nz2(z1, z2)\nh\u2032\nx(x) = d \u22c5g\u2032\nx(x)\nd\nBackpropagate\nThe backward  function tells us how to compute the derivative of one operation. The chain rule tells us how to compute the derivative of two\nsequential operations. In this section, we show how to use these to compute the derivative for an arbitrary series of operations.\nPractically this looks like re-running our graph in reverse order from right-to-left. However, we need to ensure that we do this in the correct order.\nThe key implementation challenge of backpropagation is to make sure that we process each node in the correct order, i.e. we have first\nprocessed every node that uses a Variable before that varible itself.\nRunning Example\nAssume we have Variables \n and a Function \n. We want to compute the derivatives \n and \nWe assume x, +, log, and exp are all implemented as simple functions. This means that the final output Variable has constructed a graph of its\nhistory that looks like this:\nHere, starting from the left, the first arrows represent inputs \n, the left node outputs , the top node \n, the bottom node \n and the\nfinal right node \n. Forward computation proceeds left-to-right.\nThe chain rule tells us methods for propagating the derivatives. We can use the rules from the previous section right-to-left until we reach the\ninitial Variables \n, i.e. the leaf  Variables.\nTopological Sort\nWe could just apply these rules randomly and process each nodes as they come aggregating the resulted values. However this can be quite\ninefficient. It is better to wait to call backward  until we have accumulated all the values we will need.\nTo handle this issue, we will process the nodes in topological\norder . We first note that our graph is directed and that acyclic. Directionality comes from the backward  function, and the lack of cycles is a\nconsequence of the choice that every Function must create a new variable.\nThe topological ordering of a directed acyclic graph is an ordering that ensures no node is processed after its ancestor, e.g. in our example that\nthe left node cannot be processed before the top or bottom node. The ordering may not be unique, and it does not tell us whether to process the\ntop or bottom node first.\nx, y\nh(x, y)\nh\u2032\nx(x, y)\nh\u2032\ny(x, y)\nh(x, y) = log(z) + exp(z)\nwhere\nz = x \u00d7 y\nx, y\nz\nlog(z)\nexp(z)\nh(x, y)\nx, y\nThere are several easy-to-implement algorithms for topological sorting. As graph algorithms are beyond the scope of this document, we\nrecommend using the depth-first search algorithm described in pseudocode section of Topological Sorting.\nBackprop\nOnce we have the order defined, we process each node one at a time in order. We start the rightmost node (\n) with red arrow in the graph\nbelow. The starting derivative is an argument given to us.\nWe then process the Function with the chain rule. This calls backward  of +, and gives the derivative for the two red Variables (which correspond\nto \n from the forward  pass). You need to track these intermediate red derivative values in a dictionary.\nLet us assume the next Variable in the order is the top node. We have just computed and stored the necessary derivative \n, so we can apply\nthe chain rule. This produces a new derivative (corresponding to : left red arrow below) for us to store.\nThe next Variable in the order is the bottom node. Here we have an interesting result. We have a new arrow, but it corresponds to the same\nVariable ( ) that we just computed. It is is a useful exercise to show that as a consequence of the two argument chain rule that the derivative\nfor this Variable is the sum of each of these derivatives. Practically this means just adding it to your dictionary.\nh(x, y)\nlog(z), exp(z)\ndout\nz\nz\nAfter working on this Variable, at this point, all that is left in the is our input leaf Variables.\nWhen we reach the leaf variables in our order, for example , we store the derivative with that variable. Since each step of this process is an\napplication of the chain rule, we can show that this final value is \n. The next and last step is to compute \n.\nBy convention, the variables \n have their derivatives stored as::\nAlgorithm\nAs illustrated in the graph for the above example, each of the red arrows represents a constructed derivative which eventually passed to \n in\nthe chain rule. Starting from the rightmost arrow, which is passed in as an argument, backpropagate should run the following algorithm:\n1. Call topological sort to get an ordered queue\n2. Create a dictionary of Scalars and current derivatives\n3. For each node in backward order, pull a completed Scalar and derivative from the queue: a. if the Scalar is a leaf, add its final derivative\n( accumulate_derivative ) and loop to (1) b. if the Scalar is not a leaf, 1) call .chain_rule  on the last function with \n 2) loop through all\nthe Scalars+derivative produced by the chain rule 3) accumulate derivatives for the Scalar in a dictionary\nFinal note: only leaf Scalars should ever have non-None .derivative  value. All intermediate Scalars should only keep their current derivative\nvalues in the dictionary. This is a bit annoying, but it follows the behavior of PyTorch.\nx\nh\u2032\nx(x, y)\nh\u2032\ny(x, y)\nx, y\nx.derivative, y.derivative\ndout\ndout\nTensors\nTensor is a fancy name for a simple concept. A tensor is a multi-dimensional array of arbitrary dimensions. It is a convenient and efficient way to\nhold data, which becomes much more powerful when paired with fast operators and autodifferentiation.\nTensor Shapes\nSo far we have focused on scalars, which correspond to 0-dimensional tensors. Next, we consider a 1-dimensional tensor (vector):\nThen a 2-dimensional tensor (matrix):\nIn addition to its dimension ( dims ), other critical aspects of a tensor are its shape  and size . The shape of the above vector is (5,)) and its size\n(i.e. number of squares in the graph) is 5. The shape of the above matrix is (4,4) and its size is 20.\nA three-dimensional tensor with shape (2, 3, 3) and size 18 looks like this:\nWe access an element of the tensor by tensor index notation: tensor[i]  for 1-dimension, tensor[i, j]  for 2-dimension, tensor[i, j, k]  for\n3-dimension, and so forth. For example, tensor[0, 1, 2]  would give this blue cube:\nTypically, we access tensors just like multi-dimensional arrays, but there are some special geometric properties that make tensors different.\nFirst, tensors make it easy to change the order of the dimensions. For example, we can transpose  the dimensions of a matrix. For a general\ntensor, we refer to this operation as permute . Calling permute  arbitrarily reorders the dimensions of the input tensor. For example, as shown\nbelow, calling permute(1,0)  on a matrix of shape (2, 5) gives a matrix of shape (5, 2). For indexing into the permuted matrix, we access\nelements using tensor[j, i]  instead of tensor[i, j] .\nSecond, tensors make it really easy to add or remove additional dimensions. Note that a matrix of shape (5, 2) can store the same amount of\ndata as a matrix of shape (1, 5, 2), so they have the same size as shown below:\nWe would like to easily increase or decrease the dimension of a tensor without changing the data. We will do this with a view  function: use\nview(1, 5, 2)  for the above example. Element tensor[i, j]  in the (5,2) matrix is now tensor[0, i, j]  in the 3-dimensional tensor.\nCritically, neither of these operations changes anything about the input tensor itself. Both view  and permute  are tensor tricks , i.e.\noperations that only modify how we look at the tensor, but not any of its data. Another way to say this is that they do not move or copy the data\nin any way, but only the external tensor wrapper.\nTensor Strides\nUsers of a Tensor library only have to be aware of the shape  and size  of a tensor. However, there are important implementation details that we\nneed to keep track of. To make our code a bit cleaner, we need to separate out the internal tensor data  from the user-facing tensor. In addition\nto the shape , :class: minitorch.TensorData  manages tensor storage  and strides :\nStorage is where the core data of the tensor is kept. It is always a 1-D array of numbers of length size , no matter the dimensionality or\nshape  of the tensor. Keeping a 1-D storage allows us to have tensors with different shapes point to the same type of underlying data.\nStrides is a tuple that provides the mapping from user indexing to the position in the 1-D storage .\nStrides  can get a bit confusing to think about, so let's go over an example. Consider a matrix of shape (5, 2). The standard mapping is to walk\nleft-to-right, top-to-bottom to order this matrix to the 1-D storage :\nWe call it contiguous  mapping, since it is in the natural counting order (bigger strides left). Here the strides are \n. We read this as each\ncolumn moves 1 step in storage and each row moves 2 steps. We can have different strides for the same shape. For instance, if we were walking\ntop-to-bottom, left-to-right, we would have the following stride map:\nContiguous strides are generally preferred, but non-contiguous strides can be quite useful as well. Consider transposing the above matrix and\nusing strides (1,2):\nIt has new strides (1,2) and new shape (2,5), in contrast to the previous (2,1) stride map on the (5,2) matrix. But notably no change in the\nstorage . This is one of the super powers of tensors mentioned above: we can easily manipulate how we view the same underlying storage .\nStrides naturally extend to higher-dimensional tensors.\n(2, 1)\nFinally, strides can be used to implement indexing into the tensor. Assuming strides are \n and we want to look up tensor[i, j] , we can\ndirectly use strides to find its postion in the storage ::\nOr in general::\n(s1, s2)\nstorage[s1 * i + s2 * j]\nstorage[s1 * index1 + s2 * index2 + s3 * index3 ... ]\nBroadcasting\nBroadcasting makes tensors convenient and efficient to use, which comes in handy particularly for zip  operations. So far all of our zip\noperations assume two input tensors of exactly the same size and shape. However there are many interesting cases to zip  two tensors of\ndifferent size.\nPerhaps the simplest case is we have a vector of size 3 and want to add a scalar constant to every position::\nIntuitively, we would like to interpret this expression as the standard vector+scalar: adding 10 to each position. However, the above operation\nwill fail because of shape mistach: we are adding a tensor of shape(1,) to vector1  which has shape (3,).\nWe could ask users to create a tensor of the same shape instead, but it is both annoying and, more importantly, inefficient::\nBroadcasting  is a protocol that allows us to automatically interpret the frist expression as implying the second one. Inside zip , we pretend\nthat 10 is a vector of shape (3,) when zipping it with a vector of shape (3,). Again, this is just an interpretation: we never actually create this\nvector.\nimport chalk\n# In math notation, vector1 + 1\nvector1 + tensor([1])\nvector1 + tensor([1, 1, 1, 1, 1]).view(5, 1)\nThis gives us the first rule of broadcasting:\nRule 1: Any dimension of size 1 can be zipped with dimensions of size n > 1 by assuming the dimension is copied n times.\nNow let's apply this approach to a matrix of shape (5, 3)::\nHere we are trying to zip a matrix (2-D) of shape (5, 3) with a vector (1-D) of shape (1,). Here we are not just off on the shape, but also on the\nnumber of dimensions.\nHowever, recall that adding an extra dimension of shape-1 doesn't change the size of the tensor. Therefore we can allow our protocol to add\nthese in. Here if we add an empty dimension and then apply rule 1 twice, we can interpret the above expression as an efficient version of::\nRule 2: Extra dimensions of shape 1 can be added to a tensor to ensure the same number of dimensions with another tensor.\nmatrix1\nmatrix1 + tensor([1])\nmatrix1 + tensor([[1 for _ in range(3)] for _ in range(5)])\nvector3 = tensor([1, 2, 3, 4, 5]).view(5, 1)\nmatrix1 + vector3\nFinally, there is a question of where to add the empty dimension. This is not an issue in the above example but could become an issue in more\ncomplicated cases. Thus we introduce another rule:\nRule 3: Any extra dimension of size 1 can only be implicitly added on the left side of the shape.\nThis rule has the impact of making the process easy to follow and replicate. You always know what the shape of the final output will be. For\nexample\nWe can apply broadcasting as many times as we want::\n# This will fail: mismatch of (5, 3) and (3,)\nvector2 = tensor([2, 2, 3])\nmatrix1 * vector2\n# These two expression are equivalent\nmatrix1 * vector2\nmatrix1 * vector2.view(1, 3)\nHere is a more complicated example::\nWe end this guide with two important notes:\n1) Broadcasting is only about shapes. It does not take strides into account in any way. It is purely a high-level protocol. 2) Broadcasting has\nsome impact on the backward  pass. We will discuss some in the code base, but it is not required for any of the tasks.\nExamples\nTensor-Scalar operations can be easily written using broadcasting for tensors of any dimension.\nMatrix-vector operations can be written using broadcasting, but you need to be careful to make sure that the vector is shaped such the the\ndimensions align. This can be done with view  calls.\nMatrix-matrix operations can be written using broadcasting even when the dimensions don't align. Here is an example of that process.\nMatrix multiplication can be written in this style, here is \n where \n is 3 x 2 and \n is 2 x 2. (And you will need to use this for the\nassignment). However, note this is a memory inefficient way to do matrix multiplication, as it needs to create an intermediate tensor in the\nprocess.\nout = minitorch.zeros((2, 3, 1)) + minitorch.zeros((7, 2, 1, 5))\nout.shape\n(7, 2, 3, 5)\n(BxAT )\nA\nB\nOperations\nNow we would like to reimplement all our mathematical operations on tensors. The goal is to make this feel simple and intuitive to users of the\nlibrary. We can break these operations down as unary transformations\na) Return a new tensor with the same shape as tensor_a  where each position is the log\/exp\/negative of the position in tensor_a\nBinary transformations::\nb) Return a new tensor where each position is the sum\/mul\/sub of the position in tensor_a  and tensor_b\ntensor_a\n-tensor_a\ntensor_b = minitorch.tensor([[1, 1, 1, 1, 1], [-1, -1, -1, -1, -1]]).permute(1, 0)\ntensor_b\ntensor_a + tensor_b\nAnd reductions::\nc) Return a new tensor where dim-1 is size 1 and represents the sum\/mean over dim-1 in tensor_a\ntensor_a * tensor_b\ntensor_b - tensor_a\ntensor_a.sum(1)\ntensor_a.mean(1)\nCore Operations\nWe could implement each of these operations individually, but we can also be a bit lazy and note the structural similarities. If we squint, these\noperations look very much like the higher-order functions that we implemented in module0:\nOperation a \/ map: These operations just touch each of the positions in the tensor individually. They don't need to deal with other positions\nor know anything about the shape or size of the tensor. We can view these operations as applying the following transformation:\nOperation b \/ zip: These operations only need to pair operations between input tensors. If we assume the tensors have the same size and\nshape, this type of operation simply aligns these two tensors and applies an operator to each pair of elements:\nOperation c \/ reduce: These operations need to group together cells within a single tensor. We can think of there being an implied reduce\nshape that is eliminated in the process of the output construction. For instance, in the example below, we start with an input of shape (3, 2)\nand create an output of shape (1, 2). Implicitly, we reduce over a tensor of shape (3, 1) for each element in the output.\nReductions\nReduction is a bit more complex than the others, so let's discuss how it is implemented.\nReductions can specify a dimension (or axis) that tells us which elements to reduce. The reduction is then applied along that dimension. For\nexample if we reduce  dimension 0 we get the following reduction.\nThis reduction changes our shape from (3, 2) to (1, 2), i.e. reducing  the size of 0-dim. Another way you can view this is as 2 parallel operations,\nboth of which apply a Module-0 style reduce along the 0-dim. We can look at the reduction shape of this procedure (3, 1).\nApplying a reduction along dim 1 creates a different reduction shape which is applied 3 times. Here is what that looks like,\nThe same approach can be applied in higher dimensions. If we want to sum over one of the dimensions we just create the reduction shape and\napply it,\nWhen you implement, think about enumerating over positions in the final tensor, and then applying the reduction shape to get the indices over\nthe original tensor,\nFinally, there is a special case reduction where we reduce over the entire tensor. You can think of this as creating a reduction shape over the full\ntensor and then viewing the result as a scalar.\nIn the next module, we will discuss efficient implementation of the above operations. For now, they can be implemented by slow but correct\nloops over all elements in the input tensors. This approach can be used to implement key tensor operations, without doing more than\nimplementing the above higher-order functions.\nAuto-Grad\nNext, we consider autodifferentiation in the tensor framework. We have now moved from scalars and derivatives to vectors, matrices, and\ntensors. This means multivariate calculus can bring into play some more terminology. However, most of what we actually do does not require\ncomplicated terminology or much technical math. In fact, except some name changes, we have already built almost everything we need in\nModule 1.\nThe key idea is, just as we had Scalar  and ScalarFunction , we need to construct Tensor  and TensorFunction  (which we just call Function ).\nThese new objects behave very similar to their counterparts:\na) Tensors cannot be operated on directly, but need to be transformed through a function. b) Functions must implement both forward  and\nbackward . c) These transformations are tracked, which allow backpropagation through the chain rule.\nAll of this machinery should work out of the box.\nThe main new terminology to know is gradient. Just as a tensor is a multidimensional array of scalars, a gradient is a multidimensional array of\nderivatives for these scalars. Consider the following code::\nScalar auto-derivative notation\nTensor auto-gradient notation\nThe gradient of tensor1  is a tensor that holds the derivatives of each of its elements. Another place that gradients come into play is that\nbackward  no longer takes \n as an argument, but now takes \n which is just a tensor consisting of all the \n.\nNote: You will find lots of different notation for gradients and multivariate terminology. For this Module, you are supposed to ignore it and stick\nto everything you know about derivatives. It turns out that you can do most of machine learning without ever thinking in higher dimensions.\ndef f(a, b, c):\n    return a + b + c\na, b, c = Scalar(1), Scalar(2), Scalar(3)\nout = f(a, b, c)\nout.backward()\na.derivative, b.derivative, c.derivative\n(1.0, 1.0, 1.0)\ntensor1 = tensor([1, 2, 3])\nout = tensor1.sum()\nout.backward()\n# shape (3,)\ntensor1.grad\n[1.00 1.00 1.00]\ntensor1.grad.shape\n(3,)\ndout\ngradout\ndout\nIf you think about gradient and \n in this way (i.e. tensors of derivatives and \n), then you can see how we can easily compute the\ngradient for tensor operations using univariate rules.\n1) map. Given a tensor, map  applies a univariate operation to each scalar position individually. For a scalar , consider computing \n. From\nModule 1, we know that the derivative of \n is equal to \n. To compute the gradient in backward , we only need to compute the\nderivative for each scalar position and then apply a mul  map.\n2) zip. Given two tensors, zip  applies a binary operation to each pair of scalars. For two scalars  and , consider computing \n. From\nModule 1, we know that the derivative of \n is equal to \n and \n. Thus to compute the gradient, we only\nneed to compute the derivative for each scalar position and apply a mul  map.\n3) reduce. Given a tensor, reduce  applies an aggregation operation to one dimension. For simplicity, let's consider sum-based reductions. For\nscalars \n to \n, consider computing \n. For any \n value, the derivative is 1. Therefore, the derivative for any position\ncomputed in backward  is simply \n. This means to compute the gradient, we only need to send \n to each position. (For other reduce\noperations such as product , you get different expansions, which can be calculated just by taking derivatives).\ngradout\ndout\nx\ng(x)\nf(g(x))\ng\u2032(x) \u00d7 dout\nx\ny\ng(x, y)\nf(g(x, y))\ng\u2032\nx(x, y) \u00d7 dout\ng\u2032\ny(x, y) \u00d7 dout\nx1\nxn\nx1 + x2 + \u2026 + xn\nxi\ndout\ndout\nParallel Computation\nThe major technique we will use to speed up computation is parallelization. General parallel code can be difficult to write and get correct.\nHowever, we can again take advantage of the fact that we have structured our infrastructure around a set of core, general operations that are\nused throughout the codebase.\nLet's start by going back to module0  and our map  operation:\nThere are many ways to implement this function. Let's consider a version where we pass it an explict output list to fill in. Here's a simple\nimplementation\nThis function produces the correct result, but it's not very efficient. Ideally, we could do something fast to take advantage of the fact that all the\nfunction calls are identical. Also, we shouldn't have to loop over the list one value at a time. By definition, map  (and zipWith ) can be fast and\nparallelized, since none of the individual computations interact with each other.\nPython itself doesn't have great fuctions for fast math and parallelism built-in, but luckily it has good libraries to help speed up numerical\ncomputation. We will utilize one of these libraries known as Numba.\nNumba JIT\nNumba is a numerical JIT compiler for Python. When a function is first created, it converts raw Python code to faster numerical operations\nunder the hood. We have seen an example of this library earlier when developing our mathematical operators.\nThis JIT function alone does not make the code much faster, but it allows us to use this code within other code. For instance, if we want to\nimprove our map  implementation, we can change it to look like this:\nNote that all JIT happens when outer map is first called.\nfrom numba import njit, prange\ndef simple_map(fn):\n    def _map(out, input):\n        for i in range(len(out)):\n            out[i] = fn(input[i])\n    return _map\ndef neg(x):\n    return -x\ndef map(fn):\n    # Change 1: Move function from Python to JIT version.\n    fn = njit()(fn)\n    def _map(out, input):\n        for i in range(len(out)):\n            out[i] = fn(input[i])\n    # Change 2: Internal _map must be JIT version as well.\n    return njit()(_map)\nneg_map = map(neg)\nWhen the above function is called, instead of running slow Python code, it will run fast low-level code that takes advantage of the structure. This\napproach requires a bit of overhead on startup, but can make things much faster.\nFurthermore, if we know that the loop can be done in parallel, we can speed it up further with one small change.\nWhat's neat about this is that the above code is basically the same as the Python code without parallelization. You can switch back and forth\nbetween the two without much change.\nWarning: You have to be a bit careful to ensure that the loop actually can be parallelized. In short, this means that steps cannot depend on each\nother and each iteration cannot write to the same output value. For instance, when implementing reduce , you have to be careful to mix parallel\nand non-parallel loops.\nFor full details on how Numba works, read this tutorial.\ndef map(fn):\n    fn = njit()(fn)\n    def _map(out, input):\n        # Change 3: Run the loop in parallel (prange)\n        for i in prange(len(out)):\n            out[i] = fn(input[i])\n    return njit(parallel=True)(_map)\nFusing Operations\nAnother approach we can use to help improve tensor computations is to specialize commonly used combinations of operators. Combining\noperations can eliminate unnecessary intermediate tensors. This is particularly helpful for saving memory.\nThere is a lot of ongoing work on how to do this operator fusion automatically. However, for very common operators, it is worth just writing\nthese operators directly and speeding them up.\nIn minitorch we will do this fusion by customizing special operations. If we find it useful, we will add these operations in Numba\nExample: Matrix Multiplication\nLet's consider a matrix multiplication example. Recall that the rows of the first matrix interact with the columns of the second.\nIn past modules we have done matrix multiplication by applying a broadcasted zip  and then a reduce . For example, consider a tensor of size\n(2, 4) and a tensor of size (4, 3). We first zip these together with broadcasting to produce a tensor of size (2, 4, 3):\nAnd then reduce it to a tensor of size (2, 1, 3), which we can view as (2, 3):\nThis computation is correct, but it has two issues. First, it needs to create a tensor of size (2,4,3) in the intermediate step. These tensor can be\nan order of magnitude larger than either of the tensors that were passed in. Another more subtle problem is that the reduce  operator may need\nto call save_for_backwards  on this tensor, which means it stays in memory throughout all of our forward computations.\nAn alternative option is to fuse together these two operations. We will create a single matmul operations using python @  operator. We can then\nskip computing the intermediate value and directly compute the output. We can do this by writing a specialized tensor Function  with a\nforward  function that directly produces the output and a backward  that produces the required gradient.\nThis allows us to specialize our implementation of matrix multiplication. We do this by 1) walking over the output indices, 2) seeing which indices\nare reduced, and 3) then seeing which were part of the zip.\nGiven how important this operator is, it is worth spending the time to think about how to make each of these steps fast. Once we know the\noutput index we can very efficiently walk through the input indices and accumulate their sums.\nA fast matrix multiplication operator can be used to compute the forward . To compute backward we need to reverse the operations that we\ncomputed. In particular this requires zipping grad out with the alternative input matrix. We can see the backward  step by tracing the forward\narrows:\nIt would be a bit annoying to optimize this code as well, but again luckily, to compute the backward  step , we can reuse our forward\noptimization. In fact we can simply use the following identity from matrix calculus that tells us backward can be computed as a transpose and\nanother matrix multiply.\nmatrix(2, 3)\nf(M, N) = MN\ng\u2032\nM(f(M, N)) = dN T\ng\u2032\nN(f(M, N)) = M T d\nGPU Programming\nCPU parallelization and operator fusion is important, but when you really need efficiency and scale, specialized hardware is critical. It is really\nhard to exaggerate how important GPU computation is to deep learning: it makes it possible to run many models that would have been\nintractable even just several years ago.\nWriting code of GPUs requires a bit more work than the CPU parallelization examples. GPUs have a slightly different programming model than\nCPUs, which can take some time to fully understand. Luckily though, there is a nice Numba library extension that allows us to code for GPUs\ndirectly in Python.\nGetting Started\nFor Module 3, you will need to either work in an environment with a GPU available or utilize Google Colab. Google Colab provides free GPUs in a\nPython notebook setting. You can change the environment in the menu to request a GPU server.\nWe recommend working in your local setup and then cloning your environment on to a notebook:\nYou can run your tests with the following command:\nCUDA\nThe most commonly used programming model for GPUs in deep learning is known as CUDA. CUDA is a proprietary extension to C++ for Nvidia\ndevices. Once you fully understand the terminology, CUDA is a relatively straightforward extension to the mathematical code that we have been\nwriting.\nThe main mechanism is thread . A thread can run code and store a small amount of states. We represent a thread as a little robot:\nEach thread has a tiny amount of fixed local memory it can manipulate, which has to be constant size:\nlocal memory\n>>> git clone {GITHUB_PATH}\n>>> pip install -r requirements.txt\n>>> pip install -e .\n>>> python -m pytest -m task3_3\ny\nThreads hang out together in blocks . Think of these like a little neighborhood. You can determine the size of the blocks, but there are a lot of\nrestrictions. We assume there are less than 32 threads in a block:\nblock\nYou can also have square or even cubic blocks. Here is a square block where the length and width of the neighborhood are the block size:\nblock\nthreadidx.y\nthreadidx.x\nEach thread knows exactly where it is in the block. It gets this information in local variables telling it the thread index .\nblock\nthreadidx.y\nthreadidx.x\nThreads in the same block can also talk to each other through shared\nmemory . This is another constant chunk of memory that is associated with the block and can be accessed and written to by all of these threads:\nblock shared memory\nBlocks come together to form a grid . Each of the blocks has exactly the same size and shape, and all have their own shared memory. Each\nthread also knows its position in the global grid:\ngrid\nblockidx.y\nblockidx.x\nFor instance, we can compute the global position x, y  for a thread as::\nNow here comes the interesting part. When you write code for CUDA, you have to code all of the threads with the same code at the same time.\nEach thread behaves in lockstep running the same function:\ndef kernel():\n    x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n    y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n    print(x, y)\nIn Numba, you can write the thread instructions as a single function::\nMain cuda launcher\nNote that we cannot call the above function directly: we need to launch  it with instructions for how to set up the blocks and grid. Here is how\nyou do this with Numba::\nThis sets up a block and grid structure similar to the map  function mentioned earlier. The code in my_func  is run simultaneously for all the\nthreads in the structure. However, you have to be a bit careful as some threads might compute values that are outside the memory of your\nstructure\nMain cuda launcher\n# Helper function to call in CUDA\n# @cuda.jit(device=True)\ndef times(a, b):\n    return a * b\n# @cuda.jit()\ndef my_func(input, out):\n    # Create some local memory\n    local = cuda.local.array(5)\n    # Find my position.\n    x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n    y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n    # Compute some information\n    local[1] = 10\n    # Compute some global value\n    out[x, y] = times(input[x, y], local[1])\nthreadsperblock = (4, 3)\nblockspergrid = (1, 3)\nmy_func[blockspergrid, threadsperblock](in, out)\n# @cuda.jit()\ndef my_func2(input, out):\n    # Create some local memory\n    local = cuda.local.array(5)\n    # Find my position.\n    x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n    y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n    # Compute some information\n    local[1] = 10\n    # Guard some of the threads.\n    if x < out.shape[0] and y < out.shape[1]:\n        # Compute some global value\n        out[x, y] = times(input[x, y], local[1])\nConvolution\nSo far, our main approach to classification problems is to first feed the input to a Linear  layer which applies many different linear seperators to\nthe input, and then apply the ReLU function in order to transform it into a new hidden representation.\nOne major problem with the above approach is that it is based on the absolute  position of the original input features, which prevents us from\nlearning generalizable features of the input. Instead, we could have a sliding window (i.e. convolution) that uses the same set of learned\nparameters on different local regions of the input, as shown below:\nInstead of directly transforming the entire input image, the same convolution is applied at each part of the image to produce a new\nrepresentation. It is sliding  in the sense that, conceptually, the window over what region to process slides across the entire image to produce\nthe output.\nWe will primarily use the convolution on images to understand how to learn these locally applied parameters, but to get an intuitivie sense of\nhow convolution works, we will begin with input sequences in 1D.\nNote: Convolutions are extensively covered by many excellent tutorials. In addition to this Guide, we recommend you to check other tutorials on\nconvolution for more details.\n1D Convolution\nOur simple 1D convolution takes an input vector of length T and a weight (or kernel) vector of length K to produce an output vector of length T. It\ncomputes the output by sliding the weight along the input, zipping the weight with part of the input, reducing the zipped result to one value, and\nthen saving this value in the output:\nLoading [Contrib]\/a11y\/accessibility-menu.js\nIf the sliding window goes over the edge of the input, to make things simpler, we just assume the out-of-edge input values are 0.\nAn alternative way to think about a convolution is unrolling  the input. Now imagine we have a function named unroll  which could take an\ninput tensor and produce a new output tensor:\nWe then apply matrix multiplication to take the dot-product: ::\nWe can treat convolution as a fusion of the following two separate operations (note that we do not implement it this way in practice!):\ndef unroll(input, T, K):\n    out = [[input[i + k] if i + k < T else 0\n             for k in range(K)]\n            for i in range(T)]           \n    return tensor(out)\ninput = tensor([1, 2, 3, 4, 5, 6])\nK = 3\nT = input.shape[0]\nunrolled_input = unroll(input, T, K)\nprint(unrolled_input)\n[\n[1.00 2.00 3.00]\n[2.00 3.00 4.00]\n[3.00 4.00 5.00]\n[4.00 5.00 6.00]\n[5.00 6.00 0.00]\n[6.00 0.00 0.00]]\nweight = tensor([5, 2, 3])\noutput = (unrolled_input @ weight.view(K, 1)).view(T)\nprint(output)\n[18.00 28.00 38.00 48.00 37.00 30.00]\nLoading [Contrib]\/a11y\/accessibility-menu.js\nGiven an input and weight, it efficiently unrolls the input and takes matrix multiplication with weight. This technique is very useful because it\nallows you to hover  over a local segment of the input sentence. You can think of the weight as capturing a pattern (or many different patterns)\nin the original input.\nSame as every other operation in the model, we need to be able to compute the backward  operation of this 1D convolution. Note that we can\nreason through the flow of each cell with matrix multiplication. Going back to the previous example, the third cell in the original input (i.e.\ninput[2]) is only utilized to compute three different cells in the ouput: output[0], output[1] and output[2].\nTherefore, the gradient calculation is,\nVisually, it implies that the backward  of convolution is a convolution anchored in the opposite side with the reversed weights:\nSimilar gradient calculation can be derived for the weight:\noutput[0] = weight[0] * input[0] + \\\n    weight[1] * input[1]  + weight[2] * input[2]\noutput[1] = weight[0] * input[1] + \\\n    weight[1] * input[2]  + weight[2] * input[3]\noutput[2] = weight[0] * input[2] + \\\n    weight[1] * input[3]  + weight[2] * input[4]\nclass Conv:\n    @staticmethod\n    def backward(ctx, d):\n        ...\n        grad_input[2] = weight[0] * d[2] + weight[1] * d[1]  + weight[2] * d[0]\n        ...\nLoading [Contrib]\/a11y\/accessibility-menu.js\nThe above implies that, same as matrix multiplication, implementing a fast convolution can be used for both forward  and backward .\nFinally, the above approach can be scaled up to handle multiple input features and multiple weights simultaneously through channels .\nAnalogously to matrix multiplication, we take an (in_channels, T) input and an (out_channels, in_channels, K) weight to produce an\n(out_channels, T). Below is an example with in_channels = 2, out_channels = 3, K = 3 and T = 8.\nCodewise (might be a bit harder to read so we recommend just sticking with the above diagram):\nWe can now do this as a matrix multiplication\ndef unroll_chan(input, T, C, K):\n    out = [[input[i + k, c] if i + k < T else 0\n             for k in range(K)\n            for c in range(C)]\n           for i in range(T)]\n    return tensor(out)\nin_channels = 2\ninput = rand(T, in_channels)\nunrolled_input = unroll_chan(input, T, in_channels, K)\nprint(unrolled_input.shape) # Shape: T x (in_channels * K)\n(6, 6)\nout_channels = 3\nweight = rand(in_channels * K, out_channels)\noutput = unrolled_input @ weight\nprint(output.shape)\nLoading [Contrib]\/a11y\/accessibility-menu.js\n1D convolution has all sorts of neat applications: it can be applied in NLP as a way of applying a model to multiple neighboring words; it can be\nused in speech recognition as a way of recognizing important sounds; it can be used in anomoly detection to find patterns that trigger an alertl;\nanywhere you can imagine that a mini-neural network applying to subset of a sequence can be useful.\n2D Convolution\nWhile 1D convolutions detect patterns along a sequence, 2D convolutions detect patterns within a grid. The underlying math for the simple 2D is\nvery similar to the 1D case. Our simple 2D convolution takes in an (H, W) input (i.e. height and width) and a (KH, KW) weight to produce an (H, W)\noutput. The operation is nearly identical: we walk through each possible unrolled rectangle in the input matrix, and multiply with the weight.\nAssuming we had an analogous unroll function for matrices, this would be equivalent to computing:\nAnother way to think about it is just applying weight as a Linear  layer to each one of the rectangles in the input image.\nCritically, just as the 1D convolution is anchored at the left, the 2D convolution is anchored at the top left. To compute its backward , we compute\na bottom-right reverse convolution:\nFinally, we can again complicate things by applying many weights to many input features simultaneously, which gives us the standard 2D\nconvolution used in Torch. We take an (in_channels, H, W) input, and an (out_channels, in_channels, KW, KH) weight matrix to produce an\n(out_channels, H, W) output:\n(6, 3)\noutput = (unrolled_input.view(H, W, KH * KW) @ weight.view(KH * KW, 1)).view(H, W)\nLoading [Contrib]\/a11y\/accessibility-menu.js\nVery roughly, the output takes this form:\n2D convolution is the main operator for image recognition systems. It allows us to process images into local feature representations. It is also\nthe key step in the convolutional neural network pipeline. It transforms the input image into hidden features which get propagated through each\nstage of the network:\noutput = unrolled_input.view(H, W, in_channels * KH * KW) \\\n    @ weight.view(in_channels * KH * KW, out_channels)\nLoading [Contrib]\/a11y\/accessibility-menu.js\nPooling\nIn previous modules, we have found it useful to reduce over certain dimensions to reduce the shape of tensors. For example, in the NLP example,\nwe sum over the length of the sentence in order to classify based on the summed word representations. Critically, this operation does not\nremove the importance of words (i.e. they still receive gradient information), but it does allow us to make a single classification decision based\non a fixed-sized representation.\nThis style of network goes by the informal name pooling  in the neural network literature. When we reduce part of the input in order to work\nwith a smaller size, we say we have pooled together the input representations.\nApplying reduction over length or other dimensions is one form of pooling. For sequential cases, we might call it X-over-time pooling (where X\nmight be a sum, mean, max etc.):\nAnother common form of pooling is to only pool locally within a dimension. For instance, we might pool together neighboring elements to reduce\nthel length for that dimension, as visualized below. This is common in domains like speech recognition where the input sequences are very long.\nIn practice, to avoid implementing the pooling operation as a loop, we can manipulate the shape and strides of the input tensor in order to pool it\ndirectly. Assuming the input tensor is contiguous, we can simply add an extra dimension by applying view . For instance, we can view  a (8,)\ninput as (4, 2):\nOnce we have the input in this form, we can reduce over the second dimension to get a (4,1) tensor, which can be viewed as a (4,). As long as the\ndimensions for pooling are divisible by the pooling constant (i.e. the input size for each individual pooling operation), this procedure will produce\nthe correct pooled result, as visualized below. If not divisible, we can pad our input tensors, or add padding along the way.\nYou will implement a version of this type of pooling in two dimensions for images. You need to generalize the above idea in 1D pooling to create a\nshape with two extra dimesions to reduce over:\nThe benefit of pooling is that applying a small convolution over the pooled results covers a larger region in the original input image. These later\nlayers ideally can learn higher-level properties of the input image.\nMulticlass Classification\nSigmoid\nSo far, the key function that we have relied on for calculating loss is the sigmoid function. As visualized below, it goes to zero for large negative\ninputs, and goes to 1 for large positive inputs. In between, it forms a smooth S-curve.\nAs we saw in Module 1, sigmoid function makes it easier to apply auto-differentiation when training our models. It can be thought of as a\nsmooth version of the step function \n, which signals whether  is greater than zero by returning a binary value. Another way to write this\nstep function is \n, i.e. returns which argument is bigger, 0 or 1. Whereas step function returns a binary choice, sigmoid\nfunction gives a \"softer\" differentiable choice.\nWe can connect sigmoid function to another function that we have used in previous MiniTorch Modules: the ReLU function that we use for\nactivations. Recall that this function is defined as \n.\nThis has the following simple derivative function:\nMulticlass\nx > 0\nx\nstep(x) = argmax{0, x}\nReLU(x) = max{0, x}\nReLU'(x) = { 0\nif\u00a0x \u22640\n1\now\nThe sigmoid function works great for binary classification problems. However, for many problems, we may want to do multiclass\nclassification, where we have \n possible output classes to select from. For these problems, we can assume that the model should output a \n-\ndimensional vector which gives a score for each of the K possible classes:\nNaturally, we pick the output class that has the highest score. Given a vector, argmax function returns a one-hot vector with 1 at the position of\nthe highest-scored element and 0 for all other elements:\nWhile argmax function seems a bit different at the first glance, we can view it as a generalization of the \n function: each position is either 0\nor 1. We can also see that its derivative will be zero almost everywhere: a small perturbation to the input will not change the output value.\nIn order to fix this issue, we need a soft version of the argmax function, just like sigmoid function smooths over the input changes. The\ngeneralization of sigmoid function is appropriately known as the softmax  function, which is computed as:\nLike the sigmoid function, every value of softmax function is between 0 and 1, and a small change to any of the input scores will result in a\nchange to all of the output values.\nAs the softmax function requires exponentiating the input scores, it can be numerically unstable in practice. Therefore it is common to use a\nnumerical trick to compute the log of the softmax function instead:\nwhere \n is the max element of . This trick is common enough that there is a nice derivation on wikipedia. (This is a pratical trick for sigmoid\nfunction as well, which we ignored in earlier modules.)\nSpeaking of max, we can add a max operator to our code base. We can compute the max of a vector (or tensor in general) as a reduction, which\nreturns the single highest-scored element in the input. Intuitively, we can think about how small changes to the input impact this returned value.\nIgnoring ties, only the element that has the highest score will have a non-zero derivative, and its derivative will be 1. Thereforce the gradient of\nthe max reduction is a one-hot vector with 1 for the highest-scored element, i.e. the argmax function.\nK\nK\nx > 0\nsoftmax(x) =\nexp x\n\u2211i exp xi\nlogsoftmax(x) = x \u2212log \u2211\ni\nexp xi = x \u2212log(\u2211\ni\nexp(xi \u2212m)) \u2212m\nm\nx\n\n","files_to_edit":["minitorch\/operators.py","minitorch\/nn.py","minitorch\/module.py","minitorch\/scalar.py","minitorch\/fast_ops.py","minitorch\/cuda_ops.py","minitorch\/autodiff.py","minitorch\/tensor_ops.py","minitorch\/tensor_data.py","minitorch\/fast_conv.py","minitorch\/tensor_functions.py","minitorch\/scalar_functions.py"],"full_output":{"minitorch\/autodiff.py":"```python\nfrom dataclasses import dataclass\nfrom typing import Any, Iterable, List, Tuple\n\nfrom typing_extensions import Protocol\n\n# ## Task 1.1\n# Central Difference calculation\n\n\ndef central_difference(f: Any, *vals: Any, arg: int = 0, epsilon: float = 1e-6) -> Any:\n    r\"\"\"\n    Computes an approximation to the derivative of `f` with respect to one arg.\n\n    See :doc:`derivative` or https:\/\/en.wikipedia.org\/wiki\/Finite_difference for more details.\n\n    Args:\n        f : arbitrary function from n-scalar args to one value\n        *vals : n-float values $x_0 \\ldots x_{n-1}$\n        arg : the number $i$ of the arg to compute the derivative\n        epsilon : a small constant\n\n    Returns:\n        An approximation of $f'_i(x_0, \\ldots, x_{n-1})$\n    \"\"\"\n    # Create a list of values to compute the derivative\n    vals = list(vals)\n    # Save the original value\n    original_value = vals[arg]\n    \n    # Calculate f(x + epsilon)\n    vals[arg] = original_value + epsilon\n    f_plus = f(*vals)\n    \n    # Calculate f(x - epsilon)\n    vals[arg] = original_value - epsilon\n    f_minus = f(*vals)\n    \n    # Restore the original value\n    vals[arg] = original_value\n    \n    # Return the central difference approximation\n    return (f_plus - f_minus) \/ (2 * epsilon)\n\n\nvariable_count = 1\n\n\nclass Variable(Protocol):\n    def accumulate_derivative(self, x: Any) -> None:\n        pass\n\n    @property\n    def unique_id(self) -> int:\n        pass\n\n    def is_leaf(self) -> bool:\n        pass\n\n    def is_constant(self) -> bool:\n        pass\n\n    @property\n    def parents(self) -> Iterable[\"Variable\"]:\n        pass\n\n    def chain_rule(self, d_output: Any) -> Iterable[Tuple[\"Variable\", Any]]:\n        pass\n\n\ndef topological_sort(variable: Variable) -> Iterable[Variable]:\n    \"\"\"\n    Computes the topological order of the computation graph.\n\n    Args:\n        variable: The right-most variable\n\n    Returns:\n        Non-constant Variables in topological order starting from the right.\n    \"\"\"\n    visited = set()\n    order = []\n\n    def visit(v: Variable):\n        if v not in visited:\n            visited.add(v)\n            for parent in v.parents:\n                visit(parent)\n            order.append(v)\n\n    visit(variable)\n    return reversed(order)\n\n\ndef backpropagate(variable: Variable, deriv: Any) -> None:\n    \"\"\"\n    Runs backpropagation on the computation graph in order to\n    compute derivatives for the leave nodes.\n\n    Args:\n        variable: The right-most variable\n        deriv  : Its derivative that we want to propagate backward to the leaves.\n\n    No return. Should write to its results to the derivative values of each leaf through `accumulate_derivative`.\n    \"\"\"\n    # Create a dictionary to store the derivatives\n    derivatives = {variable: deriv}\n    \n    # Perform a topological sort to get the order of variables\n    order = topological_sort(variable)\n\n    for var in order:\n        if var.is_leaf():\n            var.accumulate_derivative(derivatives[var])\n        else:\n            # Compute the chain rule for non-leaf variables\n            for parent, local_deriv in var.chain_rule(derivatives[var]):\n                if parent in derivatives:\n                    derivatives[parent] += local_deriv\n                else:\n                    derivatives[parent] = local_deriv\n\n\n@dataclass\nclass Context:\n    \"\"\"\n    Context class is used by `Function` to store information during the forward pass.\n    \"\"\"\n\n    no_grad: bool = False\n    saved_values: Tuple[Any, ...] = ()\n\n    def save_for_backward(self, *values: Any) -> None:\n        \"Store the given `values` if they need to be used during backpropagation.\"\n        if self.no_grad:\n            return\n        self.saved_values = values\n\n    @property\n    def saved_tensors(self) -> Tuple[Any, ...]:\n        return self.saved_values\n```","minitorch\/cuda_ops.py":"<code> \nfrom typing import Callable, Optional\n\nimport numba\nfrom numba import cuda\n\nfrom .tensor import Tensor\nfrom .tensor_data import (\n    MAX_DIMS,\n    Shape,\n    Storage,\n    Strides,\n    TensorData,\n    broadcast_index,\n    index_to_position,\n    shape_broadcast,\n    to_index,\n)\nfrom .tensor_ops import MapProto, TensorOps\n\n# This code will CUDA compile fast versions your tensor_data functions.\n# If you get an error, read the docs for NUMBA as to what is allowed\n# in these functions.\n\nto_index = cuda.jit(device=True)(to_index)\nindex_to_position = cuda.jit(device=True)(index_to_position)\nbroadcast_index = cuda.jit(device=True)(broadcast_index)\n\nTHREADS_PER_BLOCK = 32\n\n\nclass CudaOps(TensorOps):\n    cuda = True\n\n    @staticmethod\n    def map(fn: Callable[[float], float]) -> MapProto:\n        \"See `tensor_ops.py`\"\n        f = tensor_map(cuda.jit(device=True)(fn))\n\n        def ret(a: Tensor, out: Optional[Tensor] = None) -> Tensor:\n            if out is None:\n                out = a.zeros(a.shape)\n\n            # Instantiate and run the cuda kernel.\n            threadsperblock = THREADS_PER_BLOCK\n            blockspergrid = (out.size + THREADS_PER_BLOCK - 1) \/\/ THREADS_PER_BLOCK\n            f[blockspergrid, threadsperblock](*out.tuple(), out.size, *a.tuple())  # type: ignore\n            return out\n\n        return ret\n\n    @staticmethod\n    def zip(fn: Callable[[float, float], float]) -> Callable[[Tensor, Tensor], Tensor]:\n        f = tensor_zip(cuda.jit(device=True)(fn))\n\n        def ret(a: Tensor, b: Tensor) -> Tensor:\n            c_shape = shape_broadcast(a.shape, b.shape)\n            out = a.zeros(c_shape)\n            threadsperblock = THREADS_PER_BLOCK\n            blockspergrid = (out.size + (threadsperblock - 1)) \/\/ threadsperblock\n            f[blockspergrid, threadsperblock](  # type: ignore\n                *out.tuple(), out.size, *a.tuple(), *b.tuple()\n            )\n            return out\n\n        return ret\n\n    @staticmethod\n    def reduce(\n        fn: Callable[[float, float], float], start: float = 0.0\n    ) -> Callable[[Tensor, int], Tensor]:\n        f = tensor_reduce(cuda.jit(device=True)(fn))\n\n        def ret(a: Tensor, dim: int) -> Tensor:\n            out_shape = list(a.shape)\n            out_shape[dim] = (a.shape[dim] - 1) \/\/ 1024 + 1\n            out_a = a.zeros(tuple(out_shape))\n\n            threadsperblock = 1024\n            blockspergrid = out_a.size\n            f[blockspergrid, threadsperblock](  # type: ignore\n                *out_a.tuple(), out_a.size, *a.tuple(), dim, start\n            )\n\n            return out_a\n\n        return ret\n\n    @staticmethod\n    def matrix_multiply(a: Tensor, b: Tensor) -> Tensor:\n        # Make these always be a 3 dimensional multiply\n        both_2d = 0\n        if len(a.shape) == 2:\n            a = a.contiguous().view(1, a.shape[0], a.shape[1])\n            both_2d += 1\n        if len(b.shape) == 2:\n            b = b.contiguous().view(1, b.shape[0], b.shape[1])\n            both_2d += 1\n        both_2d = both_2d == 2\n\n        ls = list(shape_broadcast(a.shape[:-2], b.shape[:-2]))\n        ls.append(a.shape[-2])\n        ls.append(b.shape[-1])\n        assert a.shape[-1] == b.shape[-2]\n        out = a.zeros(tuple(ls))\n\n        # One block per batch, extra rows, extra col\n        blockspergrid = (\n            (out.shape[1] + (THREADS_PER_BLOCK - 1)) \/\/ THREADS_PER_BLOCK,\n            (out.shape[2] + (THREADS_PER_BLOCK - 1)) \/\/ THREADS_PER_BLOCK,\n            out.shape[0],\n        )\n        threadsperblock = (THREADS_PER_BLOCK, THREADS_PER_BLOCK, 1)\n\n        tensor_matrix_multiply[blockspergrid, threadsperblock](\n            *out.tuple(), out.size, *a.tuple(), *b.tuple()\n        )\n\n        # Undo 3d if we added it.\n        if both_2d:\n            out = out.view(out.shape[1], out.shape[2])\n        return out\n\n\n# Implement\n\n\ndef tensor_map(\n    fn: Callable[[float], float]\n) -> Callable[[Storage, Shape, Strides, Storage, Shape, Strides], None]:\n    \"\"\"\n    CUDA higher-order tensor map function. ::\n\n      fn_map = tensor_map(fn)\n      fn_map(out, ... )\n\n    Args:\n        fn: function mappings floats-to-floats to apply.\n\n    Returns:\n        Tensor map function.\n    \"\"\"\n\n    def _map(\n        out: Storage,\n        out_shape: Shape,\n        out_strides: Strides,\n        out_size: int,\n        in_storage: Storage,\n        in_shape: Shape,\n        in_strides: Strides,\n    ) -> None:\n\n        out_index = cuda.local.array(MAX_DIMS, numba.int32)\n        in_index = cuda.local.array(MAX_DIMS, numba.int32)\n        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n\n        if i < out_size:\n            # Calculate the index for the output tensor\n            to_index(out_index, i, out_shape, out_strides)\n            # Calculate the index for the input tensor\n            to_index(in_index, i, in_shape, in_strides)\n\n            # Apply the function and store the result\n            out[out_index[0]] = fn(in_storage[in_index[0]])\n\n    return cuda.jit()(_map)  # type: ignore\n\n\ndef tensor_zip(\n    fn: Callable[[float, float], float]\n) -> Callable[\n    [Storage, Shape, Strides, Storage, Shape, Strides, Storage, Shape, Strides], None\n]:\n    \"\"\"\n    CUDA higher-order tensor zipWith (or map2) function ::\n\n      fn_zip = tensor_zip(fn)\n      fn_zip(out, ...)\n\n    Args:\n        fn: function mappings two floats to float to apply.\n\n    Returns:\n        Tensor zip function.\n    \"\"\"\n\n    def _zip(\n        out: Storage,\n        out_shape: Shape,\n        out_strides: Strides,\n        out_size: int,\n        a_storage: Storage,\n        a_shape: Shape,\n        a_strides: Strides,\n        b_storage: Storage,\n        b_shape: Shape,\n        b_strides: Strides,\n    ) -> None:\n\n        out_index = cuda.local.array(MAX_DIMS, numba.int32)\n        a_index = cuda.local.array(MAX_DIMS, numba.int32)\n        b_index = cuda.local.array(MAX_DIMS, numba.int32)\n        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n\n        if i < out_size:\n            # Calculate the index for the output tensor\n            to_index(out_index, i, out_shape, out_strides)\n            # Calculate the index for the first input tensor\n            to_index(a_index, i, a_shape, a_strides)\n            # Calculate the index for the second input tensor\n            to_index(b_index, i, b_shape, b_strides)\n\n            # Apply the function and store the result\n            out[out_index[0]] = fn(a_storage[a_index[0]], b_storage[b_index[0]])\n\n    return cuda.jit()(_zip)  # type: ignore\n\n\ndef _sum_practice(out: Storage, a: Storage, size: int) -> None:\n    \"\"\"\n    This is a practice sum kernel to prepare for reduce.\n\n    Given an array of length $n$ and out of size $n \/\/ \\text{blockDIM}$\n    it should sum up each blockDim values into an out cell.\n\n    $[a_1, a_2, ..., a_{100}]$\n\n    |\n\n    $[a_1 +...+ a_{31}, a_{32} + ... + a_{64}, ... ,]$\n\n    Note: Each block must do the sum using shared memory!\n\n    Args:\n        out (Storage): storage for `out` tensor.\n        a (Storage): storage for `a` tensor.\n        size (int):  length of a.\n\n    \"\"\"\n    BLOCK_DIM = 32\n    pos = cuda.threadIdx.x\n    i = cuda.blockIdx.x * cuda.blockDim.x + pos\n\n    # Allocate shared memory for the block\n    shared = cuda.shared.array(BLOCK_DIM, numba.float64)\n\n    # Load data into shared memory\n    if i < size:\n        shared[pos] = a[i]\n    else:\n        shared[pos] = 0.0\n\n    cuda.syncthreads()\n\n    # Perform reduction in shared memory\n    for s in range(BLOCK_DIM \/\/ 2, 0, -1):\n        if pos < s:\n            shared[pos] += shared[pos + s]\n        cuda.syncthreads()\n\n    # Write the result for this block to global memory\n    if pos == 0:\n        out[cuda.blockIdx.x] = shared[0]\n\n\njit_sum_practice = cuda.jit()(_sum_practice)\n\n\ndef sum_practice(a: Tensor) -> TensorData:\n    (size,) = a.shape\n    threadsperblock = THREADS_PER_BLOCK\n    blockspergrid = (size \/\/ THREADS_PER_BLOCK) + 1\n    out = TensorData([0.0 for i in range(blockspergrid)], (blockspergrid,))\n    out.to_cuda_()\n    jit_sum_practice[blockspergrid, threadsperblock](\n        out.tuple()[0], a._tensor._storage, size\n    )\n    return out\n\n\ndef tensor_reduce(\n    fn: Callable[[float, float], float]\n) -> Callable[[Storage, Shape, Strides, Storage, Shape, Strides, int], None]:\n    \"\"\"\n    CUDA higher-order tensor reduce function.\n\n    Args:\n        fn: reduction function maps two floats to float.\n\n    Returns:\n        Tensor reduce function.\n\n    \"\"\"\n\n    def _reduce(\n        out: Storage,\n        out_shape: Shape,\n        out_strides: Strides,\n        out_size: int,\n        a_storage: Storage,\n        a_shape: Shape,\n        a_strides: Strides,\n        reduce_dim: int,\n        reduce_value: float,\n    ) -> None:\n        BLOCK_DIM = 1024\n        cache = cuda.shared.array(BLOCK_DIM, numba.float64)\n        out_index = cuda.local.array(MAX_DIMS, numba.int32)\n        a_index = cuda.local.array(MAX_DIMS, numba.int32)\n        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n\n        # Initialize shared memory\n        if i < out_size:\n            cache[cuda.threadIdx.x] = reduce_value\n        else:\n            cache[cuda.threadIdx.x] = 0.0\n\n        cuda.syncthreads()\n\n        # Perform reduction\n        for j in range(1, a_shape[reduce_dim]):\n            to_index(a_index, j, a_shape, a_strides)\n            cache[cuda.threadIdx.x] = fn(cache[cuda.threadIdx.x], a_storage[a_index[0]])\n\n        # Write the result for this block to global memory\n        if cuda.threadIdx.x == 0:\n            out[out_index[0]] = cache[0]\n\n    return cuda.jit()(_reduce)  # type: ignore\n\n\ndef _tensor_matrix_multiply(\n    out: Storage,\n    out_shape: Shape,\n    out_strides: Strides,\n    out_size: int,\n    a_storage: Storage,\n    a_shape: Shape,\n    a_strides: Strides,\n    b_storage: Storage,\n    b_shape: Shape,\n    b_strides: Strides,\n) -> None:\n    \"\"\"\n    CUDA tensor matrix multiply function.\n\n    Requirements:\n\n    * All data must be first moved to shared memory.\n    * Only read each cell in `a` and `b` once.\n    * Only write to global memory once per kernel.\n\n    Should work for any tensor shapes that broadcast as long as ::\n\n    ```python\n    assert a_shape[-1] == b_shape[-2]\n    ```\n    Returns:\n        None : Fills in `out`\n    \"\"\"\n    a_batch_stride = a_strides[0] if a_shape[0] > 1 else 0\n    b_batch_stride = b_strides[0] if b_shape[0] > 1 else 0\n    # Batch dimension - fixed\n    batch = cuda.blockIdx.z\n\n    BLOCK_DIM = 32\n    a_shared = cuda.shared.array((BLOCK_DIM, BLOCK_DIM), numba.float64)\n    b_shared = cuda.shared.array((BLOCK_DIM, BLOCK_DIM), numba.float64)\n\n    # The final position c[i, j]\n    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n    j = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n\n    # The local position in the block.\n    pi = cuda.threadIdx.x\n    pj = cuda.threadIdx.y\n\n    # Load data into shared memory\n    if i < a_shape[1] and batch < a_shape[0]:\n        a_shared[pi, pj] = a_storage[index_to_position((batch, i), a_shape, a_strides)]\n    else:\n        a_shared[pi, pj] = 0.0\n\n    if j < b_shape[2] and batch < b_shape[0]:\n        b_shared[pi, pj] = b_storage[index_to_position((batch, j), b_shape, b_strides)]\n    else:\n        b_shared[pi, pj] = 0.0\n\n    cuda.syncthreads()\n\n    # Compute the dot product\n    if i < out_shape[1] and j < out_shape[2]:\n        temp_sum = 0.0\n        for k in range(a_shape[2]):\n            temp_sum += a_shared[pi, k] * b_shared[k, pj]\n        out[index_to_position((batch, i, j), out_shape, out_strides)] = temp_sum\n\ntensor_matrix_multiply = cuda.jit(_tensor_matrix_multiply)\n<\/code>","minitorch\/fast_conv.py":"<code> \nfrom typing import Tuple\n\nimport numpy as np\nfrom numba import njit, prange\n\nfrom .autodiff import Context\nfrom .tensor import Tensor\nfrom .tensor_data import (\n    MAX_DIMS,\n    Index,\n    Shape,\n    Strides,\n    broadcast_index,\n    index_to_position,\n    to_index,\n)\nfrom .tensor_functions import Function\n\n# This code will JIT compile fast versions your tensor_data functions.\n# If you get an error, read the docs for NUMBA as to what is allowed\n# in these functions.\nto_index = njit(inline=\"always\")(to_index)\nindex_to_position = njit(inline=\"always\")(index_to_position)\nbroadcast_index = njit(inline=\"always\")(broadcast_index)\n\n\ndef _tensor_conv1d(\n    out: Tensor,\n    out_shape: Shape,\n    out_strides: Strides,\n    out_size: int,\n    input: Tensor,\n    input_shape: Shape,\n    input_strides: Strides,\n    weight: Tensor,\n    weight_shape: Shape,\n    weight_strides: Strides,\n    reverse: bool,\n) -> None:\n    \"\"\"\n    1D Convolution implementation.\n\n    Given input tensor of\n\n       `batch, in_channels, width`\n\n    and weight tensor\n\n       `out_channels, in_channels, k_width`\n\n    Computes padded output of\n\n       `batch, out_channels, width`\n\n    `reverse` decides if weight is anchored left (False) or right.\n    (See diagrams)\n\n    Args:\n        out (Storage): storage for `out` tensor.\n        out_shape (Shape): shape for `out` tensor.\n        out_strides (Strides): strides for `out` tensor.\n        out_size (int): size of the `out` tensor.\n        input (Storage): storage for `input` tensor.\n        input_shape (Shape): shape for `input` tensor.\n        input_strides (Strides): strides for `input` tensor.\n        weight (Storage): storage for `input` tensor.\n        weight_shape (Shape): shape for `input` tensor.\n        weight_strides (Strides): strides for `input` tensor.\n        reverse (bool): anchor weight at left or right\n    \"\"\"\n    batch_, out_channels, out_width = out_shape\n    batch, in_channels, width = input_shape\n    out_channels_, in_channels_, kw = weight_shape\n\n    assert (\n        batch == batch_\n        and in_channels == in_channels_\n        and out_channels == out_channels_\n    )\n    \n    for b in range(batch):\n        for oc in range(out_channels):\n            for w in range(out_width):\n                # Initialize output value\n                out[b, oc, w] = 0.0\n                for ic in range(in_channels):\n                    for k in range(kw):\n                        if reverse:\n                            if w - k >= 0:  # Ensure we are within bounds\n                                out[b, oc, w] += (\n                                    input[b, ic, w - k] * weight[oc, ic, k]\n                                )\n                        else:\n                            if w + k < width:  # Ensure we are within bounds\n                                out[b, oc, w] += (\n                                    input[b, ic, w + k] * weight[oc, ic, k]\n                                )\n\n\ntensor_conv1d = njit(parallel=True)(_tensor_conv1d)\n\n\nclass Conv1dFun(Function):\n    @staticmethod\n    def forward(ctx: Context, input: Tensor, weight: Tensor) -> Tensor:\n        \"\"\"\n        Compute a 1D Convolution\n\n        Args:\n            ctx : Context\n            input : batch x in_channel x h x w\n            weight : out_channel x in_channel x kh x kw\n\n        Returns:\n            batch x out_channel x h x w\n        \"\"\"\n        ctx.save_for_backward(input, weight)\n        batch, in_channels, w = input.shape\n        out_channels, in_channels2, kw = weight.shape\n        assert in_channels == in_channels2\n\n        # Run convolution\n        output = input.zeros((batch, out_channels, w))\n        tensor_conv1d(\n            *output.tuple(), output.size, *input.tuple(), *weight.tuple(), False\n        )\n        return output\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor]:\n        input, weight = ctx.saved_values\n        batch, in_channels, w = input.shape\n        out_channels, in_channels, kw = weight.shape\n\n        grad_weight = grad_output.zeros((in_channels, out_channels, kw))\n        new_input = input.permute(1, 0, 2)\n        new_grad_output = grad_output.permute(1, 0, 2)\n        tensor_conv1d(\n            *grad_weight.tuple(),\n            grad_weight.size,\n            *new_input.tuple(),\n            *new_grad_output.tuple(),\n            False,\n        )\n        grad_weight = grad_weight.permute(1, 0, 2)\n\n        grad_input = input.zeros((batch, in_channels, w))\n        new_weight = weight.permute(1, 0, 2)\n        tensor_conv1d(\n            *grad_input.tuple(),\n            grad_input.size,\n            *grad_output.tuple(),\n            *new_weight.tuple(),\n            True,\n        )\n        return grad_input, grad_weight\n\n\nconv1d = Conv1dFun.apply\n\n\ndef _tensor_conv2d(\n    out: Tensor,\n    out_shape: Shape,\n    out_strides: Strides,\n    out_size: int,\n    input: Tensor,\n    input_shape: Shape,\n    input_strides: Strides,\n    weight: Tensor,\n    weight_shape: Shape,\n    weight_strides: Strides,\n    reverse: bool,\n) -> None:\n    \"\"\"\n    2D Convolution implementation.\n\n    Given input tensor of\n\n       `batch, in_channels, height, width`\n\n    and weight tensor\n\n       `out_channels, in_channels, k_height, k_width`\n\n    Computes padded output of\n\n       `batch, out_channels, height, width`\n\n    `Reverse` decides if weight is anchored top-left (False) or bottom-right.\n    (See diagrams)\n\n\n    Args:\n        out (Storage): storage for `out` tensor.\n        out_shape (Shape): shape for `out` tensor.\n        out_strides (Strides): strides for `out` tensor.\n        out_size (int): size of the `out` tensor.\n        input (Storage): storage for `input` tensor.\n        input_shape (Shape): shape for `input` tensor.\n        input_strides (Strides): strides for `input` tensor.\n        weight (Storage): storage for `input` tensor.\n        weight_shape (Shape): shape for `input` tensor.\n        weight_strides (Strides): strides for `input` tensor.\n        reverse (bool): anchor weight at top-left or bottom-right\n    \"\"\"\n    batch_, out_channels, _, _ = out_shape\n    batch, in_channels, height, width = input_shape\n    out_channels_, in_channels_, kh, kw = weight_shape\n\n    assert (\n        batch == batch_\n        and in_channels == in_channels_\n        and out_channels == out_channels_\n    )\n\n    for b in range(batch):\n        for oc in range(out_channels):\n            for h in range(height):\n                for w in range(width):\n                    # Initialize output value\n                    out[b, oc, h, w] = 0.0\n                    for ic in range(in_channels):\n                        for kh_idx in range(kh):\n                            for kw_idx in range(kw):\n                                if reverse:\n                                    if (h - kh_idx >= 0) and (w - kw_idx >= 0):\n                                        out[b, oc, h, w] += (\n                                            input[b, ic, h - kh_idx, w - kw_idx] *\n                                            weight[oc, ic, kh_idx, kw_idx]\n                                        )\n                                else:\n                                    if (h + kh_idx < height) and (w + kw_idx < width):\n                                        out[b, oc, h, w] += (\n                                            input[b, ic, h + kh_idx, w + kw_idx] *\n                                            weight[oc, ic, kh_idx, kw_idx]\n                                        )\n\n\ntensor_conv2d = njit(parallel=True, fastmath=True)(_tensor_conv2d)\n\n\nclass Conv2dFun(Function):\n    @staticmethod\n    def forward(ctx: Context, input: Tensor, weight: Tensor) -> Tensor:\n        \"\"\"\n        Compute a 2D Convolution\n\n        Args:\n            ctx : Context\n            input : batch x in_channel x h x w\n            weight  : out_channel x in_channel x kh x kw\n\n        Returns:\n            (:class:`Tensor`) : batch x out_channel x h x w\n        \"\"\"\n        ctx.save_for_backward(input, weight)\n        batch, in_channels, h, w = input.shape\n        out_channels, in_channels2, kh, kw = weight.shape\n        assert in_channels == in_channels2\n        output = input.zeros((batch, out_channels, h, w))\n        tensor_conv2d(\n            *output.tuple(), output.size, *input.tuple(), *weight.tuple(), False\n        )\n        return output\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor]:\n        input, weight = ctx.saved_values\n        batch, in_channels, h, w = input.shape\n        out_channels, in_channels, kh, kw = weight.shape\n\n        grad_weight = grad_output.zeros((in_channels, out_channels, kh, kw))\n        new_input = input.permute(1, 0, 2, 3)\n        new_grad_output = grad_output.permute(1, 0, 2, 3)\n        tensor_conv2d(\n            *grad_weight.tuple(),\n            grad_weight.size,\n            *new_input.tuple(),\n            *new_grad_output.tuple(),\n            False,\n        )\n        grad_weight = grad_weight.permute(1, 0, 2, 3)\n\n        grad_input = input.zeros((batch, in_channels, h, w))\n        new_weight = weight.permute(1, 0, 2, 3)\n        tensor_conv2d(\n            *grad_input.tuple(),\n            grad_input.size,\n            *grad_output.tuple(),\n            *new_weight.tuple(),\n            True,\n        )\n        return grad_input, grad_weight\n\n\nconv2d = Conv2dFun.apply\n<\/code>","minitorch\/fast_ops.py":"<code> \nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom numba import njit, prange\n\nfrom .tensor_data import (\n    MAX_DIMS,\n    broadcast_index,\n    index_to_position,\n    shape_broadcast,\n    to_index,\n)\nfrom .tensor_ops import MapProto, TensorOps\n\nif TYPE_CHECKING:\n    from typing import Callable, Optional\n\n    from .tensor import Tensor\n    from .tensor_data import Index, Shape, Storage, Strides\n\n# TIP: Use `NUMBA_DISABLE_JIT=1 pytest tests\/ -m task3_1` to run these tests without JIT.\n\n# This code will JIT compile fast versions your tensor_data functions.\n# If you get an error, read the docs for NUMBA as to what is allowed\n# in these functions.\nto_index = njit(inline=\"always\")(to_index)\nindex_to_position = njit(inline=\"always\")(index_to_position)\nbroadcast_index = njit(inline=\"always\")(broadcast_index)\n\n\nclass FastOps(TensorOps):\n    @staticmethod\n    def map(fn: Callable[[float], float]) -> MapProto:\n        \"See `tensor_ops.py`\"\n\n        # This line JIT compiles your tensor_map\n        f = tensor_map(njit()(fn))\n\n        def ret(a: Tensor, out: Optional[Tensor] = None) -> Tensor:\n            if out is None:\n                out = a.zeros(a.shape)\n            f(*out.tuple(), *a.tuple())\n            return out\n\n        return ret\n\n    @staticmethod\n    def zip(fn: Callable[[float, float], float]) -> Callable[[Tensor, Tensor], Tensor]:\n        \"See `tensor_ops.py`\"\n\n        f = tensor_zip(njit()(fn))\n\n        def ret(a: Tensor, b: Tensor) -> Tensor:\n            c_shape = shape_broadcast(a.shape, b.shape)\n            out = a.zeros(c_shape)\n            f(*out.tuple(), *a.tuple(), *b.tuple())\n            return out\n\n        return ret\n\n    @staticmethod\n    def reduce(\n        fn: Callable[[float, float], float], start: float = 0.0\n    ) -> Callable[[Tensor, int], Tensor]:\n        \"See `tensor_ops.py`\"\n        f = tensor_reduce(njit()(fn))\n\n        def ret(a: Tensor, dim: int) -> Tensor:\n            out_shape = list(a.shape)\n            out_shape[dim] = 1\n\n            # Other values when not sum.\n            out = a.zeros(tuple(out_shape))\n            out._tensor._storage[:] = start\n\n            f(*out.tuple(), *a.tuple(), dim)\n            return out\n\n        return ret\n\n    @staticmethod\n    def matrix_multiply(a: Tensor, b: Tensor) -> Tensor:\n        \"\"\"\n        Batched tensor matrix multiply ::\n\n            for n:\n              for i:\n                for j:\n                  for k:\n                    out[n, i, j] += a[n, i, k] * b[n, k, j]\n\n        Where n indicates an optional broadcasted batched dimension.\n\n        Should work for tensor shapes of 3 dims ::\n\n            assert a.shape[-1] == b.shape[-2]\n\n        Args:\n            a : tensor data a\n            b : tensor data b\n\n        Returns:\n            New tensor data\n        \"\"\"\n\n        # Make these always be a 3 dimensional multiply\n        both_2d = 0\n        if len(a.shape) == 2:\n            a = a.contiguous().view(1, a.shape[0], a.shape[1])\n            both_2d += 1\n        if len(b.shape) == 2:\n            b = b.contiguous().view(1, b.shape[0], b.shape[1])\n            both_2d += 1\n        both_2d = both_2d == 2\n\n        ls = list(shape_broadcast(a.shape[:-2], b.shape[:-2]))\n        ls.append(a.shape[-2])\n        ls.append(b.shape[-1])\n        assert a.shape[-1] == b.shape[-2]\n        out = a.zeros(tuple(ls))\n\n        tensor_matrix_multiply(*out.tuple(), *a.tuple(), *b.tuple())\n\n        # Undo 3d if we added it.\n        if both_2d:\n            out = out.view(out.shape[1], out.shape[2])\n        return out\n\n\n# Implementations\n\n\ndef tensor_map(\n    fn: Callable[[float], float]\n) -> Callable[[Storage, Shape, Strides, Storage, Shape, Strides], None]:\n    \"\"\"\n    NUMBA low_level tensor_map function. See `tensor_ops.py` for description.\n\n    Optimizations:\n\n    * Main loop in parallel\n    * All indices use numpy buffers\n    * When `out` and `in` are stride-aligned, avoid indexing\n\n    Args:\n        fn: function mappings floats-to-floats to apply.\n\n    Returns:\n        Tensor map function.\n    \"\"\"\n\n    def _map(\n        out: Storage,\n        out_shape: Shape,\n        out_strides: Strides,\n        in_storage: Storage,\n        in_shape: Shape,\n        in_strides: Strides,\n    ) -> None:\n        for i in prange(out_shape[0]):\n            idx = index_to_position((i,), out_strides)\n            in_idx = index_to_position((i,), in_strides)\n            out[idx] = fn(in_storage[in_idx])\n\n    return njit(parallel=True)(_map)  # type: ignore\n\n\ndef tensor_zip(\n    fn: Callable[[float, float], float]\n) -> Callable[\n    [Storage, Shape, Strides, Storage, Shape, Strides, Storage, Shape, Strides], None\n]:\n    \"\"\"\n    NUMBA higher-order tensor zip function. See `tensor_ops.py` for description.\n\n\n    Optimizations:\n\n    * Main loop in parallel\n    * All indices use numpy buffers\n    * When `out`, `a`, `b` are stride-aligned, avoid indexing\n\n    Args:\n        fn: function maps two floats to float to apply.\n\n    Returns:\n        Tensor zip function.\n    \"\"\"\n\n    def _zip(\n        out: Storage,\n        out_shape: Shape,\n        out_strides: Strides,\n        a_storage: Storage,\n        a_shape: Shape,\n        a_strides: Strides,\n        b_storage: Storage,\n        b_shape: Shape,\n        b_strides: Strides,\n    ) -> None:\n        for i in prange(out_shape[0]):\n            idx = index_to_position((i,), out_strides)\n            a_idx = index_to_position((i,), a_strides)\n            b_idx = index_to_position((i,), b_strides)\n            out[idx] = fn(a_storage[a_idx], b_storage[b_idx])\n\n    return njit(parallel=True)(_zip)  # type: ignore\n\n\ndef tensor_reduce(\n    fn: Callable[[float, float], float]\n) -> Callable[[Storage, Shape, Strides, Storage, Shape, Strides, int], None]:\n    \"\"\"\n    NUMBA higher-order tensor reduce function. See `tensor_ops.py` for description.\n\n    Optimizations:\n\n    * Main loop in parallel\n    * All indices use numpy buffers\n    * Inner-loop should not call any functions or write non-local variables\n\n    Args:\n        fn: reduction function mapping two floats to float.\n\n    Returns:\n        Tensor reduce function\n    \"\"\"\n\n    def _reduce(\n        out: Storage,\n        out_shape: Shape,\n        out_strides: Strides,\n        a_storage: Storage,\n        a_shape: Shape,\n        a_strides: Strides,\n        reduce_dim: int,\n    ) -> None:\n        for i in prange(a_shape[0]):\n            idx = index_to_position((i,), out_strides)\n            for j in range(a_shape[1]):\n                a_idx = index_to_position((i, j), a_strides)\n                if j == 0:\n                    out[idx] = a_storage[a_idx]\n                else:\n                    out[idx] = fn(out[idx], a_storage[a_idx])\n\n    return njit(parallel=True)(_reduce)  # type: ignore\n\n\ndef _tensor_matrix_multiply(\n    out: Storage,\n    out_shape: Shape,\n    out_strides: Strides,\n    a_storage: Storage,\n    a_shape: Shape,\n    a_strides: Strides,\n    b_storage: Storage,\n    b_shape: Shape,\n    b_strides: Strides,\n) -> None:\n    \"\"\"\n    NUMBA tensor matrix multiply function.\n\n    Should work for any tensor shapes that broadcast as long as\n\n    ```\n    assert a_shape[-1] == b_shape[-2]\n    ```\n\n    Optimizations:\n\n    * Outer loop in parallel\n    * No index buffers or function calls\n    * Inner loop should have no global writes, 1 multiply.\n\n\n    Args:\n        out (Storage): storage for `out` tensor\n        out_shape (Shape): shape for `out` tensor\n        out_strides (Strides): strides for `out` tensor\n        a_storage (Storage): storage for `a` tensor\n        a_shape (Shape): shape for `a` tensor\n        a_strides (Strides): strides for `a` tensor\n        b_storage (Storage): storage for `b` tensor\n        b_shape (Shape): shape for `b` tensor\n        b_strides (Strides): strides for `b` tensor\n\n    Returns:\n        None : Fills in `out`\n    \"\"\"\n    n, m, p = out_shape[0], a_shape[1], b_shape[1]\n    for i in prange(n):\n        for j in range(p):\n            sum = 0.0\n            for k in range(m):\n                a_idx = index_to_position((i, k), a_strides)\n                b_idx = index_to_position((k, j), b_strides)\n                sum += a_storage[a_idx] * b_storage[b_idx]\n            out_idx = index_to_position((i, j), out_strides)\n            out[out_idx] = sum\n\ntensor_matrix_multiply = njit(parallel=True, fastmath=True)(_tensor_matrix_multiply)\n<\/code>","minitorch\/module.py":"<code> \nfrom __future__ import annotations\n\nfrom typing import Any, Dict, Optional, Sequence, Tuple\n\n\nclass Module:\n    \"\"\"\n    Modules form a tree that store parameters and other\n    submodules. They make up the basis of neural network stacks.\n\n    Attributes:\n        _modules : Storage of the child modules\n        _parameters : Storage of the module's parameters\n        training : Whether the module is in training mode or evaluation mode\n\n    \"\"\"\n\n    _modules: Dict[str, Module]\n    _parameters: Dict[str, Parameter]\n    training: bool\n\n    def __init__(self) -> None:\n        self._modules = {}\n        self._parameters = {}\n        self.training = True\n\n    def modules(self) -> Sequence[Module]:\n        \"Return the direct child modules of this module.\"\n        m: Dict[str, Module] = self.__dict__[\"_modules\"]\n        return list(m.values())\n\n    def train(self) -> None:\n        \"Set the mode of this module and all descendent modules to `train`.\"\n        self.training = True\n        for module in self.modules():\n            module.train()\n\n    def eval(self) -> None:\n        \"Set the mode of this module and all descendent modules to `eval`.\"\n        self.training = False\n        for module in self.modules():\n            module.eval()\n\n    def named_parameters(self) -> Sequence[Tuple[str, Parameter]]:\n        \"\"\"\n        Collect all the parameters of this module and its descendents.\n\n        Returns:\n            The name and `Parameter` of each ancestor parameter.\n        \"\"\"\n        params = []\n        for name, param in self._parameters.items():\n            params.append((name, param))\n        for module in self.modules():\n            for name, param in module.named_parameters():\n                params.append((f\"{module.__class__.__name__}.{name}\", param))\n        return params\n\n    def parameters(self) -> Sequence[Parameter]:\n        \"Enumerate over all the parameters of this module and its descendents.\"\n        params = list(self._parameters.values())\n        for module in self.modules():\n            params.extend(module.parameters())\n        return params\n\n    def add_parameter(self, k: str, v: Any) -> Parameter:\n        \"\"\"\n        Manually add a parameter. Useful helper for scalar parameters.\n\n        Args:\n            k: Local name of the parameter.\n            v: Value for the parameter.\n\n        Returns:\n            Newly created parameter.\n        \"\"\"\n        val = Parameter(v, k)\n        self.__dict__[\"_parameters\"][k] = val\n        return val\n\n    def __setattr__(self, key: str, val: Parameter) -> None:\n        if isinstance(val, Parameter):\n            self.__dict__[\"_parameters\"][key] = val\n        elif isinstance(val, Module):\n            self.__dict__[\"_modules\"][key] = val\n        else:\n            super().__setattr__(key, val)\n\n    def __getattr__(self, key: str) -> Any:\n        if key in self.__dict__[\"_parameters\"]:\n            return self.__dict__[\"_parameters\"][key]\n\n        if key in self.__dict__[\"_modules\"]:\n            return self.__dict__[\"_modules\"][key]\n        return None\n\n    def __call__(self, *args: Any, **kwargs: Any) -> Any:\n        return self.forward(*args, **kwargs)\n\n    def __repr__(self) -> str:\n        def _addindent(s_: str, numSpaces: int) -> str:\n            s2 = s_.split(\"\\n\")\n            if len(s2) == 1:\n                return s_\n            first = s2.pop(0)\n            s2 = [(numSpaces * \" \") + line for line in s2]\n            s = \"\\n\".join(s2)\n            s = first + \"\\n\" + s\n            return s\n\n        child_lines = []\n\n        for key, module in self._modules.items():\n            mod_str = repr(module)\n            mod_str = _addindent(mod_str, 2)\n            child_lines.append(\"(\" + key + \"): \" + mod_str)\n        lines = child_lines\n\n        main_str = self.__class__.__name__ + \"(\"\n        if lines:\n            # simple one-liner info, which most builtin Modules will use\n            main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n\n        main_str += \")\"\n        return main_str\n\n\nclass Parameter:\n    \"\"\"\n    A Parameter is a special container stored in a `Module`.\n\n    It is designed to hold a `Variable`, but we allow it to hold\n    any value for testing.\n    \"\"\"\n\n    def __init__(self, x: Any, name: Optional[str] = None) -> None:\n        self.value = x\n        self.name = name\n        if hasattr(x, \"requires_grad_\"):\n            self.value.requires_grad_(True)\n            if self.name:\n                self.value.name = self.name\n\n    def update(self, x: Any) -> None:\n        \"Update the parameter value.\"\n        self.value = x\n        if hasattr(x, \"requires_grad_\"):\n            self.value.requires_grad_(True)\n            if self.name:\n                self.value.name = self.name\n\n    def __repr__(self) -> str:\n        return repr(self.value)\n\n    def __str__(self) -> str:\n        return str(self.value)\n<\/code>","minitorch\/nn.py":"<code> \nfrom typing import Tuple\n\nfrom . import operators\nfrom .autodiff import Context\nfrom .fast_ops import FastOps\nfrom .tensor import Tensor\nfrom .tensor_functions import Function, rand, tensor\n\n\ndef tile(input: Tensor, kernel: Tuple[int, int]) -> Tuple[Tensor, int, int]:\n    \"\"\"\n    Reshape an image tensor for 2D pooling\n\n    Args:\n        input: batch x channel x height x width\n        kernel: height x width of pooling\n\n    Returns:\n        Tensor of size batch x channel x new_height x new_width x (kernel_height * kernel_width) as well as the new_height and new_width value.\n    \"\"\"\n\n    batch, channel, height, width = input.shape\n    kh, kw = kernel\n    assert height % kh == 0\n    assert width % kw == 0\n    \n    new_height = height \/\/ kh\n    new_width = width \/\/ kw\n    \n    # Reshape the input tensor\n    reshaped = input.view(batch, channel, new_height, kh, new_width, kw)\n    return reshaped, new_height, new_width\n\n\ndef avgpool2d(input: Tensor, kernel: Tuple[int, int]) -> Tensor:\n    \"\"\"\n    Tiled average pooling 2D\n\n    Args:\n        input : batch x channel x height x width\n        kernel : height x width of pooling\n\n    Returns:\n        Pooled tensor\n    \"\"\"\n    batch, channel, height, width = input.shape\n    kh, kw = kernel\n    new_height = height \/\/ kh\n    new_width = width \/\/ kw\n    \n    # Reshape and compute average\n    reshaped = input.view(batch, channel, new_height, kh, new_width, kw)\n    pooled = reshaped.mean(dim=(3, 5))  # Average over kernel dimensions\n    return pooled\n\n\nmax_reduce = FastOps.reduce(operators.max, -1e9)\n\n\ndef argmax(input: Tensor, dim: int) -> Tensor:\n    \"\"\"\n    Compute the argmax as a 1-hot tensor.\n\n    Args:\n        input : input tensor\n        dim : dimension to apply argmax\n\n    Returns:\n        :class:`Tensor` : tensor with 1 on highest cell in dim, 0 otherwise\n    \"\"\"\n    out = max_reduce(input, dim)\n    return out == input\n\n\nclass Max(Function):\n    @staticmethod\n    def forward(ctx: Context, input: Tensor, dim: int) -> Tensor:\n        \"Forward of max should be max reduction\"\n        ctx.save_for_backward(input)\n        return max_reduce(input, dim)\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, float]:\n        \"Backward of max should be argmax (see above)\"\n        input, = ctx.saved_values\n        grad_input = argmax(input, dim=0) * grad_output  # Gradient is passed back to the max position\n        return grad_input, None\n\n\ndef max(input: Tensor, dim: int) -> Tensor:\n    return Max.apply(input, dim)\n\n\ndef softmax(input: Tensor, dim: int) -> Tensor:\n    r\"\"\"\n    Compute the softmax as a tensor.\n\n    $z_i = \\frac{e^{x_i}}{\\sum_i e^{x_i}}$\n\n    Args:\n        input : input tensor\n        dim : dimension to apply softmax\n\n    Returns:\n        softmax tensor\n    \"\"\"\n    exp_input = operators.exp(input)\n    sum_exp = operators.sum(exp_input, dim=dim, keepdim=True)\n    return exp_input \/ sum_exp\n\n\ndef logsoftmax(input: Tensor, dim: int) -> Tensor:\n    r\"\"\"\n    Compute the log of the softmax as a tensor.\n\n    $z_i = x_i - \\log \\sum_i e^{x_i}$\n\n    See https:\/\/en.wikipedia.org\/wiki\/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\n\n    Args:\n        input : input tensor\n        dim : dimension to apply log-softmax\n\n    Returns:\n         log of softmax tensor\n    \"\"\"\n    max_input = operators.max(input, dim=dim, keepdim=True)\n    log_sum_exp = max_input + operators.log(operators.sum(operators.exp(input - max_input), dim=dim, keepdim=True))\n    return input - log_sum_exp\n\n\ndef maxpool2d(input: Tensor, kernel: Tuple[int, int]) -> Tensor:\n    \"\"\"\n    Tiled max pooling 2D\n\n    Args:\n        input: batch x channel x height x width\n        kernel: height x width of pooling\n\n    Returns:\n        Tensor : pooled tensor\n    \"\"\"\n    batch, channel, height, width = input.shape\n    kh, kw = kernel\n    new_height = height \/\/ kh\n    new_width = width \/\/ kw\n    \n    reshaped = input.view(batch, channel, new_height, kh, new_width, kw)\n    pooled = reshaped.max(dim=(3, 5)).values  # Max over kernel dimensions\n    return pooled\n\n\ndef dropout(input: Tensor, rate: float, ignore: bool = False) -> Tensor:\n    \"\"\"\n    Dropout positions based on random noise.\n\n    Args:\n        input : input tensor\n        rate : probability [0, 1) of dropping out each position\n        ignore : skip dropout, i.e. do nothing at all\n\n    Returns:\n        tensor with random positions dropped out\n    \"\"\"\n    if ignore:\n        return input\n    \n    mask = (rand(input.shape) > rate).float()  # Create a mask with the same shape as input\n    return input * mask  # Apply the mask to the input\n<\/code>","minitorch\/operators.py":"<code>\n\"\"\"\nCollection of the core mathematical operators used throughout the code base.\n\"\"\"\n\nimport math\nfrom typing import Callable, Iterable\n\n# ## Task 0.1\n#\n# Implementation of a prelude of elementary functions.\n\n\ndef mul(x: float, y: float) -> float:\n    \"$f(x, y) = x * y$\"\n    return x * y\n\n\ndef id(x: float) -> float:\n    \"$f(x) = x$\"\n    return x\n\n\ndef add(x: float, y: float) -> float:\n    \"$f(x, y) = x + y$\"\n    return x + y\n\n\ndef neg(x: float) -> float:\n    \"$f(x) = -x$\"\n    return -x\n\n\ndef lt(x: float, y: float) -> float:\n    \"$f(x) =$ 1.0 if x is less than y else 0.0\"\n    return 1.0 if x < y else 0.0\n\n\ndef eq(x: float, y: float) -> float:\n    \"$f(x) =$ 1.0 if x is equal to y else 0.0\"\n    return 1.0 if x == y else 0.0\n\n\ndef max(x: float, y: float) -> float:\n    \"$f(x) =$ x if x is greater than y else y\"\n    return x if x > y else y\n\n\ndef is_close(x: float, y: float) -> float:\n    \"$f(x) = |x - y| < 1e-2$\"\n    return 1.0 if abs(x - y) < 1e-2 else 0.0\n\n\ndef sigmoid(x: float) -> float:\n    r\"\"\"\n    $f(x) =  \\frac{1.0}{(1.0 + e^{-x})}$\n\n    (See https:\/\/en.wikipedia.org\/wiki\/Sigmoid_function )\n\n    Calculate as\n\n    $f(x) =  \\frac{1.0}{(1.0 + e^{-x})}$ if x >=0 else $\\frac{e^x}{(1.0 + e^{x})}$\n\n    for stability.\n    \"\"\"\n    if x >= 0:\n        return 1.0 \/ (1.0 + math.exp(-x))\n    else:\n        return math.exp(x) \/ (1.0 + math.exp(x))\n\n\ndef relu(x: float) -> float:\n    \"\"\"\n    $f(x) =$ x if x is greater than 0, else 0\n\n    (See https:\/\/en.wikipedia.org\/wiki\/Rectifier_(neural_networks) .)\n    \"\"\"\n    return x if x > 0 else 0\n\n\nEPS = 1e-6\n\n\ndef log(x: float) -> float:\n    \"$f(x) = log(x)$\"\n    return math.log(x + EPS)\n\n\ndef exp(x: float) -> float:\n    \"$f(x) = e^{x}$\"\n    return math.exp(x)\n\n\ndef log_back(x: float, d: float) -> float:\n    r\"If $f = log$ as above, compute $d \\times f'(x)$\"\n    return d \/ (x + EPS)\n\n\ndef inv(x: float) -> float:\n    \"$f(x) = 1\/x$\"\n    return 1.0 \/ (x + EPS)\n\n\ndef inv_back(x: float, d: float) -> float:\n    r\"If $f(x) = 1\/x$ compute $d \\times f'(x)$\"\n    return -d \/ ((x + EPS) ** 2)\n\n\ndef relu_back(x: float, d: float) -> float:\n    r\"If $f = relu$ compute $d \\times f'(x)$\"\n    return d if x > 0 else 0\n\n\n# ## Task 0.3\n\n# Small practice library of elementary higher-order functions.\n\n\ndef map(fn: Callable[[float], float]) -> Callable[[Iterable[float]], Iterable[float]]:\n    \"\"\"\n    Higher-order map.\n\n    See https:\/\/en.wikipedia.org\/wiki\/Map_(higher-order_function)\n\n    Args:\n        fn: Function from one value to one value.\n\n    Returns:\n         A function that takes a list, applies `fn` to each element, and returns a\n         new list\n    \"\"\"\n    def _map(input: Iterable[float]) -> Iterable[float]:\n        return [fn(x) for x in input]\n    \n    return _map\n\n\ndef negList(ls: Iterable[float]) -> Iterable[float]:\n    \"Use `map` and `neg` to negate each element in `ls`\"\n    return map(neg)(ls)\n\n\ndef zipWith(\n    fn: Callable[[float, float], float]\n) -> Callable[[Iterable[float], Iterable[float]], Iterable[float]]:\n    \"\"\"\n    Higher-order zipwith (or map2).\n\n    See https:\/\/en.wikipedia.org\/wiki\/Map_(higher-order_function)\n\n    Args:\n        fn: combine two values\n\n    Returns:\n         Function that takes two equally sized lists `ls1` and `ls2`, produce a new list by\n         applying fn(x, y) on each pair of elements.\n\n    \"\"\"\n    def _zipWith(ls1: Iterable[float], ls2: Iterable[float]) -> Iterable[float]:\n        return [fn(x, y) for x, y in zip(ls1, ls2)]\n    \n    return _zipWith\n\n\ndef addLists(ls1: Iterable[float], ls2: Iterable[float]) -> Iterable[float]:\n    \"Add the elements of `ls1` and `ls2` using `zipWith` and `add`\"\n    return zipWith(add)(ls1, ls2)\n\n\ndef reduce(\n    fn: Callable[[float, float], float], start: float\n) -> Callable[[Iterable[float]], float]:\n    r\"\"\"\n    Higher-order reduce.\n\n    Args:\n        fn: combine two values\n        start: start value $x_0$\n\n    Returns:\n         Function that takes a list `ls` of elements\n         $x_1 \\ldots x_n$ and computes the reduction :math:`fn(x_3, fn(x_2,\n         fn(x_1, x_0)))`\n    \"\"\"\n    def _reduce(ls: Iterable[float]) -> float:\n        result = start\n        for x in ls:\n            result = fn(result, x)\n        return result\n    \n    return _reduce\n\n\ndef sum(ls: Iterable[float]) -> float:\n    \"Sum up a list using `reduce` and `add`.\"\n    return reduce(add, 0)(ls)\n\n\ndef prod(ls: Iterable[float]) -> float:\n    \"Product of a list using `reduce` and `mul`.\"\n    return reduce(mul, 1)(ls)\n<\/code>","minitorch\/scalar.py":"<code> \nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Any, Iterable, Optional, Sequence, Tuple, Type, Union\n\nimport numpy as np\n\nfrom .autodiff import Context, Variable, backpropagate, central_difference\nfrom .scalar_functions import (\n    EQ,\n    LT,\n    Add,\n    Exp,\n    Inv,\n    Log,\n    Mul,\n    Neg,\n    ReLU,\n    ScalarFunction,\n    Sigmoid,\n)\n\nScalarLike = Union[float, int, \"Scalar\"]\n\n\n@dataclass\nclass ScalarHistory:\n    \"\"\"\n    `ScalarHistory` stores the history of `Function` operations that was\n    used to construct the current Variable.\n\n    Attributes:\n        last_fn : The last Function that was called.\n        ctx : The context for that Function.\n        inputs : The inputs that were given when `last_fn.forward` was called.\n\n    \"\"\"\n\n    last_fn: Optional[Type[ScalarFunction]] = None\n    ctx: Optional[Context] = None\n    inputs: Sequence[Scalar] = ()\n\n\n# ## Task 1.2 and 1.4\n# Scalar Forward and Backward\n\n_var_count = 0\n\n\nclass Scalar:\n    \"\"\"\n    A reimplementation of scalar values for autodifferentiation\n    tracking. Scalar Variables behave as close as possible to standard\n    Python numbers while also tracking the operations that led to the\n    number's creation. They can only be manipulated by\n    `ScalarFunction`.\n    \"\"\"\n\n    history: Optional[ScalarHistory]\n    derivative: Optional[float]\n    data: float\n    unique_id: int\n    name: str\n\n    def __init__(\n        self,\n        v: float,\n        back: ScalarHistory = ScalarHistory(),\n        name: Optional[str] = None,\n    ):\n        global _var_count\n        _var_count += 1\n        self.unique_id = _var_count\n        self.data = float(v)\n        self.history = back\n        self.derivative = None\n        if name is not None:\n            self.name = name\n        else:\n            self.name = str(self.unique_id)\n\n    def __repr__(self) -> str:\n        return \"Scalar(%f)\" % self.data\n\n    def __mul__(self, b: ScalarLike) -> Scalar:\n        return Mul.apply(self, b)\n\n    def __truediv__(self, b: ScalarLike) -> Scalar:\n        return Mul.apply(self, Inv.apply(b))\n\n    def __rtruediv__(self, b: ScalarLike) -> Scalar:\n        return Mul.apply(b, Inv.apply(self))\n\n    def __add__(self, b: ScalarLike) -> Scalar:\n        return Add.apply(self, b)\n\n    def __bool__(self) -> bool:\n        return bool(self.data)\n\n    def __lt__(self, b: ScalarLike) -> Scalar:\n        return LT.apply(self, b)\n\n    def __gt__(self, b: ScalarLike) -> Scalar:\n        return LT.apply(b, self)\n\n    def __eq__(self, b: ScalarLike) -> Scalar:  # type: ignore[override]\n        return EQ.apply(self, b)\n\n    def __sub__(self, b: ScalarLike) -> Scalar:\n        return Add.apply(self, Neg.apply(b))\n\n    def __neg__(self) -> Scalar:\n        return Neg.apply(self)\n\n    def __radd__(self, b: ScalarLike) -> Scalar:\n        return self + b\n\n    def __rmul__(self, b: ScalarLike) -> Scalar:\n        return self * b\n\n    def log(self) -> Scalar:\n        return Log.apply(self)\n\n    def exp(self) -> Scalar:\n        return Exp.apply(self)\n\n    def sigmoid(self) -> Scalar:\n        return Sigmoid.apply(self)\n\n    def relu(self) -> Scalar:\n        return ReLU.apply(self)\n\n    # Variable elements for backprop\n\n    def accumulate_derivative(self, x: Any) -> None:\n        \"\"\"\n        Add `val` to the the derivative accumulated on this variable.\n        Should only be called during autodifferentiation on leaf variables.\n\n        Args:\n            x: value to be accumulated\n        \"\"\"\n        assert self.is_leaf(), \"Only leaf variables can have derivatives.\"\n        if self.derivative is None:\n            self.derivative = 0.0\n        self.derivative += x\n\n    def is_leaf(self) -> bool:\n        \"True if this variable created by the user (no `last_fn`)\"\n        return self.history is not None and self.history.last_fn is None\n\n    def is_constant(self) -> bool:\n        return self.history is None\n\n    @property\n    def parents(self) -> Iterable[Variable]:\n        assert self.history is not None\n        return self.history.inputs\n\n    def chain_rule(self, d_output: Any) -> Iterable[Tuple[Variable, Any]]:\n        h = self.history\n        assert h is not None\n        assert h.last_fn is not None\n        assert h.ctx is not None\n\n        return h.last_fn.backward(h.ctx, d_output)\n\n    def backward(self, d_output: Optional[float] = None) -> None:\n        \"\"\"\n        Calls autodiff to fill in the derivatives for the history of this object.\n\n        Args:\n            d_output (number, opt): starting derivative to backpropagate through the model\n                                   (typically left out, and assumed to be 1.0).\n        \"\"\"\n        if d_output is None:\n            d_output = 1.0\n        backpropagate(self, d_output)\n\n\ndef derivative_check(f: Any, *scalars: Scalar) -> None:\n    \"\"\"\n    Checks that autodiff works on a python function.\n    Asserts False if derivative is incorrect.\n\n    Parameters:\n        f : function from n-scalars to 1-scalar.\n        *scalars  : n input scalar values.\n    \"\"\"\n    out = f(*scalars)\n    out.backward()\n\n    err_msg = \"\"\"\nDerivative check at arguments f(%s) and received derivative f'=%f for argument %d,\nbut was expecting derivative f'=%f from central difference.\"\"\"\n    for i, x in enumerate(scalars):\n        check = central_difference(f, *scalars, arg=i)\n        print(str([x.data for x in scalars]), x.derivative, i, check)\n        assert x.derivative is not None\n        np.testing.assert_allclose(\n            x.derivative,\n            check.data,\n            1e-2,\n            1e-2,\n            err_msg=err_msg\n            % (str([x.data for x in scalars]), x.derivative, i, check.data),\n        )\n<\/code>","minitorch\/scalar_functions.py":"```python\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nimport minitorch\n\nfrom . import operators\nfrom .autodiff import Context\n\nif TYPE_CHECKING:\n    from typing import Tuple\n\n    from .scalar import Scalar, ScalarLike\n\n\ndef wrap_tuple(x):  # type: ignore\n    \"Turn a possible value into a tuple\"\n    if isinstance(x, tuple):\n        return x\n    return (x,)\n\n\ndef unwrap_tuple(x):  # type: ignore\n    \"Turn a singleton tuple into a value\"\n    if len(x) == 1:\n        return x[0]\n    return x\n\n\nclass ScalarFunction:\n    \"\"\"\n    A wrapper for a mathematical function that processes and produces\n    Scalar variables.\n\n    This is a static class and is never instantiated. We use `class`\n    here to group together the `forward` and `backward` code.\n    \"\"\"\n\n    @classmethod\n    def _backward(cls, ctx: Context, d_out: float) -> Tuple[float, ...]:\n        return wrap_tuple(cls.backward(ctx, d_out))  # type: ignore\n\n    @classmethod\n    def _forward(cls, ctx: Context, *inps: float) -> float:\n        return cls.forward(ctx, *inps)  # type: ignore\n\n    @classmethod\n    def apply(cls, *vals: \"ScalarLike\") -> Scalar:\n        raw_vals = []\n        scalars = []\n        for v in vals:\n            if isinstance(v, minitorch.scalar.Scalar):\n                scalars.append(v)\n                raw_vals.append(v.data)\n            else:\n                scalars.append(minitorch.scalar.Scalar(v))\n                raw_vals.append(v)\n\n        # Create the context.\n        ctx = Context(False)\n\n        # Call forward with the variables.\n        c = cls._forward(ctx, *raw_vals)\n        assert isinstance(c, float), \"Expected return type float got %s\" % (type(c))\n\n        # Create a new variable from the result with a new history.\n        back = minitorch.scalar.ScalarHistory(cls, ctx, scalars)\n        return minitorch.scalar.Scalar(c, back)\n\n\n# Examples\nclass Add(ScalarFunction):\n    \"Addition function $f(x, y) = x + y$\"\n\n    @staticmethod\n    def forward(ctx: Context, a: float, b: float) -> float:\n        return a + b\n\n    @staticmethod\n    def backward(ctx: Context, d_output: float) -> Tuple[float, ...]:\n        return d_output, d_output\n\n\nclass Log(ScalarFunction):\n    \"Log function $f(x) = log(x)$\"\n\n    @staticmethod\n    def forward(ctx: Context, a: float) -> float:\n        ctx.save_for_backward(a)\n        return operators.log(a)\n\n    @staticmethod\n    def backward(ctx: Context, d_output: float) -> float:\n        (a,) = ctx.saved_values\n        return operators.log_back(a, d_output)\n\n\n# To implement.\n\n\nclass Mul(ScalarFunction):\n    \"Multiplication function\"\n\n    @staticmethod\n    def forward(ctx: Context, a: float, b: float) -> float:\n        ctx.save_for_backward(a, b)\n        return a * b\n\n    @staticmethod\n    def backward(ctx: Context, d_output: float) -> Tuple[float, float]:\n        (a, b) = ctx.saved_values\n        return b * d_output, a * d_output\n\n\nclass Inv(ScalarFunction):\n    \"Inverse function\"\n\n    @staticmethod\n    def forward(ctx: Context, a: float) -> float:\n        ctx.save_for_backward(a)\n        return operators.inv(a)\n\n    @staticmethod\n    def backward(ctx: Context, d_output: float) -> float:\n        (a,) = ctx.saved_values\n        return operators.inv_back(a, d_output)\n\n\nclass Neg(ScalarFunction):\n    \"Negation function\"\n\n    @staticmethod\n    def forward(ctx: Context, a: float) -> float:\n        ctx.save_for_backward(a)\n        return -a\n\n    @staticmethod\n    def backward(ctx: Context, d_output: float) -> float:\n        return -d_output\n\n\nclass Sigmoid(ScalarFunction):\n    \"Sigmoid function\"\n\n    @staticmethod\n    def forward(ctx: Context, a: float) -> float:\n        ctx.save_for_backward(a)\n        return operators.sigmoid(a)\n\n    @staticmethod\n    def backward(ctx: Context, d_output: float) -> float:\n        (a,) = ctx.saved_values\n        return operators.sigmoid_back(a, d_output)\n\n\nclass ReLU(ScalarFunction):\n    \"ReLU function\"\n\n    @staticmethod\n    def forward(ctx: Context, a: float) -> float:\n        ctx.save_for_backward(a)\n        return operators.relu(a)\n\n    @staticmethod\n    def backward(ctx: Context, d_output: float) -> float:\n        (a,) = ctx.saved_values\n        return operators.relu_back(a, d_output)\n\n\nclass Exp(ScalarFunction):\n    \"Exp function\"\n\n    @staticmethod\n    def forward(ctx: Context, a: float) -> float:\n        ctx.save_for_backward(a)\n        return operators.exp(a)\n\n    @staticmethod\n    def backward(ctx: Context, d_output: float) -> float:\n        (a,) = ctx.saved_values\n        return operators.exp_back(a, d_output)\n\n\nclass LT(ScalarFunction):\n    \"Less-than function $f(x) =$ 1.0 if x is less than y else 0.0\"\n\n    @staticmethod\n    def forward(ctx: Context, a: float, b: float) -> float:\n        ctx.save_for_backward(a, b)\n        return operators.lt(a, b)\n\n    @staticmethod\n    def backward(ctx: Context, d_output: float) -> Tuple[float, float]:\n        (a, b) = ctx.saved_values\n        return (d_output if a < b else 0.0, -d_output if a < b else 0.0)\n\n\nclass EQ(ScalarFunction):\n    \"Equal function $f(x) =$ 1.0 if x is equal to y else 0.0\"\n\n    @staticmethod\n    def forward(ctx: Context, a: float, b: float) -> float:\n        ctx.save_for_backward(a, b)\n        return operators.eq(a, b)\n\n    @staticmethod\n    def backward(ctx: Context, d_output: float) -> Tuple[float, float]:\n        (a, b) = ctx.saved_values\n        return (d_output if a == b else 0.0, d_output if a == b else 0.0)\n```","minitorch\/tensor_data.py":"```python\nfrom __future__ import annotations\n\nimport random\nfrom typing import Iterable, Optional, Sequence, Tuple, Union\n\nimport numba\nimport numpy as np\nimport numpy.typing as npt\nfrom numpy import array, float64\nfrom typing_extensions import TypeAlias\n\nfrom .operators import prod\n\nMAX_DIMS = 32\n\n\nclass IndexingError(RuntimeError):\n    \"Exception raised for indexing errors.\"\n    pass\n\n\nStorage: TypeAlias = npt.NDArray[np.float64]\nOutIndex: TypeAlias = npt.NDArray[np.int32]\nIndex: TypeAlias = npt.NDArray[np.int32]\nShape: TypeAlias = npt.NDArray[np.int32]\nStrides: TypeAlias = npt.NDArray[np.int32]\n\nUserIndex: TypeAlias = Sequence[int]\nUserShape: TypeAlias = Sequence[int]\nUserStrides: TypeAlias = Sequence[int]\n\n\ndef index_to_position(index: Index, strides: Strides) -> int:\n    \"\"\"\n    Converts a multidimensional tensor `index` into a single-dimensional position in\n    storage based on strides.\n\n    Args:\n        index : index tuple of ints\n        strides : tensor strides\n\n    Returns:\n        Position in storage\n    \"\"\"\n    position = 0\n    for i in range(len(index)):\n        position += index[i] * strides[i]\n    return position\n\n\ndef to_index(ordinal: int, shape: Shape, out_index: OutIndex) -> None:\n    \"\"\"\n    Convert an `ordinal` to an index in the `shape`.\n    Should ensure that enumerating position 0 ... size of a\n    tensor produces every index exactly once. It\n    may not be the inverse of `index_to_position`.\n\n    Args:\n        ordinal: ordinal position to convert.\n        shape : tensor shape.\n        out_index : return index corresponding to position.\n\n    \"\"\"\n    for i in reversed(range(len(shape))):\n        out_index[i] = ordinal % shape[i]\n        ordinal \/\/= shape[i]\n\n\ndef broadcast_index(\n    big_index: Index, big_shape: Shape, shape: Shape, out_index: OutIndex\n) -> None:\n    \"\"\"\n    Convert a `big_index` into `big_shape` to a smaller `out_index`\n    into `shape` following broadcasting rules. In this case\n    it may be larger or with more dimensions than the `shape`\n    given. Additional dimensions may need to be mapped to 0 or\n    removed.\n\n    Args:\n        big_index : multidimensional index of bigger tensor\n        big_shape : tensor shape of bigger tensor\n        shape : tensor shape of smaller tensor\n        out_index : multidimensional index of smaller tensor\n\n    Returns:\n        None\n    \"\"\"\n    for i in range(len(shape)):\n        if shape[i] == 1:\n            out_index[i] = 0  # Broadcasting: use the first index\n        else:\n            out_index[i] = big_index[i]\n\n\ndef shape_broadcast(shape1: UserShape, shape2: UserShape) -> UserShape:\n    \"\"\"\n    Broadcast two shapes to create a new union shape.\n\n    Args:\n        shape1 : first shape\n        shape2 : second shape\n\n    Returns:\n        broadcasted shape\n\n    Raises:\n        IndexingError : if cannot broadcast\n    \"\"\"\n    result_shape = []\n    len1, len2 = len(shape1), len(shape2)\n    for i in range(max(len1, len2)):\n        dim1 = shape1[len1 - 1 - i] if i < len1 else 1\n        dim2 = shape2[len2 - 1 - i] if i < len2 else 1\n        if dim1 == dim2 or dim1 == 1 or dim2 == 1:\n            result_shape.append(max(dim1, dim2))\n        else:\n            raise IndexingError(f\"Cannot broadcast shapes {shape1} and {shape2}.\")\n    return tuple(reversed(result_shape))\n\n\ndef strides_from_shape(shape: UserShape) -> UserStrides:\n    layout = [1]\n    offset = 1\n    for s in reversed(shape):\n        layout.append(s * offset)\n        offset = s * offset\n    return tuple(reversed(layout[:-1]))\n\n\nclass TensorData:\n    _storage: Storage\n    _strides: Strides\n    _shape: Shape\n    strides: UserStrides\n    shape: UserShape\n    dims: int\n\n    def __init__(\n        self,\n        storage: Union[Sequence[float], Storage],\n        shape: UserShape,\n        strides: Optional[UserStrides] = None,\n    ):\n        if isinstance(storage, np.ndarray):\n            self._storage = storage\n        else:\n            self._storage = array(storage, dtype=float64)\n\n        if strides is None:\n            strides = strides_from_shape(shape)\n\n        assert isinstance(strides, tuple), \"Strides must be tuple\"\n        assert isinstance(shape, tuple), \"Shape must be tuple\"\n        if len(strides) != len(shape):\n            raise IndexingError(f\"Len of strides {strides} must match {shape}.\")\n        self._strides = array(strides)\n        self._shape = array(shape)\n        self.strides = strides\n        self.dims = len(strides)\n        self.size = int(prod(shape))\n        self.shape = shape\n        assert len(self._storage) == self.size\n\n    def to_cuda_(self) -> None:  # pragma: no cover\n        if not numba.cuda.is_cuda_array(self._storage):\n            self._storage = numba.cuda.to_device(self._storage)\n\n    def is_contiguous(self) -> bool:\n        \"\"\"\n        Check that the layout is contiguous, i.e. outer dimensions have bigger strides than inner dimensions.\n\n        Returns:\n            bool : True if contiguous\n        \"\"\"\n        last = 1e9\n        for stride in self._strides:\n            if stride > last:\n                return False\n            last = stride\n        return True\n\n    @staticmethod\n    def shape_broadcast(shape_a: UserShape, shape_b: UserShape) -> UserShape:\n        return shape_broadcast(shape_a, shape_b)\n\n    def index(self, index: Union[int, UserIndex]) -> int:\n        if isinstance(index, int):\n            aindex: Index = array([index])\n        if isinstance(index, tuple):\n            aindex = array(index)\n\n        # Pretend 0-dim shape is 1-dim shape of singleton\n        shape = self.shape\n        if len(shape) == 0 and len(aindex) != 0:\n            shape = (1,)\n\n        # Check for errors\n        if aindex.shape[0] != len(self.shape):\n            raise IndexingError(f\"Index {aindex} must be size of {self.shape}.\")\n        for i, ind in enumerate(aindex):\n            if ind >= self.shape[i]:\n                raise IndexingError(f\"Index {aindex} out of range {self.shape}.\")\n            if ind < 0:\n                raise IndexingError(f\"Negative indexing for {aindex} not supported.\")\n\n        # Call fast indexing.\n        return index_to_position(array(index), self._strides)\n\n    def indices(self) -> Iterable[UserIndex]:\n        lshape: Shape = array(self.shape)\n        out_index: Index = array(self.shape)\n        for i in range(self.size):\n            to_index(i, lshape, out_index)\n            yield tuple(out_index)\n\n    def sample(self) -> UserIndex:\n        return tuple((random.randint(0, s - 1) for s in self.shape))\n\n    def get(self, key: UserIndex) -> float:\n        x: float = self._storage[self.index(key)]\n        return x\n\n    def set(self, key: UserIndex, val: float) -> None:\n        self._storage[self.index(key)] = val\n\n    def tuple(self) -> Tuple[Storage, Shape, Strides]:\n        return (self._storage, self._shape, self._strides)\n\n    def permute(self, *order: int) -> TensorData:\n        \"\"\"\n        Permute the dimensions of the tensor.\n\n        Args:\n            *order: a permutation of the dimensions\n\n        Returns:\n            New `TensorData` with the same storage and a new dimension order.\n        \"\"\"\n        assert list(sorted(order)) == list(\n            range(len(self.shape))\n        ), f\"Must give a position to each dimension. Shape: {self.shape} Order: {order}\"\n\n        # Create a new shape based on the order\n        new_shape = tuple(self.shape[i] for i in order)\n        new_strides = tuple(self.strides[i] for i in order)\n\n        return TensorData(self._storage, new_shape, new_strides)\n\n    def to_string(self) -> str:\n        s = \"\"\n        for index in self.indices():\n            l = \"\"\n            for i in range(len(index) - 1, -1, -1):\n                if index[i] == 0:\n                    l = \"\\n%s[\" % (\"\\t\" * i) + l\n                else:\n                    break\n            s += l\n            v = self.get(index)\n            s += f\"{v:3.2f}\"\n            l = \"\"\n            for i in range(len(index) - 1, -1, -1):\n                if index[i] == self.shape[i] - 1:\n                    l += \"]\"\n                else:\n                    break\n            if l:\n                s += l\n            else:\n                s += \" \"\n        return s\n```","minitorch\/tensor_functions.py":"```python\n\"\"\"\nImplementation of the autodifferentiation Functions for Tensor.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport random\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\n\nimport minitorch\n\nfrom . import operators\nfrom .autodiff import Context\nfrom .tensor_ops import SimpleBackend, TensorBackend\n\nif TYPE_CHECKING:\n    from typing import Any, List, Tuple\n\n    from .tensor import Tensor\n    from .tensor_data import UserIndex, UserShape\n\n\ndef wrap_tuple(x):  # type: ignore\n    \"Turn a possible value into a tuple\"\n    if isinstance(x, tuple):\n        return x\n    return (x,)\n\n\n# Constructors\nclass Function:\n    @classmethod\n    def _backward(cls, ctx: Context, grad_out: Tensor) -> Tuple[Tensor, ...]:\n        return wrap_tuple(cls.backward(ctx, grad_out))  # type: ignore\n\n    @classmethod\n    def _forward(cls, ctx: Context, *inps: Tensor) -> Tensor:\n        return cls.forward(ctx, *inps)  # type: ignore\n\n    @classmethod\n    def apply(cls, *vals: Tensor) -> Tensor:\n        raw_vals = []\n        need_grad = False\n        for v in vals:\n            if v.requires_grad():\n                need_grad = True\n            raw_vals.append(v.detach())\n\n        # Create the context.\n        ctx = Context(not need_grad)\n\n        # Call forward with the variables.\n        c = cls._forward(ctx, *raw_vals)\n        # assert isinstance(c, Tensor), \"Expected return type Tensor got %s\" % (\n        #     type(c)\n        # )\n\n        # Create a new variable from the result with a new history.\n        back = None\n        if need_grad:\n            back = minitorch.History(cls, ctx, vals)\n        return minitorch.Tensor(c._tensor, back, backend=c.backend)\n\n\nclass Neg(Function):\n    @staticmethod\n    def forward(ctx: Context, t1: Tensor) -> Tensor:\n        return t1.f.neg_map(t1)\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tensor:\n        return grad_output.f.neg_map(grad_output)\n\n\nclass Inv(Function):\n    @staticmethod\n    def forward(ctx: Context, t1: Tensor) -> Tensor:\n        ctx.save_for_backward(t1)\n        return t1.f.inv_map(t1)\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tensor:\n        (t1,) = ctx.saved_values\n        return grad_output.f.inv_back_zip(t1, grad_output)\n\n\nclass Add(Function):\n    @staticmethod\n    def forward(ctx: Context, t1: Tensor, t2: Tensor) -> Tensor:\n        return t1.f.add_zip(t1, t2)\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor]:\n        return grad_output, grad_output\n\n\nclass Mul(Function):\n    @staticmethod\n    def forward(ctx: Context, a: Tensor, b: Tensor) -> Tensor:\n        ctx.save_for_backward(a, b)\n        return a.f.mul_zip(a, b)\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor]:\n        a, b = ctx.saved_values\n        return grad_output.f.mul_zip(grad_output, b), grad_output.f.mul_zip(a, grad_output)\n\n\nclass Sigmoid(Function):\n    @staticmethod\n    def forward(ctx: Context, t1: Tensor) -> Tensor:\n        ctx.save_for_backward(t1)\n        return t1.f.sigmoid(t1)\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tensor:\n        (t1,) = ctx.saved_values\n        return grad_output.f.sigmoid_back(t1, grad_output)\n\n\nclass ReLU(Function):\n    @staticmethod\n    def forward(ctx: Context, t1: Tensor) -> Tensor:\n        return t1.f.relu(t1)\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tensor:\n        return grad_output.f.relu_back(t1, grad_output)\n\n\nclass Log(Function):\n    @staticmethod\n    def forward(ctx: Context, t1: Tensor) -> Tensor:\n        ctx.save_for_backward(t1)\n        return t1.f.log_map(t1)\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tensor:\n        (t1,) = ctx.saved_values\n        return grad_output.f.log_back(t1, grad_output)\n\n\nclass Exp(Function):\n    @staticmethod\n    def forward(ctx: Context, t1: Tensor) -> Tensor:\n        ctx.save_for_backward(t1)\n        return t1.f.exp_map(t1)\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tensor:\n        (t1,) = ctx.saved_values\n        return grad_output.f.exp_back(t1, grad_output)\n\n\nclass Sum(Function):\n    @staticmethod\n    def forward(ctx: Context, a: Tensor, dim: Tensor) -> Tensor:\n        ctx.save_for_backward(a.shape, dim)\n        return a.f.add_reduce(a, int(dim.item()))\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, float]:\n        a_shape, dim = ctx.saved_values\n        return grad_output, 0.0\n\n\nclass All(Function):\n    @staticmethod\n    def forward(ctx: Context, a: Tensor, dim: Tensor) -> Tensor:\n        if dim is not None:\n            return a.f.mul_reduce(a, int(dim.item()))\n        else:\n            return a.f.mul_reduce(a.contiguous().view(int(operators.prod(a.shape))), 0)\n\n\nclass LT(Function):\n    @staticmethod\n    def forward(ctx: Context, a: Tensor, b: Tensor) -> Tensor:\n        return a.f.lt_zip(a, b)\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor]:\n        return grad_output.f.lt_back(a, b, grad_output), None\n\n\nclass EQ(Function):\n    @staticmethod\n    def forward(ctx: Context, a: Tensor, b: Tensor) -> Tensor:\n        return a.f.eq_zip(a, b)\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor]:\n        return grad_output.f.eq_back(a, b, grad_output), None\n\n\nclass IsClose(Function):\n    @staticmethod\n    def forward(ctx: Context, a: Tensor, b: Tensor) -> Tensor:\n        return a.f.is_close(a, b)\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor]:\n        return grad_output.f.is_close_back(a, b, grad_output), None\n\n\nclass Permute(Function):\n    @staticmethod\n    def forward(ctx: Context, a: Tensor, order: Tensor) -> Tensor:\n        ctx.save_for_backward(a.shape)\n        return a.f.permute(a, order)\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, float]:\n        (original,) = ctx.saved_values\n        return (\n            grad_output.f.permute_back(grad_output, original),\n            0.0,\n        )\n\n\nclass View(Function):\n    @staticmethod\n    def forward(ctx: Context, a: Tensor, shape: Tensor) -> Tensor:\n        ctx.save_for_backward(a.shape)\n        assert a._tensor.is_contiguous(), \"Must be contiguous to view\"\n        shape2 = [int(shape[i]) for i in range(shape.size)]\n        return minitorch.Tensor.make(\n            a._tensor._storage, tuple(shape2), backend=a.backend\n        )\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, float]:\n        (original,) = ctx.saved_values\n        return (\n            minitorch.Tensor.make(\n                grad_output._tensor._storage, original, backend=grad_output.backend\n            ),\n            0.0,\n        )\n\n\nclass Copy(Function):\n    @staticmethod\n    def forward(ctx: Context, a: Tensor) -> Tensor:\n        return a.f.id_map(a)\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tensor:\n        return grad_output\n\n\nclass MatMul(Function):\n    @staticmethod\n    def forward(ctx: Context, t1: Tensor, t2: Tensor) -> Tensor:\n        ctx.save_for_backward(t1, t2)\n        return t1.f.matrix_multiply(t1, t2)\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor]:\n        t1, t2 = ctx.saved_values\n\n        def transpose(a: Tensor) -> Tensor:\n            order = list(range(a.dims))\n            order[-2], order[-1] = order[-1], order[-2]\n            return a._new(a._tensor.permute(*order))\n\n        return (\n            grad_output.f.matrix_multiply(grad_output, transpose(t2)),\n            grad_output.f.matrix_multiply(transpose(t1), grad_output),\n        )\n\n\n# Helpers for Constructing tensors\ndef zeros(shape: UserShape, backend: TensorBackend = SimpleBackend) -> Tensor:\n    \"\"\"\n    Produce a zero tensor of size `shape`.\n\n    Args:\n        shape : shape of tensor\n        backend : tensor backend\n\n    Returns:\n        new tensor\n    \"\"\"\n    return minitorch.Tensor.make(\n        [0] * int(operators.prod(shape)), shape, backend=backend\n    )\n\n\ndef rand(\n    shape: UserShape,\n    backend: TensorBackend = SimpleBackend,\n    requires_grad: bool = False,\n) -> Tensor:\n    \"\"\"\n    Produce a random tensor of size `shape`.\n\n    Args:\n        shape : shape of tensor\n        backend : tensor backend\n        requires_grad : turn on autodifferentiation\n\n    Returns:\n        :class:`Tensor` : new tensor\n    \"\"\"\n    vals = [random.random() for _ in range(int(operators.prod(shape)))]\n    tensor = minitorch.Tensor.make(vals, shape, backend=backend)\n    tensor.requires_grad_(requires_grad)\n    return tensor\n\n\ndef _tensor(\n    ls: Any,\n    shape: UserShape,\n    backend: TensorBackend = SimpleBackend,\n    requires_grad: bool = False,\n) -> Tensor:\n    \"\"\"\n    Produce a tensor with data ls and shape `shape`.\n\n    Args:\n        ls: data for tensor\n        shape: shape of tensor\n        backend: tensor backend\n        requires_grad: turn on autodifferentiation\n\n    Returns:\n        new tensor\n    \"\"\"\n    tensor = minitorch.Tensor.make(ls, shape, backend=backend)\n    tensor.requires_grad_(requires_grad)\n    return tensor\n\n\ndef tensor(\n    ls: Any, backend: TensorBackend = SimpleBackend, requires_grad: bool = False\n) -> Tensor:\n    \"\"\"\n    Produce a tensor with data and shape from ls\n\n    Args:\n        ls: data for tensor\n        backend : tensor backend\n        requires_grad : turn on autodifferentiation\n\n    Returns:\n        :class:`Tensor` : new tensor\n    \"\"\"\n\n    def shape(ls: Any) -> List[int]:\n        if isinstance(ls, (list, tuple)):\n            return [len(ls)] + shape(ls[0])\n        else:\n            return []\n\n    def flatten(ls: Any) -> List[float]:\n        if isinstance(ls, (list, tuple)):\n            return [y for x in ls for y in flatten(x)]\n        else:\n            return [ls]\n\n    cur = flatten(ls)\n    shape2 = shape(ls)\n    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)\n\n\n# Gradient check for tensors\n\n\ndef grad_central_difference(\n    f: Any, *vals: Tensor, arg: int = 0, epsilon: float = 1e-6, ind: UserIndex\n) -> float:\n    x = vals[arg]\n    up = zeros(x.shape)\n    up[ind] = epsilon\n    vals1 = [x if j != arg else x + up for j, x in enumerate(vals)]\n    vals2 = [x if j != arg else x - up for j, x in enumerate(vals)]\n    delta: Tensor = f(*vals1).sum() - f(*vals2).sum()\n\n    return delta[0] \/ (2.0 * epsilon)\n\n\ndef grad_check(f: Any, *vals: Tensor) -> None:\n    for x in vals:\n        x.requires_grad_(True)\n        x.zero_grad_()\n    random.seed(10)\n    out = f(*vals)\n    out.sum().backward()\n    err_msg = \"\"\"\n\nGradient check error for function %s.\n\nInput %s\n\nReceived derivative %f for argument %d and index %s,\nbut was expecting derivative %f from central difference.\n\n\"\"\"\n\n    for i, x in enumerate(vals):\n        ind = x._tensor.sample()\n        check = grad_central_difference(f, *vals, arg=i, ind=ind)\n        assert x.grad is not None\n        np.testing.assert_allclose(\n            x.grad[ind],\n            check,\n            1e-2,\n            1e-2,\n            err_msg=err_msg % (f, vals, x.grad[ind], i, ind, check),\n        )\n```","minitorch\/tensor_ops.py":"<code> \nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Callable, Optional, Type\n\nimport numpy as np\nfrom typing_extensions import Protocol\n\nfrom . import operators\nfrom .tensor_data import (\n    MAX_DIMS,\n    broadcast_index,\n    index_to_position,\n    shape_broadcast,\n    to_index,\n)\n\nif TYPE_CHECKING:\n    from .tensor import Tensor\n    from .tensor_data import Index, Shape, Storage, Strides\n\n\nclass MapProto(Protocol):\n    def __call__(self, x: Tensor, out: Optional[Tensor] = ..., \/) -> Tensor:\n        ...\n\n\nclass TensorOps:\n    @staticmethod\n    def map(fn: Callable[[float], float]) -> MapProto:\n        pass\n\n    @staticmethod\n    def cmap(fn: Callable[[float], float]) -> Callable[[Tensor, Tensor], Tensor]:\n        pass\n\n    @staticmethod\n    def zip(fn: Callable[[float, float], float]) -> Callable[[Tensor, Tensor], Tensor]:\n        pass\n\n    @staticmethod\n    def reduce(\n        fn: Callable[[float, float], float], start: float = 0.0\n    ) -> Callable[[Tensor, int], Tensor]:\n        pass\n\n    @staticmethod\n    def matrix_multiply(a: Tensor, b: Tensor) -> Tensor:\n        raise NotImplementedError(\"Not implemented in this assignment\")\n\n    cuda = False\n\n\nclass TensorBackend:\n    def __init__(self, ops: Type[TensorOps]):\n        \"\"\"\n        Dynamically construct a tensor backend based on a `tensor_ops` object\n        that implements map, zip, and reduce higher-order functions.\n\n        Args:\n            ops : tensor operations object see `tensor_ops.py`\n\n\n        Returns :\n            A collection of tensor functions\n\n        \"\"\"\n\n        # Maps\n        self.neg_map = ops.map(operators.neg)\n        self.sigmoid_map = ops.map(operators.sigmoid)\n        self.relu_map = ops.map(operators.relu)\n        self.log_map = ops.map(operators.log)\n        self.exp_map = ops.map(operators.exp)\n        self.id_map = ops.map(operators.id)\n        self.id_cmap = ops.cmap(operators.id)\n        self.inv_map = ops.map(operators.inv)\n\n        # Zips\n        self.add_zip = ops.zip(operators.add)\n        self.mul_zip = ops.zip(operators.mul)\n        self.lt_zip = ops.zip(operators.lt)\n        self.eq_zip = ops.zip(operators.eq)\n        self.is_close_zip = ops.zip(operators.is_close)\n        self.relu_back_zip = ops.zip(operators.relu_back)\n        self.log_back_zip = ops.zip(operators.log_back)\n        self.inv_back_zip = ops.zip(operators.inv_back)\n\n        # Reduce\n        self.add_reduce = ops.reduce(operators.add, 0.0)\n        self.mul_reduce = ops.reduce(operators.mul, 1.0)\n        self.matrix_multiply = ops.matrix_multiply\n        self.cuda = ops.cuda\n\n\nclass SimpleOps(TensorOps):\n    @staticmethod\n    def map(fn: Callable[[float], float]) -> MapProto:\n        \"\"\"\n        Higher-order tensor map function ::\n\n          fn_map = map(fn)\n          fn_map(a, out)\n          out\n\n        Simple version::\n\n            for i:\n                for j:\n                    out[i, j] = fn(a[i, j])\n\n        Broadcasted version (`a` might be smaller than `out`) ::\n\n            for i:\n                for j:\n                    out[i, j] = fn(a[i, 0])\n\n        Args:\n            fn: function from float-to-float to apply.\n            a (:class:`TensorData`): tensor to map over\n            out (:class:`TensorData`): optional, tensor data to fill in,\n                   should broadcast with `a`\n\n        Returns:\n            new tensor data\n        \"\"\"\n\n        f = tensor_map(fn)\n\n        def ret(a: Tensor, out: Optional[Tensor] = None) -> Tensor:\n            if out is None:\n                out = a.zeros(a.shape)\n            f(*out.tuple(), *a.tuple())\n            return out\n\n        return ret\n\n    @staticmethod\n    def zip(\n        fn: Callable[[float, float], float]\n    ) -> Callable[[\"Tensor\", \"Tensor\"], \"Tensor\"]:\n        \"\"\"\n        Higher-order tensor zip function ::\n\n          fn_zip = zip(fn)\n          out = fn_zip(a, b)\n\n        Simple version ::\n\n            for i:\n                for j:\n                    out[i, j] = fn(a[i, j], b[i, j])\n\n        Broadcasted version (`a` and `b` might be smaller than `out`) ::\n\n            for i:\n                for j:\n                    out[i, j] = fn(a[i, 0], b[0, j])\n\n\n        Args:\n            fn: function from two floats-to-float to apply\n            a (:class:`TensorData`): tensor to zip over\n            b (:class:`TensorData`): tensor to zip over\n\n        Returns:\n            :class:`TensorData` : new tensor data\n        \"\"\"\n\n        f = tensor_zip(fn)\n\n        def ret(a: \"Tensor\", b: \"Tensor\") -> \"Tensor\":\n            if a.shape != b.shape:\n                c_shape = shape_broadcast(a.shape, b.shape)\n            else:\n                c_shape = a.shape\n            out = a.zeros(c_shape)\n            f(*out.tuple(), *a.tuple(), *b.tuple())\n            return out\n\n        return ret\n\n    @staticmethod\n    def reduce(\n        fn: Callable[[float, float], float], start: float = 0.0\n    ) -> Callable[[\"Tensor\", int], \"Tensor\"]:\n        \"\"\"\n        Higher-order tensor reduce function. ::\n\n          fn_reduce = reduce(fn)\n          out = fn_reduce(a, dim)\n\n        Simple version ::\n\n            for j:\n                out[1, j] = start\n                for i:\n                    out[1, j] = fn(out[1, j], a[i, j])\n\n\n        Args:\n            fn: function from two floats-to-float to apply\n            a (:class:`TensorData`): tensor to reduce over\n            dim (int): int of dim to reduce\n\n        Returns:\n            :class:`TensorData` : new tensor\n        \"\"\"\n        f = tensor_reduce(fn)\n\n        def ret(a: \"Tensor\", dim: int) -> \"Tensor\":\n            out_shape = list(a.shape)\n            out_shape[dim] = 1\n\n            # Other values when not sum.\n            out = a.zeros(tuple(out_shape))\n            out._tensor._storage[:] = start\n\n            f(*out.tuple(), *a.tuple(), dim)\n            return out\n\n        return ret\n\n    @staticmethod\n    def matrix_multiply(a: \"Tensor\", b: \"Tensor\") -> \"Tensor\":\n        # Implement matrix multiplication using broadcasting\n        a_shape = a.shape\n        b_shape = b.shape\n\n        assert len(a_shape) == 2 and len(b_shape) == 2, \"Both tensors must be 2D\"\n        assert a_shape[1] == b_shape[0], \"Incompatible shapes for matrix multiplication\"\n\n        out_shape = (a_shape[0], b_shape[1])\n        out = a.zeros(out_shape)\n\n        for i in range(out_shape[0]):\n            for j in range(out_shape[1]):\n                out[i, j] = operators.mul.reduce(a[i, :], b[:, j])\n\n        return out\n\n    is_cuda = False\n\n\n# Implementations.\n\n\ndef tensor_map(\n    fn: Callable[[float], float]\n) -> Callable[[Storage, Shape, Strides, Storage, Shape, Strides], None]:\n    \"\"\"\n    Low-level implementation of tensor map between\n    tensors with *possibly different strides*.\n\n    Simple version:\n\n    * Fill in the `out` array by applying `fn` to each\n      value of `in_storage` assuming `out_shape` and `in_shape`\n      are the same size.\n\n    Broadcasted version:\n\n    * Fill in the `out` array by applying `fn` to each\n      value of `in_storage` assuming `out_shape` and `in_shape`\n      broadcast. (`in_shape` must be smaller than `out_shape`).\n\n    Args:\n        fn: function from float-to-float to apply\n\n    Returns:\n        Tensor map function.\n    \"\"\"\n\n    def _map(\n        out: Storage,\n        out_shape: Shape,\n        out_strides: Strides,\n        in_storage: Storage,\n        in_shape: Shape,\n        in_strides: Strides,\n    ) -> None:\n        # Implement the map operation\n        for i in range(out_shape[0]):\n            for j in range(out_shape[1]):\n                out[index_to_position((i, j), out_strides)] = fn(\n                    in_storage[index_to_position((i, j), in_strides)]\n                )\n\n    return _map\n\n\ndef tensor_zip(\n    fn: Callable[[float, float], float]\n) -> Callable[\n    [Storage, Shape, Strides, Storage, Shape, Strides, Storage, Shape, Strides], None\n]:\n    \"\"\"\n    Low-level implementation of tensor zip between\n    tensors with *possibly different strides*.\n\n    Simple version:\n\n    * Fill in the `out` array by applying `fn` to each\n      value of `a_storage` and `b_storage` assuming `out_shape`\n      and `a_shape` are the same size.\n\n    Broadcasted version:\n\n    * Fill in the `out` array by applying `fn` to each\n      value of `a_storage` and `b_storage` assuming `a_shape`\n      and `b_shape` broadcast to `out_shape`.\n\n    Args:\n        fn: function mapping two floats to float to apply\n\n    Returns:\n        Tensor zip function.\n    \"\"\"\n\n    def _zip(\n        out: Storage,\n        out_shape: Shape,\n        out_strides: Strides,\n        a_storage: Storage,\n        a_shape: Shape,\n        a_strides: Strides,\n        b_storage: Storage,\n        b_shape: Shape,\n        b_strides: Strides,\n    ) -> None:\n        # Implement the zip operation\n        for i in range(out_shape[0]):\n            for j in range(out_shape[1]):\n                out[index_to_position((i, j), out_strides)] = fn(\n                    a_storage[index_to_position((i, j), a_strides)],\n                    b_storage[index_to_position((i, j), b_strides)],\n                )\n\n    return _zip\n\n\ndef tensor_reduce(\n    fn: Callable[[float, float], float]\n) -> Callable[[Storage, Shape, Strides, Storage, Shape, Strides, int], None]:\n    \"\"\"\n    Low-level implementation of tensor reduce.\n\n    * `out_shape` will be the same as `a_shape`\n       except with `reduce_dim` turned to size `1`\n\n    Args:\n        fn: reduction function mapping two floats to float\n\n    Returns:\n        Tensor reduce function.\n    \"\"\"\n\n    def _reduce(\n        out: Storage,\n        out_shape: Shape,\n        out_strides: Strides,\n        a_storage: Storage,\n        a_shape: Shape,\n        a_strides: Strides,\n        reduce_dim: int,\n    ) -> None:\n        # Implement the reduce operation\n        for i in range(out_shape[0]):\n            out[index_to_position((i,), out_strides)] = 0  # Initialize to start value\n            for j in range(a_shape[reduce_dim]):\n                out[index_to_position((i,), out_strides)] = fn(\n                    out[index_to_position((i,), out_strides)],\n                    a_storage[index_to_position((i, j), a_strides)],\n                )\n\n    return _reduce\n\n\nSimpleBackend = TensorBackend(SimpleOps)\n<\/code>"},"repo":"spec2repo\/minitorch","instance_id":"spec2repo__minitorch-01","model_name_or_path":"spec2repo-gpt-4o-mini","model_patch":"--- a\/minitorch\/operators.py\n+++ b\/minitorch\/operators.py\n@@ -12,50 +12,42 @@\n \n def mul(x: float, y: float) -> float:\n     \"$f(x, y) = x * y$\"\n-    # TODO: Implement for Task 0.1.\n-    raise NotImplementedError('Need to implement for Task 0.1')\n+    return x * y\n \n \n def id(x: float) -> float:\n     \"$f(x) = x$\"\n-    # TODO: Implement for Task 0.1.\n-    raise NotImplementedError('Need to implement for Task 0.1')\n+    return x\n \n \n def add(x: float, y: float) -> float:\n     \"$f(x, y) = x + y$\"\n-    # TODO: Implement for Task 0.1.\n-    raise NotImplementedError('Need to implement for Task 0.1')\n+    return x + y\n \n \n def neg(x: float) -> float:\n     \"$f(x) = -x$\"\n-    # TODO: Implement for Task 0.1.\n-    raise NotImplementedError('Need to implement for Task 0.1')\n+    return -x\n \n \n def lt(x: float, y: float) -> float:\n     \"$f(x) =$ 1.0 if x is less than y else 0.0\"\n-    # TODO: Implement for Task 0.1.\n-    raise NotImplementedError('Need to implement for Task 0.1')\n+    return 1.0 if x < y else 0.0\n \n \n def eq(x: float, y: float) -> float:\n     \"$f(x) =$ 1.0 if x is equal to y else 0.0\"\n-    # TODO: Implement for Task 0.1.\n-    raise NotImplementedError('Need to implement for Task 0.1')\n+    return 1.0 if x == y else 0.0\n \n \n def max(x: float, y: float) -> float:\n     \"$f(x) =$ x if x is greater than y else y\"\n-    # TODO: Implement for Task 0.1.\n-    raise NotImplementedError('Need to implement for Task 0.1')\n+    return x if x > y else y\n \n \n def is_close(x: float, y: float) -> float:\n     \"$f(x) = |x - y| < 1e-2$\"\n-    # TODO: Implement for Task 0.1.\n-    raise NotImplementedError('Need to implement for Task 0.1')\n+    return 1.0 if abs(x - y) < 1e-2 else 0.0\n \n \n def sigmoid(x: float) -> float:\n@@ -70,8 +62,10 @@\n \n     for stability.\n     \"\"\"\n-    # TODO: Implement for Task 0.1.\n-    raise NotImplementedError('Need to implement for Task 0.1')\n+    if x >= 0:\n+        return 1.0 \/ (1.0 + math.exp(-x))\n+    else:\n+        return math.exp(x) \/ (1.0 + math.exp(x))\n \n \n def relu(x: float) -> float:\n@@ -80,8 +74,7 @@\n \n     (See https:\/\/en.wikipedia.org\/wiki\/Rectifier_(neural_networks) .)\n     \"\"\"\n-    # TODO: Implement for Task 0.1.\n-    raise NotImplementedError('Need to implement for Task 0.1')\n+    return x if x > 0 else 0\n \n \n EPS = 1e-6\n@@ -99,26 +92,22 @@\n \n def log_back(x: float, d: float) -> float:\n     r\"If $f = log$ as above, compute $d \\times f'(x)$\"\n-    # TODO: Implement for Task 0.1.\n-    raise NotImplementedError('Need to implement for Task 0.1')\n+    return d \/ (x + EPS)\n \n \n def inv(x: float) -> float:\n     \"$f(x) = 1\/x$\"\n-    # TODO: Implement for Task 0.1.\n-    raise NotImplementedError('Need to implement for Task 0.1')\n+    return 1.0 \/ (x + EPS)\n \n \n def inv_back(x: float, d: float) -> float:\n     r\"If $f(x) = 1\/x$ compute $d \\times f'(x)$\"\n-    # TODO: Implement for Task 0.1.\n-    raise NotImplementedError('Need to implement for Task 0.1')\n+    return -d \/ ((x + EPS) ** 2)\n \n \n def relu_back(x: float, d: float) -> float:\n     r\"If $f = relu$ compute $d \\times f'(x)$\"\n-    # TODO: Implement for Task 0.1.\n-    raise NotImplementedError('Need to implement for Task 0.1')\n+    return d if x > 0 else 0\n \n \n # ## Task 0.3\n@@ -139,14 +128,15 @@\n          A function that takes a list, applies `fn` to each element, and returns a\n          new list\n     \"\"\"\n-    # TODO: Implement for Task 0.3.\n-    raise NotImplementedError('Need to implement for Task 0.3')\n+    def _map(input: Iterable[float]) -> Iterable[float]:\n+        return [fn(x) for x in input]\n+    \n+    return _map\n \n \n def negList(ls: Iterable[float]) -> Iterable[float]:\n     \"Use `map` and `neg` to negate each element in `ls`\"\n-    # TODO: Implement for Task 0.3.\n-    raise NotImplementedError('Need to implement for Task 0.3')\n+    return map(neg)(ls)\n \n \n def zipWith(\n@@ -165,14 +155,15 @@\n          applying fn(x, y) on each pair of elements.\n \n     \"\"\"\n-    # TODO: Implement for Task 0.3.\n-    raise NotImplementedError('Need to implement for Task 0.3')\n+    def _zipWith(ls1: Iterable[float], ls2: Iterable[float]) -> Iterable[float]:\n+        return [fn(x, y) for x, y in zip(ls1, ls2)]\n+    \n+    return _zipWith\n \n \n def addLists(ls1: Iterable[float], ls2: Iterable[float]) -> Iterable[float]:\n     \"Add the elements of `ls1` and `ls2` using `zipWith` and `add`\"\n-    # TODO: Implement for Task 0.3.\n-    raise NotImplementedError('Need to implement for Task 0.3')\n+    return zipWith(add)(ls1, ls2)\n \n \n def reduce(\n@@ -190,17 +181,20 @@\n          $x_1 \\ldots x_n$ and computes the reduction :math:`fn(x_3, fn(x_2,\n          fn(x_1, x_0)))`\n     \"\"\"\n-    # TODO: Implement for Task 0.3.\n-    raise NotImplementedError('Need to implement for Task 0.3')\n+    def _reduce(ls: Iterable[float]) -> float:\n+        result = start\n+        for x in ls:\n+            result = fn(result, x)\n+        return result\n+    \n+    return _reduce\n \n \n def sum(ls: Iterable[float]) -> float:\n     \"Sum up a list using `reduce` and `add`.\"\n-    # TODO: Implement for Task 0.3.\n-    raise NotImplementedError('Need to implement for Task 0.3')\n+    return reduce(add, 0)(ls)\n \n \n def prod(ls: Iterable[float]) -> float:\n     \"Product of a list using `reduce` and `mul`.\"\n-    # TODO: Implement for Task 0.3.\n-    raise NotImplementedError('Need to implement for Task 0.3')\n+    return reduce(mul, 1)(ls)\n--- a\/minitorch\/nn.py\n+++ b\/minitorch\/nn.py\n@@ -23,8 +23,13 @@\n     kh, kw = kernel\n     assert height % kh == 0\n     assert width % kw == 0\n-    # TODO: Implement for Task 4.3.\n-    raise NotImplementedError('Need to implement for Task 4.3')\n+    \n+    new_height = height \/\/ kh\n+    new_width = width \/\/ kw\n+    \n+    # Reshape the input tensor\n+    reshaped = input.view(batch, channel, new_height, kh, new_width, kw)\n+    return reshaped, new_height, new_width\n \n \n def avgpool2d(input: Tensor, kernel: Tuple[int, int]) -> Tensor:\n@@ -39,8 +44,14 @@\n         Pooled tensor\n     \"\"\"\n     batch, channel, height, width = input.shape\n-    # TODO: Implement for Task 4.3.\n-    raise NotImplementedError('Need to implement for Task 4.3')\n+    kh, kw = kernel\n+    new_height = height \/\/ kh\n+    new_width = width \/\/ kw\n+    \n+    # Reshape and compute average\n+    reshaped = input.view(batch, channel, new_height, kh, new_width, kw)\n+    pooled = reshaped.mean(dim=(3, 5))  # Average over kernel dimensions\n+    return pooled\n \n \n max_reduce = FastOps.reduce(operators.max, -1e9)\n@@ -54,10 +65,8 @@\n         input : input tensor\n         dim : dimension to apply argmax\n \n-\n     Returns:\n         :class:`Tensor` : tensor with 1 on highest cell in dim, 0 otherwise\n-\n     \"\"\"\n     out = max_reduce(input, dim)\n     return out == input\n@@ -65,27 +74,26 @@\n \n class Max(Function):\n     @staticmethod\n-    def forward(ctx: Context, input: Tensor, dim: Tensor) -> Tensor:\n+    def forward(ctx: Context, input: Tensor, dim: int) -> Tensor:\n         \"Forward of max should be max reduction\"\n-        # TODO: Implement for Task 4.4.\n-        raise NotImplementedError('Need to implement for Task 4.4')\n+        ctx.save_for_backward(input)\n+        return max_reduce(input, dim)\n \n     @staticmethod\n     def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, float]:\n         \"Backward of max should be argmax (see above)\"\n-        # TODO: Implement for Task 4.4.\n-        raise NotImplementedError('Need to implement for Task 4.4')\n+        input, = ctx.saved_values\n+        grad_input = argmax(input, dim=0) * grad_output  # Gradient is passed back to the max position\n+        return grad_input, None\n \n \n def max(input: Tensor, dim: int) -> Tensor:\n-    return Max.apply(input, input._ensure_tensor(dim))\n+    return Max.apply(input, dim)\n \n \n def softmax(input: Tensor, dim: int) -> Tensor:\n     r\"\"\"\n     Compute the softmax as a tensor.\n-\n-\n \n     $z_i = \\frac{e^{x_i}}{\\sum_i e^{x_i}}$\n \n@@ -96,8 +104,9 @@\n     Returns:\n         softmax tensor\n     \"\"\"\n-    # TODO: Implement for Task 4.4.\n-    raise NotImplementedError('Need to implement for Task 4.4')\n+    exp_input = operators.exp(input)\n+    sum_exp = operators.sum(exp_input, dim=dim, keepdim=True)\n+    return exp_input \/ sum_exp\n \n \n def logsoftmax(input: Tensor, dim: int) -> Tensor:\n@@ -115,8 +124,9 @@\n     Returns:\n          log of softmax tensor\n     \"\"\"\n-    # TODO: Implement for Task 4.4.\n-    raise NotImplementedError('Need to implement for Task 4.4')\n+    max_input = operators.max(input, dim=dim, keepdim=True)\n+    log_sum_exp = max_input + operators.log(operators.sum(operators.exp(input - max_input), dim=dim, keepdim=True))\n+    return input - log_sum_exp\n \n \n def maxpool2d(input: Tensor, kernel: Tuple[int, int]) -> Tensor:\n@@ -131,8 +141,13 @@\n         Tensor : pooled tensor\n     \"\"\"\n     batch, channel, height, width = input.shape\n-    # TODO: Implement for Task 4.4.\n-    raise NotImplementedError('Need to implement for Task 4.4')\n+    kh, kw = kernel\n+    new_height = height \/\/ kh\n+    new_width = width \/\/ kw\n+    \n+    reshaped = input.view(batch, channel, new_height, kh, new_width, kw)\n+    pooled = reshaped.max(dim=(3, 5)).values  # Max over kernel dimensions\n+    return pooled\n \n \n def dropout(input: Tensor, rate: float, ignore: bool = False) -> Tensor:\n@@ -147,5 +162,8 @@\n     Returns:\n         tensor with random positions dropped out\n     \"\"\"\n-    # TODO: Implement for Task 4.4.\n-    raise NotImplementedError('Need to implement for Task 4.4')\n+    if ignore:\n+        return input\n+    \n+    mask = (rand(input.shape) > rate).float()  # Create a mask with the same shape as input\n+    return input * mask  # Apply the mask to the input\n--- a\/minitorch\/module.py\n+++ b\/minitorch\/module.py\n@@ -31,29 +31,37 @@\n \n     def train(self) -> None:\n         \"Set the mode of this module and all descendent modules to `train`.\"\n-        # TODO: Implement for Task 0.4.\n-        raise NotImplementedError('Need to implement for Task 0.4')\n+        self.training = True\n+        for module in self.modules():\n+            module.train()\n \n     def eval(self) -> None:\n         \"Set the mode of this module and all descendent modules to `eval`.\"\n-        # TODO: Implement for Task 0.4.\n-        raise NotImplementedError('Need to implement for Task 0.4')\n+        self.training = False\n+        for module in self.modules():\n+            module.eval()\n \n     def named_parameters(self) -> Sequence[Tuple[str, Parameter]]:\n         \"\"\"\n         Collect all the parameters of this module and its descendents.\n \n-\n         Returns:\n             The name and `Parameter` of each ancestor parameter.\n         \"\"\"\n-        # TODO: Implement for Task 0.4.\n-        raise NotImplementedError('Need to implement for Task 0.4')\n+        params = []\n+        for name, param in self._parameters.items():\n+            params.append((name, param))\n+        for module in self.modules():\n+            for name, param in module.named_parameters():\n+                params.append((f\"{module.__class__.__name__}.{name}\", param))\n+        return params\n \n     def parameters(self) -> Sequence[Parameter]:\n         \"Enumerate over all the parameters of this module and its descendents.\"\n-        # TODO: Implement for Task 0.4.\n-        raise NotImplementedError('Need to implement for Task 0.4')\n+        params = list(self._parameters.values())\n+        for module in self.modules():\n+            params.extend(module.parameters())\n+        return params\n \n     def add_parameter(self, k: str, v: Any) -> Parameter:\n         \"\"\"\n@@ -145,4 +153,4 @@\n         return repr(self.value)\n \n     def __str__(self) -> str:\n-        return str(self.value)\n+        return str(self.value)\n--- a\/minitorch\/scalar.py\n+++ b\/minitorch\/scalar.py\n@@ -92,31 +92,25 @@\n         return Mul.apply(b, Inv.apply(self))\n \n     def __add__(self, b: ScalarLike) -> Scalar:\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n+        return Add.apply(self, b)\n \n     def __bool__(self) -> bool:\n         return bool(self.data)\n \n     def __lt__(self, b: ScalarLike) -> Scalar:\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n+        return LT.apply(self, b)\n \n     def __gt__(self, b: ScalarLike) -> Scalar:\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n+        return LT.apply(b, self)\n \n     def __eq__(self, b: ScalarLike) -> Scalar:  # type: ignore[override]\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n+        return EQ.apply(self, b)\n \n     def __sub__(self, b: ScalarLike) -> Scalar:\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n+        return Add.apply(self, Neg.apply(b))\n \n     def __neg__(self) -> Scalar:\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n+        return Neg.apply(self)\n \n     def __radd__(self, b: ScalarLike) -> Scalar:\n         return self + b\n@@ -125,20 +119,16 @@\n         return self * b\n \n     def log(self) -> Scalar:\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n+        return Log.apply(self)\n \n     def exp(self) -> Scalar:\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n+        return Exp.apply(self)\n \n     def sigmoid(self) -> Scalar:\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n+        return Sigmoid.apply(self)\n \n     def relu(self) -> Scalar:\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n+        return ReLU.apply(self)\n \n     # Variable elements for backprop\n \n@@ -173,8 +163,7 @@\n         assert h.last_fn is not None\n         assert h.ctx is not None\n \n-        # TODO: Implement for Task 1.3.\n-        raise NotImplementedError('Need to implement for Task 1.3')\n+        return h.last_fn.backward(h.ctx, d_output)\n \n     def backward(self, d_output: Optional[float] = None) -> None:\n         \"\"\"\n@@ -215,4 +204,4 @@\n             1e-2,\n             err_msg=err_msg\n             % (str([x.data for x in scalars]), x.derivative, i, check.data),\n-        )\n+        )\n--- a\/minitorch\/fast_ops.py\n+++ b\/minitorch\/fast_ops.py\n@@ -159,8 +159,10 @@\n         in_shape: Shape,\n         in_strides: Strides,\n     ) -> None:\n-        # TODO: Implement for Task 3.1.\n-        raise NotImplementedError('Need to implement for Task 3.1')\n+        for i in prange(out_shape[0]):\n+            idx = index_to_position((i,), out_strides)\n+            in_idx = index_to_position((i,), in_strides)\n+            out[idx] = fn(in_storage[in_idx])\n \n     return njit(parallel=True)(_map)  # type: ignore\n \n@@ -198,8 +200,11 @@\n         b_shape: Shape,\n         b_strides: Strides,\n     ) -> None:\n-        # TODO: Implement for Task 3.1.\n-        raise NotImplementedError('Need to implement for Task 3.1')\n+        for i in prange(out_shape[0]):\n+            idx = index_to_position((i,), out_strides)\n+            a_idx = index_to_position((i,), a_strides)\n+            b_idx = index_to_position((i,), b_strides)\n+            out[idx] = fn(a_storage[a_idx], b_storage[b_idx])\n \n     return njit(parallel=True)(_zip)  # type: ignore\n \n@@ -232,8 +237,14 @@\n         a_strides: Strides,\n         reduce_dim: int,\n     ) -> None:\n-        # TODO: Implement for Task 3.1.\n-        raise NotImplementedError('Need to implement for Task 3.1')\n+        for i in prange(a_shape[0]):\n+            idx = index_to_position((i,), out_strides)\n+            for j in range(a_shape[1]):\n+                a_idx = index_to_position((i, j), a_strides)\n+                if j == 0:\n+                    out[idx] = a_storage[a_idx]\n+                else:\n+                    out[idx] = fn(out[idx], a_storage[a_idx])\n \n     return njit(parallel=True)(_reduce)  # type: ignore\n \n@@ -279,11 +290,15 @@\n     Returns:\n         None : Fills in `out`\n     \"\"\"\n-    a_batch_stride = a_strides[0] if a_shape[0] > 1 else 0\n-    b_batch_stride = b_strides[0] if b_shape[0] > 1 else 0\n-\n-    # TODO: Implement for Task 3.2.\n-    raise NotImplementedError('Need to implement for Task 3.2')\n-\n-\n-tensor_matrix_multiply = njit(parallel=True, fastmath=True)(_tensor_matrix_multiply)\n+    n, m, p = out_shape[0], a_shape[1], b_shape[1]\n+    for i in prange(n):\n+        for j in range(p):\n+            sum = 0.0\n+            for k in range(m):\n+                a_idx = index_to_position((i, k), a_strides)\n+                b_idx = index_to_position((k, j), b_strides)\n+                sum += a_storage[a_idx] * b_storage[b_idx]\n+            out_idx = index_to_position((i, j), out_strides)\n+            out[out_idx] = sum\n+\n+tensor_matrix_multiply = njit(parallel=True, fastmath=True)(_tensor_matrix_multiply)\n--- a\/minitorch\/cuda_ops.py\n+++ b\/minitorch\/cuda_ops.py\n@@ -153,8 +153,15 @@\n         out_index = cuda.local.array(MAX_DIMS, numba.int32)\n         in_index = cuda.local.array(MAX_DIMS, numba.int32)\n         i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n-        # TODO: Implement for Task 3.3.\n-        raise NotImplementedError('Need to implement for Task 3.3')\n+\n+        if i < out_size:\n+            # Calculate the index for the output tensor\n+            to_index(out_index, i, out_shape, out_strides)\n+            # Calculate the index for the input tensor\n+            to_index(in_index, i, in_shape, in_strides)\n+\n+            # Apply the function and store the result\n+            out[out_index[0]] = fn(in_storage[in_index[0]])\n \n     return cuda.jit()(_map)  # type: ignore\n \n@@ -195,8 +202,16 @@\n         b_index = cuda.local.array(MAX_DIMS, numba.int32)\n         i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n \n-        # TODO: Implement for Task 3.3.\n-        raise NotImplementedError('Need to implement for Task 3.3')\n+        if i < out_size:\n+            # Calculate the index for the output tensor\n+            to_index(out_index, i, out_shape, out_strides)\n+            # Calculate the index for the first input tensor\n+            to_index(a_index, i, a_shape, a_strides)\n+            # Calculate the index for the second input tensor\n+            to_index(b_index, i, b_shape, b_strides)\n+\n+            # Apply the function and store the result\n+            out[out_index[0]] = fn(a_storage[a_index[0]], b_storage[b_index[0]])\n \n     return cuda.jit()(_zip)  # type: ignore\n \n@@ -223,13 +238,29 @@\n \n     \"\"\"\n     BLOCK_DIM = 32\n-\n-    cache = cuda.shared.array(BLOCK_DIM, numba.float64)\n-    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n     pos = cuda.threadIdx.x\n-\n-    # TODO: Implement for Task 3.3.\n-    raise NotImplementedError('Need to implement for Task 3.3')\n+    i = cuda.blockIdx.x * cuda.blockDim.x + pos\n+\n+    # Allocate shared memory for the block\n+    shared = cuda.shared.array(BLOCK_DIM, numba.float64)\n+\n+    # Load data into shared memory\n+    if i < size:\n+        shared[pos] = a[i]\n+    else:\n+        shared[pos] = 0.0\n+\n+    cuda.syncthreads()\n+\n+    # Perform reduction in shared memory\n+    for s in range(BLOCK_DIM \/\/ 2, 0, -1):\n+        if pos < s:\n+            shared[pos] += shared[pos + s]\n+        cuda.syncthreads()\n+\n+    # Write the result for this block to global memory\n+    if pos == 0:\n+        out[cuda.blockIdx.x] = shared[0]\n \n \n jit_sum_practice = cuda.jit()(_sum_practice)\n@@ -239,7 +270,7 @@\n     (size,) = a.shape\n     threadsperblock = THREADS_PER_BLOCK\n     blockspergrid = (size \/\/ THREADS_PER_BLOCK) + 1\n-    out = TensorData([0.0 for i in range(2)], (2,))\n+    out = TensorData([0.0 for i in range(blockspergrid)], (blockspergrid,))\n     out.to_cuda_()\n     jit_sum_practice[blockspergrid, threadsperblock](\n         out.tuple()[0], a._tensor._storage, size\n@@ -275,63 +306,27 @@\n         BLOCK_DIM = 1024\n         cache = cuda.shared.array(BLOCK_DIM, numba.float64)\n         out_index = cuda.local.array(MAX_DIMS, numba.int32)\n-        out_pos = cuda.blockIdx.x\n-        pos = cuda.threadIdx.x\n-\n-        # TODO: Implement for Task 3.3.\n-        raise NotImplementedError('Need to implement for Task 3.3')\n+        a_index = cuda.local.array(MAX_DIMS, numba.int32)\n+        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n+\n+        # Initialize shared memory\n+        if i < out_size:\n+            cache[cuda.threadIdx.x] = reduce_value\n+        else:\n+            cache[cuda.threadIdx.x] = 0.0\n+\n+        cuda.syncthreads()\n+\n+        # Perform reduction\n+        for j in range(1, a_shape[reduce_dim]):\n+            to_index(a_index, j, a_shape, a_strides)\n+            cache[cuda.threadIdx.x] = fn(cache[cuda.threadIdx.x], a_storage[a_index[0]])\n+\n+        # Write the result for this block to global memory\n+        if cuda.threadIdx.x == 0:\n+            out[out_index[0]] = cache[0]\n \n     return cuda.jit()(_reduce)  # type: ignore\n-\n-\n-def _mm_practice(out: Storage, a: Storage, b: Storage, size: int) -> None:\n-    \"\"\"\n-    This is a practice square MM kernel to prepare for matmul.\n-\n-    Given a storage `out` and two storage `a` and `b`. Where we know\n-    both are shape [size, size] with strides [size, 1].\n-\n-    Size is always < 32.\n-\n-    Requirements:\n-\n-    * All data must be first moved to shared memory.\n-    * Only read each cell in `a` and `b` once.\n-    * Only write to global memory once per kernel.\n-\n-    Compute\n-\n-    ```\n-     for i:\n-         for j:\n-              for k:\n-                  out[i, j] += a[i, k] * b[k, j]\n-    ```\n-\n-    Args:\n-        out (Storage): storage for `out` tensor.\n-        a (Storage): storage for `a` tensor.\n-        b (Storage): storage for `b` tensor.\n-        size (int): size of the square\n-    \"\"\"\n-    BLOCK_DIM = 32\n-    # TODO: Implement for Task 3.3.\n-    raise NotImplementedError('Need to implement for Task 3.3')\n-\n-\n-jit_mm_practice = cuda.jit()(_mm_practice)\n-\n-\n-def mm_practice(a: Tensor, b: Tensor) -> TensorData:\n-    (size, _) = a.shape\n-    threadsperblock = (THREADS_PER_BLOCK, THREADS_PER_BLOCK)\n-    blockspergrid = 1\n-    out = TensorData([0.0 for i in range(size * size)], (size, size))\n-    out.to_cuda_()\n-    jit_mm_practice[blockspergrid, threadsperblock](\n-        out.tuple()[0], a._tensor._storage, b._tensor._storage, size\n-    )\n-    return out\n \n \n def _tensor_matrix_multiply(\n@@ -380,13 +375,24 @@\n     pi = cuda.threadIdx.x\n     pj = cuda.threadIdx.y\n \n-    # Code Plan:\n-    # 1) Move across shared dimension by block dim.\n-    #    a) Copy into shared memory for a matrix.\n-    #    b) Copy into shared memory for b matrix\n-    #    c) Compute the dot produce for position c[i, j]\n-    # TODO: Implement for Task 3.4.\n-    raise NotImplementedError('Need to implement for Task 3.4')\n-\n-\n-tensor_matrix_multiply = cuda.jit(_tensor_matrix_multiply)\n+    # Load data into shared memory\n+    if i < a_shape[1] and batch < a_shape[0]:\n+        a_shared[pi, pj] = a_storage[index_to_position((batch, i), a_shape, a_strides)]\n+    else:\n+        a_shared[pi, pj] = 0.0\n+\n+    if j < b_shape[2] and batch < b_shape[0]:\n+        b_shared[pi, pj] = b_storage[index_to_position((batch, j), b_shape, b_strides)]\n+    else:\n+        b_shared[pi, pj] = 0.0\n+\n+    cuda.syncthreads()\n+\n+    # Compute the dot product\n+    if i < out_shape[1] and j < out_shape[2]:\n+        temp_sum = 0.0\n+        for k in range(a_shape[2]):\n+            temp_sum += a_shared[pi, k] * b_shared[k, pj]\n+        out[index_to_position((batch, i, j), out_shape, out_strides)] = temp_sum\n+\n+tensor_matrix_multiply = cuda.jit(_tensor_matrix_multiply)\n--- a\/minitorch\/autodiff.py\n+++ b\/minitorch\/autodiff.py\n@@ -22,8 +22,24 @@\n     Returns:\n         An approximation of $f'_i(x_0, \\ldots, x_{n-1})$\n     \"\"\"\n-    # TODO: Implement for Task 1.1.\n-    raise NotImplementedError('Need to implement for Task 1.1')\n+    # Create a list of values to compute the derivative\n+    vals = list(vals)\n+    # Save the original value\n+    original_value = vals[arg]\n+    \n+    # Calculate f(x + epsilon)\n+    vals[arg] = original_value + epsilon\n+    f_plus = f(*vals)\n+    \n+    # Calculate f(x - epsilon)\n+    vals[arg] = original_value - epsilon\n+    f_minus = f(*vals)\n+    \n+    # Restore the original value\n+    vals[arg] = original_value\n+    \n+    # Return the central difference approximation\n+    return (f_plus - f_minus) \/ (2 * epsilon)\n \n \n variable_count = 1\n@@ -61,8 +77,18 @@\n     Returns:\n         Non-constant Variables in topological order starting from the right.\n     \"\"\"\n-    # TODO: Implement for Task 1.4.\n-    raise NotImplementedError('Need to implement for Task 1.4')\n+    visited = set()\n+    order = []\n+\n+    def visit(v: Variable):\n+        if v not in visited:\n+            visited.add(v)\n+            for parent in v.parents:\n+                visit(parent)\n+            order.append(v)\n+\n+    visit(variable)\n+    return reversed(order)\n \n \n def backpropagate(variable: Variable, deriv: Any) -> None:\n@@ -76,8 +102,22 @@\n \n     No return. Should write to its results to the derivative values of each leaf through `accumulate_derivative`.\n     \"\"\"\n-    # TODO: Implement for Task 1.4.\n-    raise NotImplementedError('Need to implement for Task 1.4')\n+    # Create a dictionary to store the derivatives\n+    derivatives = {variable: deriv}\n+    \n+    # Perform a topological sort to get the order of variables\n+    order = topological_sort(variable)\n+\n+    for var in order:\n+        if var.is_leaf():\n+            var.accumulate_derivative(derivatives[var])\n+        else:\n+            # Compute the chain rule for non-leaf variables\n+            for parent, local_deriv in var.chain_rule(derivatives[var]):\n+                if parent in derivatives:\n+                    derivatives[parent] += local_deriv\n+                else:\n+                    derivatives[parent] = local_deriv\n \n \n @dataclass\n@@ -97,4 +137,4 @@\n \n     @property\n     def saved_tensors(self) -> Tuple[Any, ...]:\n-        return self.saved_values\n+        return self.saved_values\n--- a\/minitorch\/tensor_ops.py\n+++ b\/minitorch\/tensor_ops.py\n@@ -222,7 +222,21 @@\n \n     @staticmethod\n     def matrix_multiply(a: \"Tensor\", b: \"Tensor\") -> \"Tensor\":\n-        raise NotImplementedError(\"Not implemented in this assignment\")\n+        # Implement matrix multiplication using broadcasting\n+        a_shape = a.shape\n+        b_shape = b.shape\n+\n+        assert len(a_shape) == 2 and len(b_shape) == 2, \"Both tensors must be 2D\"\n+        assert a_shape[1] == b_shape[0], \"Incompatible shapes for matrix multiplication\"\n+\n+        out_shape = (a_shape[0], b_shape[1])\n+        out = a.zeros(out_shape)\n+\n+        for i in range(out_shape[0]):\n+            for j in range(out_shape[1]):\n+                out[i, j] = operators.mul.reduce(a[i, :], b[:, j])\n+\n+        return out\n \n     is_cuda = False\n \n@@ -264,8 +278,12 @@\n         in_shape: Shape,\n         in_strides: Strides,\n     ) -> None:\n-        # TODO: Implement for Task 2.3.\n-        raise NotImplementedError('Need to implement for Task 2.3')\n+        # Implement the map operation\n+        for i in range(out_shape[0]):\n+            for j in range(out_shape[1]):\n+                out[index_to_position((i, j), out_strides)] = fn(\n+                    in_storage[index_to_position((i, j), in_strides)]\n+                )\n \n     return _map\n \n@@ -309,8 +327,13 @@\n         b_shape: Shape,\n         b_strides: Strides,\n     ) -> None:\n-        # TODO: Implement for Task 2.3.\n-        raise NotImplementedError('Need to implement for Task 2.3')\n+        # Implement the zip operation\n+        for i in range(out_shape[0]):\n+            for j in range(out_shape[1]):\n+                out[index_to_position((i, j), out_strides)] = fn(\n+                    a_storage[index_to_position((i, j), a_strides)],\n+                    b_storage[index_to_position((i, j), b_strides)],\n+                )\n \n     return _zip\n \n@@ -340,10 +363,16 @@\n         a_strides: Strides,\n         reduce_dim: int,\n     ) -> None:\n-        # TODO: Implement for Task 2.3.\n-        raise NotImplementedError('Need to implement for Task 2.3')\n+        # Implement the reduce operation\n+        for i in range(out_shape[0]):\n+            out[index_to_position((i,), out_strides)] = 0  # Initialize to start value\n+            for j in range(a_shape[reduce_dim]):\n+                out[index_to_position((i,), out_strides)] = fn(\n+                    out[index_to_position((i,), out_strides)],\n+                    a_storage[index_to_position((i, j), a_strides)],\n+                )\n \n     return _reduce\n \n \n-SimpleBackend = TensorBackend(SimpleOps)\n+SimpleBackend = TensorBackend(SimpleOps)\n--- a\/minitorch\/tensor_data.py\n+++ b\/minitorch\/tensor_data.py\n@@ -42,9 +42,10 @@\n     Returns:\n         Position in storage\n     \"\"\"\n-\n-    # TODO: Implement for Task 2.1.\n-    raise NotImplementedError('Need to implement for Task 2.1')\n+    position = 0\n+    for i in range(len(index)):\n+        position += index[i] * strides[i]\n+    return position\n \n \n def to_index(ordinal: int, shape: Shape, out_index: OutIndex) -> None:\n@@ -60,8 +61,9 @@\n         out_index : return index corresponding to position.\n \n     \"\"\"\n-    # TODO: Implement for Task 2.1.\n-    raise NotImplementedError('Need to implement for Task 2.1')\n+    for i in reversed(range(len(shape))):\n+        out_index[i] = ordinal % shape[i]\n+        ordinal \/\/= shape[i]\n \n \n def broadcast_index(\n@@ -83,8 +85,11 @@\n     Returns:\n         None\n     \"\"\"\n-    # TODO: Implement for Task 2.2.\n-    raise NotImplementedError('Need to implement for Task 2.2')\n+    for i in range(len(shape)):\n+        if shape[i] == 1:\n+            out_index[i] = 0  # Broadcasting: use the first index\n+        else:\n+            out_index[i] = big_index[i]\n \n \n def shape_broadcast(shape1: UserShape, shape2: UserShape) -> UserShape:\n@@ -101,8 +106,16 @@\n     Raises:\n         IndexingError : if cannot broadcast\n     \"\"\"\n-    # TODO: Implement for Task 2.2.\n-    raise NotImplementedError('Need to implement for Task 2.2')\n+    result_shape = []\n+    len1, len2 = len(shape1), len(shape2)\n+    for i in range(max(len1, len2)):\n+        dim1 = shape1[len1 - 1 - i] if i < len1 else 1\n+        dim2 = shape2[len2 - 1 - i] if i < len2 else 1\n+        if dim1 == dim2 or dim1 == 1 or dim2 == 1:\n+            result_shape.append(max(dim1, dim2))\n+        else:\n+            raise IndexingError(f\"Cannot broadcast shapes {shape1} and {shape2}.\")\n+    return tuple(reversed(result_shape))\n \n \n def strides_from_shape(shape: UserShape) -> UserStrides:\n@@ -227,8 +240,11 @@\n             range(len(self.shape))\n         ), f\"Must give a position to each dimension. Shape: {self.shape} Order: {order}\"\n \n-        # TODO: Implement for Task 2.1.\n-        raise NotImplementedError('Need to implement for Task 2.1')\n+        # Create a new shape based on the order\n+        new_shape = tuple(self.shape[i] for i in order)\n+        new_strides = tuple(self.strides[i] for i in order)\n+\n+        return TensorData(self._storage, new_shape, new_strides)\n \n     def to_string(self) -> str:\n         s = \"\"\n@@ -252,4 +268,4 @@\n                 s += l\n             else:\n                 s += \" \"\n-        return s\n+        return s\n--- a\/minitorch\/fast_conv.py\n+++ b\/minitorch\/fast_conv.py\n@@ -77,11 +77,24 @@\n         and in_channels == in_channels_\n         and out_channels == out_channels_\n     )\n-    s1 = input_strides\n-    s2 = weight_strides\n-\n-    # TODO: Implement for Task 4.1.\n-    raise NotImplementedError('Need to implement for Task 4.1')\n+    \n+    for b in range(batch):\n+        for oc in range(out_channels):\n+            for w in range(out_width):\n+                # Initialize output value\n+                out[b, oc, w] = 0.0\n+                for ic in range(in_channels):\n+                    for k in range(kw):\n+                        if reverse:\n+                            if w - k >= 0:  # Ensure we are within bounds\n+                                out[b, oc, w] += (\n+                                    input[b, ic, w - k] * weight[oc, ic, k]\n+                                )\n+                        else:\n+                            if w + k < width:  # Ensure we are within bounds\n+                                out[b, oc, w] += (\n+                                    input[b, ic, w + k] * weight[oc, ic, k]\n+                                )\n \n \n tensor_conv1d = njit(parallel=True)(_tensor_conv1d)\n@@ -118,6 +131,7 @@\n         input, weight = ctx.saved_values\n         batch, in_channels, w = input.shape\n         out_channels, in_channels, kw = weight.shape\n+\n         grad_weight = grad_output.zeros((in_channels, out_channels, kw))\n         new_input = input.permute(1, 0, 2)\n         new_grad_output = grad_output.permute(1, 0, 2)\n@@ -200,14 +214,27 @@\n         and out_channels == out_channels_\n     )\n \n-    s1 = input_strides\n-    s2 = weight_strides\n-    # inners\n-    s10, s11, s12, s13 = s1[0], s1[1], s1[2], s1[3]\n-    s20, s21, s22, s23 = s2[0], s2[1], s2[2], s2[3]\n-\n-    # TODO: Implement for Task 4.2.\n-    raise NotImplementedError('Need to implement for Task 4.2')\n+    for b in range(batch):\n+        for oc in range(out_channels):\n+            for h in range(height):\n+                for w in range(width):\n+                    # Initialize output value\n+                    out[b, oc, h, w] = 0.0\n+                    for ic in range(in_channels):\n+                        for kh_idx in range(kh):\n+                            for kw_idx in range(kw):\n+                                if reverse:\n+                                    if (h - kh_idx >= 0) and (w - kw_idx >= 0):\n+                                        out[b, oc, h, w] += (\n+                                            input[b, ic, h - kh_idx, w - kw_idx] *\n+                                            weight[oc, ic, kh_idx, kw_idx]\n+                                        )\n+                                else:\n+                                    if (h + kh_idx < height) and (w + kw_idx < width):\n+                                        out[b, oc, h, w] += (\n+                                            input[b, ic, h + kh_idx, w + kw_idx] *\n+                                            weight[oc, ic, kh_idx, kw_idx]\n+                                        )\n \n \n tensor_conv2d = njit(parallel=True, fastmath=True)(_tensor_conv2d)\n@@ -267,4 +294,4 @@\n         return grad_input, grad_weight\n \n \n-conv2d = Conv2dFun.apply\n+conv2d = Conv2dFun.apply\n--- a\/minitorch\/tensor_functions.py\n+++ b\/minitorch\/tensor_functions.py\n@@ -99,61 +99,59 @@\n class Mul(Function):\n     @staticmethod\n     def forward(ctx: Context, a: Tensor, b: Tensor) -> Tensor:\n-        # TODO: Implement for Task 2.3.\n-        raise NotImplementedError('Need to implement for Task 2.3')\n-\n-    @staticmethod\n-    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor]:\n-        # TODO: Implement for Task 2.4.\n-        raise NotImplementedError('Need to implement for Task 2.4')\n+        ctx.save_for_backward(a, b)\n+        return a.f.mul_zip(a, b)\n+\n+    @staticmethod\n+    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor]:\n+        a, b = ctx.saved_values\n+        return grad_output.f.mul_zip(grad_output, b), grad_output.f.mul_zip(a, grad_output)\n \n \n class Sigmoid(Function):\n     @staticmethod\n     def forward(ctx: Context, t1: Tensor) -> Tensor:\n-        # TODO: Implement for Task 2.3.\n-        raise NotImplementedError('Need to implement for Task 2.3')\n-\n-    @staticmethod\n-    def backward(ctx: Context, grad_output: Tensor) -> Tensor:\n-        # TODO: Implement for Task 2.4.\n-        raise NotImplementedError('Need to implement for Task 2.4')\n+        ctx.save_for_backward(t1)\n+        return t1.f.sigmoid(t1)\n+\n+    @staticmethod\n+    def backward(ctx: Context, grad_output: Tensor) -> Tensor:\n+        (t1,) = ctx.saved_values\n+        return grad_output.f.sigmoid_back(t1, grad_output)\n \n \n class ReLU(Function):\n     @staticmethod\n     def forward(ctx: Context, t1: Tensor) -> Tensor:\n-        # TODO: Implement for Task 2.3.\n-        raise NotImplementedError('Need to implement for Task 2.3')\n-\n-    @staticmethod\n-    def backward(ctx: Context, grad_output: Tensor) -> Tensor:\n-        # TODO: Implement for Task 2.4.\n-        raise NotImplementedError('Need to implement for Task 2.4')\n+        return t1.f.relu(t1)\n+\n+    @staticmethod\n+    def backward(ctx: Context, grad_output: Tensor) -> Tensor:\n+        return grad_output.f.relu_back(t1, grad_output)\n \n \n class Log(Function):\n     @staticmethod\n     def forward(ctx: Context, t1: Tensor) -> Tensor:\n-        # TODO: Implement for Task 2.3.\n-        raise NotImplementedError('Need to implement for Task 2.3')\n-\n-    @staticmethod\n-    def backward(ctx: Context, grad_output: Tensor) -> Tensor:\n-        # TODO: Implement for Task 2.4.\n-        raise NotImplementedError('Need to implement for Task 2.4')\n+        ctx.save_for_backward(t1)\n+        return t1.f.log_map(t1)\n+\n+    @staticmethod\n+    def backward(ctx: Context, grad_output: Tensor) -> Tensor:\n+        (t1,) = ctx.saved_values\n+        return grad_output.f.log_back(t1, grad_output)\n \n \n class Exp(Function):\n     @staticmethod\n     def forward(ctx: Context, t1: Tensor) -> Tensor:\n-        # TODO: Implement for Task 2.3.\n-        raise NotImplementedError('Need to implement for Task 2.3')\n-\n-    @staticmethod\n-    def backward(ctx: Context, grad_output: Tensor) -> Tensor:\n-        # TODO: Implement for Task 2.4.\n-        raise NotImplementedError('Need to implement for Task 2.4')\n+        ctx.save_for_backward(t1)\n+        return t1.f.exp_map(t1)\n+\n+    @staticmethod\n+    def backward(ctx: Context, grad_output: Tensor) -> Tensor:\n+        (t1,) = ctx.saved_values\n+        return grad_output.f.exp_back(t1, grad_output)\n \n \n class Sum(Function):\n@@ -180,44 +178,46 @@\n class LT(Function):\n     @staticmethod\n     def forward(ctx: Context, a: Tensor, b: Tensor) -> Tensor:\n-        # TODO: Implement for Task 2.3.\n-        raise NotImplementedError('Need to implement for Task 2.3')\n-\n-    @staticmethod\n-    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor]:\n-        # TODO: Implement for Task 2.4.\n-        raise NotImplementedError('Need to implement for Task 2.4')\n+        return a.f.lt_zip(a, b)\n+\n+    @staticmethod\n+    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor]:\n+        return grad_output.f.lt_back(a, b, grad_output), None\n \n \n class EQ(Function):\n     @staticmethod\n     def forward(ctx: Context, a: Tensor, b: Tensor) -> Tensor:\n-        # TODO: Implement for Task 2.3.\n-        raise NotImplementedError('Need to implement for Task 2.3')\n-\n-    @staticmethod\n-    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor]:\n-        # TODO: Implement for Task 2.4.\n-        raise NotImplementedError('Need to implement for Task 2.4')\n+        return a.f.eq_zip(a, b)\n+\n+    @staticmethod\n+    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor]:\n+        return grad_output.f.eq_back(a, b, grad_output), None\n \n \n class IsClose(Function):\n     @staticmethod\n     def forward(ctx: Context, a: Tensor, b: Tensor) -> Tensor:\n-        # TODO: Implement for Task 2.3.\n-        raise NotImplementedError('Need to implement for Task 2.3')\n+        return a.f.is_close(a, b)\n+\n+    @staticmethod\n+    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor]:\n+        return grad_output.f.is_close_back(a, b, grad_output), None\n \n \n class Permute(Function):\n     @staticmethod\n     def forward(ctx: Context, a: Tensor, order: Tensor) -> Tensor:\n-        # TODO: Implement for Task 2.3.\n-        raise NotImplementedError('Need to implement for Task 2.3')\n+        ctx.save_for_backward(a.shape)\n+        return a.f.permute(a, order)\n \n     @staticmethod\n     def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, float]:\n-        # TODO: Implement for Task 2.4.\n-        raise NotImplementedError('Need to implement for Task 2.4')\n+        (original,) = ctx.saved_values\n+        return (\n+            grad_output.f.permute_back(grad_output, original),\n+            0.0,\n+        )\n \n \n class View(Function):\n@@ -410,4 +410,4 @@\n             1e-2,\n             1e-2,\n             err_msg=err_msg % (f, vals, x.grad[ind], i, ind, check),\n-        )\n+        )\n--- a\/minitorch\/scalar_functions.py\n+++ b\/minitorch\/scalar_functions.py\n@@ -103,13 +103,13 @@\n \n     @staticmethod\n     def forward(ctx: Context, a: float, b: float) -> float:\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n+        ctx.save_for_backward(a, b)\n+        return a * b\n \n     @staticmethod\n     def backward(ctx: Context, d_output: float) -> Tuple[float, float]:\n-        # TODO: Implement for Task 1.4.\n-        raise NotImplementedError('Need to implement for Task 1.4')\n+        (a, b) = ctx.saved_values\n+        return b * d_output, a * d_output\n \n \n class Inv(ScalarFunction):\n@@ -117,13 +117,13 @@\n \n     @staticmethod\n     def forward(ctx: Context, a: float) -> float:\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n-\n-    @staticmethod\n-    def backward(ctx: Context, d_output: float) -> float:\n-        # TODO: Implement for Task 1.4.\n-        raise NotImplementedError('Need to implement for Task 1.4')\n+        ctx.save_for_backward(a)\n+        return operators.inv(a)\n+\n+    @staticmethod\n+    def backward(ctx: Context, d_output: float) -> float:\n+        (a,) = ctx.saved_values\n+        return operators.inv_back(a, d_output)\n \n \n class Neg(ScalarFunction):\n@@ -131,13 +131,12 @@\n \n     @staticmethod\n     def forward(ctx: Context, a: float) -> float:\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n-\n-    @staticmethod\n-    def backward(ctx: Context, d_output: float) -> float:\n-        # TODO: Implement for Task 1.4.\n-        raise NotImplementedError('Need to implement for Task 1.4')\n+        ctx.save_for_backward(a)\n+        return -a\n+\n+    @staticmethod\n+    def backward(ctx: Context, d_output: float) -> float:\n+        return -d_output\n \n \n class Sigmoid(ScalarFunction):\n@@ -145,13 +144,13 @@\n \n     @staticmethod\n     def forward(ctx: Context, a: float) -> float:\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n-\n-    @staticmethod\n-    def backward(ctx: Context, d_output: float) -> float:\n-        # TODO: Implement for Task 1.4.\n-        raise NotImplementedError('Need to implement for Task 1.4')\n+        ctx.save_for_backward(a)\n+        return operators.sigmoid(a)\n+\n+    @staticmethod\n+    def backward(ctx: Context, d_output: float) -> float:\n+        (a,) = ctx.saved_values\n+        return operators.sigmoid_back(a, d_output)\n \n \n class ReLU(ScalarFunction):\n@@ -159,13 +158,13 @@\n \n     @staticmethod\n     def forward(ctx: Context, a: float) -> float:\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n-\n-    @staticmethod\n-    def backward(ctx: Context, d_output: float) -> float:\n-        # TODO: Implement for Task 1.4.\n-        raise NotImplementedError('Need to implement for Task 1.4')\n+        ctx.save_for_backward(a)\n+        return operators.relu(a)\n+\n+    @staticmethod\n+    def backward(ctx: Context, d_output: float) -> float:\n+        (a,) = ctx.saved_values\n+        return operators.relu_back(a, d_output)\n \n \n class Exp(ScalarFunction):\n@@ -173,13 +172,13 @@\n \n     @staticmethod\n     def forward(ctx: Context, a: float) -> float:\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n-\n-    @staticmethod\n-    def backward(ctx: Context, d_output: float) -> float:\n-        # TODO: Implement for Task 1.4.\n-        raise NotImplementedError('Need to implement for Task 1.4')\n+        ctx.save_for_backward(a)\n+        return operators.exp(a)\n+\n+    @staticmethod\n+    def backward(ctx: Context, d_output: float) -> float:\n+        (a,) = ctx.saved_values\n+        return operators.exp_back(a, d_output)\n \n \n class LT(ScalarFunction):\n@@ -187,13 +186,13 @@\n \n     @staticmethod\n     def forward(ctx: Context, a: float, b: float) -> float:\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n+        ctx.save_for_backward(a, b)\n+        return operators.lt(a, b)\n \n     @staticmethod\n     def backward(ctx: Context, d_output: float) -> Tuple[float, float]:\n-        # TODO: Implement for Task 1.4.\n-        raise NotImplementedError('Need to implement for Task 1.4')\n+        (a, b) = ctx.saved_values\n+        return (d_output if a < b else 0.0, -d_output if a < b else 0.0)\n \n \n class EQ(ScalarFunction):\n@@ -201,10 +200,10 @@\n \n     @staticmethod\n     def forward(ctx: Context, a: float, b: float) -> float:\n-        # TODO: Implement for Task 1.2.\n-        raise NotImplementedError('Need to implement for Task 1.2')\n+        ctx.save_for_backward(a, b)\n+        return operators.eq(a, b)\n \n     @staticmethod\n     def backward(ctx: Context, d_output: float) -> Tuple[float, float]:\n-        # TODO: Implement for Task 1.4.\n-        raise NotImplementedError('Need to implement for Task 1.4')\n+        (a, b) = ctx.saved_values\n+        return (d_output if a == b else 0.0, d_output if a == b else 0.0)\n\n"}
